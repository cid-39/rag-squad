{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff254a04",
   "metadata": {},
   "source": [
    "# Making some noise\n",
    "So we saw I got some big results in the metrics, this is expected as there is only so much data to sift through. Idea here is to get text that is similar with the general topics and similar in the way of typing, so I will pull 19 articles and chunk them. With that we will test the results both with and without the cross-encoder reranker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c135e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 19 new articles for noise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2077 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 369 chunks of thematic noise from wikipedia articles.\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "similar_articles = [\n",
    "    \"Operating_system\", \"Central_processing_unit\", \"Network_security\", \n",
    "    \"Logic_gate\", \"Relational_database\", \"Virtual_machine\", \n",
    "    \"Compiler\", \"Data_structure\", \"Ethernet\", \"Cryptography\", \n",
    "    \"Internet_protocol_suite\", \"Algorithm\", \"Embedded_system\", \n",
    "    \"Random-access_memory\", \"Microprocessor\", \"Cloud_computing\",\n",
    "    \"Flash_memory\", \"Integrated_circuit\", \"Silicon_Valley\"\n",
    "]\n",
    "\n",
    "# Same as in notebook 04\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/embeddinggemma-300m\")\n",
    "splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer,chunk_size=512, chunk_overlap=75)\n",
    "\n",
    "noise_docs = []\n",
    "print(f\"Fetching {len(similar_articles)} new articles for noise\")\n",
    "\n",
    "for title in similar_articles:\n",
    "    try:\n",
    "        page = wikipedia.page(title, auto_suggest=False)\n",
    "        chunks = splitter.split_text(page.content)\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            noise_docs.append(Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    \"para_id\": 9999,\n",
    "                    \"article\": title,\n",
    "                    \"is_noise\": True\n",
    "                }\n",
    "            ))\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {title}: {e}\")\n",
    "\n",
    "print(f\"Created {len(noise_docs)} chunks of thematic noise from wikipedia articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6fce32",
   "metadata": {},
   "source": [
    "With 369 chunks, we will have just under 40% of noise chunks. For now we will treat them as wrong answers even though some questions might be answerable with this noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63b42f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_processed_data\n",
    "docs_for_splitter, questions = load_processed_data()\n",
    "splits = splitter.split_documents(docs_for_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e2616b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks for Chroma: 950\n"
     ]
    }
   ],
   "source": [
    "ready_chunks = splits + noise_docs\n",
    "print(f\"Chunks for Chroma: {len(ready_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dac772",
   "metadata": {},
   "source": [
    "We've prepared everything for creating the vectorstore. After this, next move is testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d18ff169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing embeddings...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "persist_directory = \"./chroma/06_testing_with_noise\"\n",
    "embed = HuggingFaceEmbeddings(model_name=\"google/embeddinggemma-300m\",\n",
    "                              encode_kwargs={'normalize_embeddings': True})\n",
    "if os.path.exists(persist_directory):\n",
    "    print(\"Loading existing embeddings...\")\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=persist_directory, \n",
    "        embedding_function=embed\n",
    "    )\n",
    "else:\n",
    "    print(\"No existing index found. Generating embeddings...\")\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=ready_chunks,\n",
    "        embedding=embed,\n",
    "        persist_directory=persist_directory,\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0ca2d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_mini_question_set\n",
    "questions_mini = load_mini_question_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "676627f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
    "from langchain_classic.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "model = HuggingFaceCrossEncoder(model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "compressor = CrossEncoderReranker(model=model, top_n=10)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, \n",
    "    base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9d5928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import evaluate_retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fa842a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on 350 questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:35<00:00,  9.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "MRR: 0.8088\n",
      "Hit Rate@1: 72.29%\n",
      "Hit Rate@3: 87.71%\n",
      "Hit Rate@5: 91.71%\n",
      "Hit Rate@7: 94.57%\n",
      "Hit Rate@10: 95.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_no_rerank = evaluate_retrieval(questions_mini, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "534084de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on 350 questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [02:19<00:00,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "MRR: 0.9060\n",
      "Hit Rate@1: 86.57%\n",
      "Hit Rate@3: 95.14%\n",
      "Hit Rate@5: 95.71%\n",
      "Hit Rate@7: 95.71%\n",
      "Hit Rate@10: 95.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_rerank = evaluate_retrieval(questions_mini, compression_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd71bd9",
   "metadata": {},
   "source": [
    "These results make sense, with more noise, simple system with no ranking dropped Top 1 metric by 6% when the the noise is added, we can see the progression from top 1 to top 10 is slower when noise is added, but the difference between top 10 with and without noise is 1%, meaning the noise that was added was able to muddy the water just enough to lower the top 1 and top 3, but if we retrieve enough documents, we will almost always get the target paragraph.<br>\n",
    "Looking at difference between reranker systems with and without noise, difference is still noticable at top 1, but at top 3 the decrease is negligable. This makes sense, as I mentioned above, with only so much noise, we were able to still get the right document in our retrieved 10, meaning if reranker is effective enough he will be able to cut through the noise and more effectively bring us the target paragraph."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
