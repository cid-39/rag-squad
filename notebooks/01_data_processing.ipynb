{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74e0df51",
   "metadata": {},
   "source": [
    "# Data loading and preproccessing\n",
    "First of, we will laod the data and take the select few articles related to data science and computers, with one outlier being Memory article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "125493e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "target_articles = {}\n",
    "# Loading articles from dev\n",
    "with open('../../data/stanford-question-answering-dataset/dev-v1.1.json', 'r', encoding='utf-8') as f:\n",
    "    squad_data_dev = json.load(f)\n",
    "for i, article in enumerate(squad_data_dev['data']):\n",
    "    if i==4 or i==19 :\n",
    "        target_articles[article['title']] = article['paragraphs']\n",
    "# Loading articles from train\n",
    "with open('../../data/stanford-question-answering-dataset/train-v1.1.json', 'r', encoding='utf-8') as f2:\n",
    "    squad_data_train = json.load(f2)\n",
    "for i, article in enumerate(squad_data_train['data']):\n",
    "    if i in {30, 62, 104, 124, 149, 157, 177, 229, 293, 295, 330, 333, 389, 393} :\n",
    "        target_articles[article['title']] = article['paragraphs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08603aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: Computational_complexity_theory\n",
      "Article: Packet_switching\n",
      "Article: Wayback_Machine\n",
      "Article: Web_browser\n",
      "Article: Computer\n",
      "Article: Computer_security\n",
      "Article: ASCII\n",
      "Article: Macintosh\n",
      "Article: Memory\n",
      "Article: Data_compression\n",
      "Article: USB\n",
      "Article: Unicode\n",
      "Article: IBM\n",
      "Article: Software_testing\n",
      "Article: Database\n",
      "Article: Printed_circuit_board\n"
     ]
    }
   ],
   "source": [
    "for article_title, paragraphs in target_articles.items():\n",
    "    print(f\"\"\"Article: {article_title}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff7e4b1",
   "metadata": {},
   "source": [
    "After loading in wanted the articles, we can prepare text for chunking and questions for testing the retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e8cd956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "docs_for_splitter = []\n",
    "questions_ground_truth = []\n",
    "p_idx=0\n",
    "for article_title, paragraphs in target_articles.items():\n",
    "    for _, para_data in enumerate(paragraphs):\n",
    "        context_text = para_data['context']\n",
    "        \n",
    "        # Creating documents with needed metadata\n",
    "        doc = Document(\n",
    "            page_content=context_text,\n",
    "            metadata={\n",
    "                \"para_id\": p_idx,\n",
    "                \"article\": article_title\n",
    "            }\n",
    "        )\n",
    "        docs_for_splitter.append(doc)\n",
    "        \n",
    "        # Extracting questions and linking them to doc that should be retrieved for them\n",
    "        for qa in para_data['qas']:\n",
    "            questions_ground_truth.append({\n",
    "                \"question\": qa['question'],\n",
    "                \"ground_truth_para_id\": p_idx,\n",
    "                # \"qa_id\": qa['id'],\n",
    "                # \"article\": article_title\n",
    "            })\n",
    "        p_idx+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821d780a",
   "metadata": {},
   "source": [
    "Finally, we will save this data for later use in pkl format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094dfe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully exported to ../../data/processed/squad_processed.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "output_path = \"../../data/processed/squad_processed.pkl\"\n",
    "output_data = {\n",
    "    \"docs_for_splitter\": docs_for_splitter,\n",
    "    \"questions_ground_truth\": questions_ground_truth\n",
    "}\n",
    "\n",
    "with open(output_path, \"xb\") as f:\n",
    "    pickle.dump(output_data, f)\n",
    "\n",
    "print(f\"Data successfully exported to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
