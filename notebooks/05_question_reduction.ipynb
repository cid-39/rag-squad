{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "969e84b3",
   "metadata": {},
   "source": [
    "# Reduction of set of questions for evaluation\n",
    "After reaching a high top 1, we can reduce the set of questions for quicker further experimentation, we will try to do this in a way to keep the questions stratified.<br>\n",
    "To attempt this, we will just run the code for evaluatin and keep track of which questions were not abled to be anwsered, then will create a mini set of questions with the similar ratio of anwsered and unanwsered questions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f0da14",
   "metadata": {},
   "source": [
    "Next cell just loads the data, vector store and the model from 04 notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bcb25dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
    "from langchain_classic.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from utils import load_processed_data\n",
    "\n",
    "_, questions_ground_truth = load_processed_data(\"../data/processed/squad_processed.pkl\")\n",
    "\n",
    "embed = HuggingFaceEmbeddings(model_name=\"google/embeddinggemma-300m\", encode_kwargs={'normalize_embeddings': True})\n",
    "persist_directory = \"./chroma/04_google_gemma\"\n",
    "vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embed)\n",
    "\n",
    "rerank_model = HuggingFaceCrossEncoder(model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "compressor = CrossEncoderReranker(model=rerank_model, top_n=10)\n",
    "retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, \n",
    "    base_retriever=vectorstore.as_retriever(search_kwargs={'k': 10})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b523f88a",
   "metadata": {},
   "source": [
    "Now we can run the evaluation and keep questions in separate lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cc30a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on 2265 questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2265/2265 [15:13<00:00,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "MRR: 0.9226\n",
      "Hit Rate@1: 89.05%\n",
      "Hit Rate@3: 95.32%\n",
      "Hit Rate@5: 96.42%\n",
      "Hit Rate@7: 96.64%\n",
      "Hit Rate@10: 96.78%\n",
      "\n",
      "Categorization Summary:\n",
      "Total Found: 2192\n",
      "Total Not Found: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "k_values = [1, 3, 5, 7, 10]\n",
    "hits = {k: 0 for k in k_values}\n",
    "total_mrr = 0.0\n",
    "total_questions = len(questions_ground_truth)\n",
    "max_k = max(k_values)\n",
    "\n",
    "#categories lists\n",
    "found_questions = []\n",
    "not_found_questions = []\n",
    "\n",
    "print(f\"Starting evaluation on {total_questions} questions...\")\n",
    "\n",
    "for q_data in tqdm(questions_ground_truth):\n",
    "    question = q_data['question']\n",
    "    target_id = q_data['ground_truth_para_id']\n",
    "    \n",
    "    retrieved_docs = retriever.invoke(question, k=max_k) \n",
    "    retrieved_ids = [doc.metadata.get('para_id') for doc in retrieved_docs]\n",
    "\n",
    "    if target_id in retrieved_ids:\n",
    "        # Mark as found\n",
    "        found_questions.append(q_data)\n",
    "        \n",
    "        # Metrics\n",
    "        rank = retrieved_ids.index(target_id) + 1\n",
    "        total_mrr += 1.0 / rank\n",
    "        for k in k_values:\n",
    "            if rank <= k:\n",
    "                hits[k] += 1\n",
    "    else:\n",
    "        # mark as not found\n",
    "        not_found_questions.append(q_data)\n",
    "\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "print(f\"MRR: {total_mrr / total_questions:.4f}\")\n",
    "for k in k_values:\n",
    "    print(f\"Hit Rate@{k}: {(hits[k] / total_questions)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nCategorization Summary:\")\n",
    "print(f\"Total Found: {len(found_questions)}\")\n",
    "print(f\"Total Not Found: {len(not_found_questions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37452dd",
   "metadata": {},
   "source": [
    "With that done, we will create the mini set of questions and run the evaluation on them to see if the metrics are up there with the larger set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c677b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified mini set created with 350 questions.\n"
     ]
    }
   ],
   "source": [
    "sample_size = 350\n",
    "\n",
    "\n",
    "found_ratio = len(found_questions) / total_questions\n",
    "n_found = int(sample_size * found_ratio)\n",
    "n_not_found = sample_size - n_found\n",
    "\n",
    "questions_mini_set = random.sample(found_questions, n_found) + random.sample(not_found_questions, n_not_found)\n",
    "random.shuffle(questions_mini_set) # just in case :D\n",
    "\n",
    "print(f\"Stratified mini set created with {len(questions_mini_set)} questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e913f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on 350 questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [02:09<00:00,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "MRR: 0.9128\n",
      "Hit Rate@1: 87.14%\n",
      "Hit Rate@3: 95.71%\n",
      "Hit Rate@5: 96.57%\n",
      "Hit Rate@7: 96.57%\n",
      "Hit Rate@10: 96.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import evaluate_retrieval\n",
    "results_mini_set = evaluate_retrieval(questions_mini_set, retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472a7331",
   "metadata": {},
   "source": [
    "MRR and hit rates at 1, 3 and 5 stayed in the decent range, while we see hit rates at 7 and 9 plateau. After testing multiple times to check that i dont always run with the same seed, these three rates always stayed at 96.57%. Thinking this through, I've realized that the span between 96.42% (Hit Rate@5 with full question set) and 96.78% (Hit Rate@10 with full question set) consists of around 8 questions that get their answer after 5th document. One (or all for that matter) of these questions being picked in ~338 from 2192 total answered questions is unlikely. <br>\n",
    "I will not stratify the mini set to account for these 8 as I will accept the 96.57% as plateau for this system.<br><br>\n",
    "Lastly, I will save the data for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "506abb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully exported to ../data/processed/squad_processed_mini.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "output_path = \"../data/processed/squad_processed_mini.pkl\"\n",
    "output_data = {\n",
    "    \"questions_ground_truth\": questions_mini_set\n",
    "}\n",
    "\n",
    "with open(output_path, \"xb\") as f:\n",
    "    pickle.dump(output_data, f)\n",
    "\n",
    "print(f\"Data successfully exported to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
