{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e879b1fc",
   "metadata": {},
   "source": [
    "# The G in RAG\n",
    "After experimenting with retrieval models and tactics, we can finnaly complete the rag system. I will be using phi3 mini model as its small and should be able to run relatively easily locally on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fc5fc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
    "from langchain_classic.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "persist_directory = \"./chroma/06_testing_with_noise\"\n",
    "embed = HuggingFaceEmbeddings(model_name=\"google/embeddinggemma-300m\", encode_kwargs={'normalize_embeddings': True})\n",
    "vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embed)\n",
    "\n",
    "model = HuggingFaceCrossEncoder(model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "compressor = CrossEncoderReranker(model=model, top_n=10)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, \n",
    "    base_retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdaa3d3",
   "metadata": {},
   "source": [
    "The first step is to get our vectorstore and our reranking retriever from earlier. Then we can make a chain using phi3 mini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9cd2186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOllama(model=\"phi3:mini\", temperature=0)\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an expert technical assistant. Use the following pieces of retrieved \"\n",
    "    \"context to answer the question. If you don't know the answer based on the \"\n",
    "    \"context, say that you don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(compression_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68720f6",
   "metadata": {},
   "source": [
    "And now, we can see how the \"ai assistant\" works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d8c5e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func(question) :\n",
    "    response = rag_chain.invoke({\"input\": question})\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {response['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f8041a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many companies did Apple promise were develping products for the new computer?\n",
      "Answer: Apple promised that 79 companies, including Lotus, Digital Research, and Ashton-Tate, were developing products for the new computer.\n",
      "Question: What did Donald Davies Develop\n",
      "Answer: Donald Davies independently developed packet switching at NPL and proposed a nationwide network for the UK, which later influenced networks like ARPANET in the US. He coined the term \"packet switching\" after learning about Baran's work on Distributed Adaptive Message Block Switching.\n",
      "Question: What character represents the \"line feed\" function?\n",
      "Answer: Character 10 represents the \"line feed\" function in ASCII. It causes a printer or display system to advance its paper by one line without moving the printhead, effectively starting on a new line of text.\n",
      "Question: What proposal has been made for the Mayan script? \n",
      "Answer: No encoding proposals have yet been made for the Rongorongo and Mayan scripts; user communities are still working out details of their repertoire.\n",
      "Question: How were the connections supported? \n",
      "Answer: The standard connectors used for USB devices are Type-A and Type-B receptacles, which enforce a directed topology to prevent incorrect power supply connections.\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "from utils import load_processed_data\n",
    "\n",
    "_, questions = load_processed_data()\n",
    "for x in sample(questions,5) :\n",
    "    test_func(x['question'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
