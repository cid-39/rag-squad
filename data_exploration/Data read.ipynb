{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4cf6116",
   "metadata": {},
   "source": [
    "Looking for computer science related articles, first in dev, then in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6023236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def show_titles(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        squad_data = json.load(f)\n",
    "    articles = squad_data['data']\n",
    "    print(f\"Total articles: {len(articles)}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    for i, article in enumerate(articles):\n",
    "        title = article['title']\n",
    "        num_paragraphs = len(article['paragraphs'])\n",
    "        print(f\"{i+1}. {title} ({num_paragraphs} paragraphs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96aa7709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev file:\n",
      "Total articles: 48\n",
      "------------------------------\n",
      "1. Super_Bowl_50 (54 paragraphs)\n",
      "2. Warsaw (49 paragraphs)\n",
      "3. Normans (45 paragraphs)\n",
      "4. Nikola_Tesla (92 paragraphs)\n",
      "5. Computational_complexity_theory (48 paragraphs)\n",
      "6. Teacher (58 paragraphs)\n",
      "7. Martin_Luther (98 paragraphs)\n",
      "8. Southern_California (39 paragraphs)\n",
      "9. Sky_(United_Kingdom) (22 paragraphs)\n",
      "10. Victoria_(Australia) (25 paragraphs)\n",
      "11. Huguenot (44 paragraphs)\n",
      "12. Steam_engine (46 paragraphs)\n",
      "13. Oxygen (43 paragraphs)\n",
      "14. 1973_oil_crisis (24 paragraphs)\n",
      "15. Apollo_program (57 paragraphs)\n",
      "16. European_Union_law (41 paragraphs)\n",
      "17. Amazon_rainforest (21 paragraphs)\n",
      "18. Ctenophora (31 paragraphs)\n",
      "19. Fresno,_California (28 paragraphs)\n",
      "20. Packet_switching (23 paragraphs)\n",
      "21. Black_Death (23 paragraphs)\n",
      "22. Geology (25 paragraphs)\n",
      "23. Newcastle_upon_Tyne (53 paragraphs)\n",
      "24. Victoria_and_Albert_Museum (59 paragraphs)\n",
      "25. American_Broadcasting_Company (98 paragraphs)\n",
      "26. Genghis_Khan (54 paragraphs)\n",
      "27. Pharmacy (26 paragraphs)\n",
      "28. Immune_system (49 paragraphs)\n",
      "29. Civil_disobedience (26 paragraphs)\n",
      "30. Construction (22 paragraphs)\n",
      "31. Private_school (26 paragraphs)\n",
      "32. Harvard_University (30 paragraphs)\n",
      "33. Jacksonville,_Florida (21 paragraphs)\n",
      "34. Economic_inequality (44 paragraphs)\n",
      "35. Doctor_Who (64 paragraphs)\n",
      "36. University_of_Chicago (37 paragraphs)\n",
      "37. Yuan_dynasty (47 paragraphs)\n",
      "38. Kenya (54 paragraphs)\n",
      "39. Intergovernmental_Panel_on_Climate_Change (24 paragraphs)\n",
      "40. Chloroplast (63 paragraphs)\n",
      "41. Prime_number (31 paragraphs)\n",
      "42. Rhine (44 paragraphs)\n",
      "43. Scottish_Parliament (39 paragraphs)\n",
      "44. Islamism (39 paragraphs)\n",
      "45. Imperialism (39 paragraphs)\n",
      "46. United_Methodist_Church (52 paragraphs)\n",
      "47. French_and_Indian_War (46 paragraphs)\n",
      "48. Force (44 paragraphs)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dev file:\")\n",
    "show_titles('../data/stanford-question-answering-dataset/dev-v1.1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb05e224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file:\n",
      "Total articles: 442\n",
      "------------------------------\n",
      "1. University_of_Notre_Dame (55 paragraphs)\n",
      "2. Beyoncé (66 paragraphs)\n",
      "3. Montana (44 paragraphs)\n",
      "4. Genocide (26 paragraphs)\n",
      "5. Antibiotics (26 paragraphs)\n",
      "6. Frédéric_Chopin (82 paragraphs)\n",
      "7. Sino-Tibetan_relations_during_the_Ming_dynasty (72 paragraphs)\n",
      "8. IPod (60 paragraphs)\n",
      "9. The_Legend_of_Zelda:_Twilight_Princess (32 paragraphs)\n",
      "10. Spectre_(2015_film) (43 paragraphs)\n",
      "11. 2008_Sichuan_earthquake (77 paragraphs)\n",
      "12. New_York_City (148 paragraphs)\n",
      "13. To_Kill_a_Mockingbird (62 paragraphs)\n",
      "14. Solar_energy (52 paragraphs)\n",
      "15. Tajikistan (32 paragraphs)\n",
      "16. Anthropology (45 paragraphs)\n",
      "17. Portugal (98 paragraphs)\n",
      "18. Kanye_West (79 paragraphs)\n",
      "19. Buddhism (149 paragraphs)\n",
      "20. American_Idol (127 paragraphs)\n",
      "21. Dog (75 paragraphs)\n",
      "22. 2008_Summer_Olympics_torch_relay (74 paragraphs)\n",
      "23. Alfred_North_Whitehead (47 paragraphs)\n",
      "24. Financial_crisis_of_2007%E2%80%9308 (79 paragraphs)\n",
      "25. Saint_Barth%C3%A9lemy (24 paragraphs)\n",
      "26. Genome (25 paragraphs)\n",
      "27. Comprehensive_school (24 paragraphs)\n",
      "28. Republic_of_the_Congo (39 paragraphs)\n",
      "29. Prime_minister (35 paragraphs)\n",
      "30. Institute_of_technology (77 paragraphs)\n",
      "31. Wayback_Machine (26 paragraphs)\n",
      "32. Dutch_Republic (21 paragraphs)\n",
      "33. Symbiosis (23 paragraphs)\n",
      "34. Canadian_Armed_Forces (45 paragraphs)\n",
      "35. Cardinal_(Catholicism) (43 paragraphs)\n",
      "36. Iranian_languages (21 paragraphs)\n",
      "37. Lighting (57 paragraphs)\n",
      "38. Separation_of_powers_under_the_United_States_Constitution (27 paragraphs)\n",
      "39. Architecture (38 paragraphs)\n",
      "40. Human_Development_Index (32 paragraphs)\n",
      "41. Southern_Europe (25 paragraphs)\n",
      "42. BBC_Television (31 paragraphs)\n",
      "43. Arnold_Schwarzenegger (86 paragraphs)\n",
      "44. Plymouth (81 paragraphs)\n",
      "45. Heresy (29 paragraphs)\n",
      "46. Warsaw_Pact (22 paragraphs)\n",
      "47. Materialism (27 paragraphs)\n",
      "48. Space_Race (70 paragraphs)\n",
      "49. Pub (90 paragraphs)\n",
      "50. Christian (26 paragraphs)\n",
      "51. Sony_Music_Entertainment (32 paragraphs)\n",
      "52. Oklahoma_City (82 paragraphs)\n",
      "53. Hunter-gatherer (34 paragraphs)\n",
      "54. United_Nations_Population_Fund (22 paragraphs)\n",
      "55. Russian_Soviet_Federative_Socialist_Republic (34 paragraphs)\n",
      "56. Universal_Studios (42 paragraphs)\n",
      "57. Alexander_Graham_Bell (74 paragraphs)\n",
      "58. Internet_service_provider (21 paragraphs)\n",
      "59. Comics (36 paragraphs)\n",
      "60. Saint_Helena (82 paragraphs)\n",
      "61. Aspirated_consonant (34 paragraphs)\n",
      "62. Hydrogen (60 paragraphs)\n",
      "63. Web_browser (24 paragraphs)\n",
      "64. Boston (79 paragraphs)\n",
      "65. BeiDou_Navigation_Satellite_System (34 paragraphs)\n",
      "66. Canon_law (22 paragraphs)\n",
      "67. Communications_in_Somalia (25 paragraphs)\n",
      "68. Catalan_language (80 paragraphs)\n",
      "69. Estonian_language (33 paragraphs)\n",
      "70. Paper (31 paragraphs)\n",
      "71. Arena_Football_League (55 paragraphs)\n",
      "72. Adult_contemporary_music (53 paragraphs)\n",
      "73. Matter (5 paragraphs)\n",
      "74. Westminster_Abbey (46 paragraphs)\n",
      "75. Nanjing (66 paragraphs)\n",
      "76. Bern (52 paragraphs)\n",
      "77. Daylight_saving_time (63 paragraphs)\n",
      "78. Royal_Institute_of_British_Architects (27 paragraphs)\n",
      "79. National_Archives_and_Records_Administration (28 paragraphs)\n",
      "80. Tristan_da_Cunha (16 paragraphs)\n",
      "81. University_of_Kansas (36 paragraphs)\n",
      "82. Political_corruption (57 paragraphs)\n",
      "83. Dialect (44 paragraphs)\n",
      "84. Classical_music (68 paragraphs)\n",
      "85. Slavs (49 paragraphs)\n",
      "86. Southampton (95 paragraphs)\n",
      "87. Treaty (57 paragraphs)\n",
      "88. Josip_Broz_Tito (67 paragraphs)\n",
      "89. Marshall_Islands (51 paragraphs)\n",
      "90. Szlachta (63 paragraphs)\n",
      "91. Virgil (23 paragraphs)\n",
      "92. Alps (87 paragraphs)\n",
      "93. Gene (56 paragraphs)\n",
      "94. Guinea-Bissau (50 paragraphs)\n",
      "95. List_of_numbered_streets_in_Manhattan (50 paragraphs)\n",
      "96. Brain (64 paragraphs)\n",
      "97. Near_East (59 paragraphs)\n",
      "98. Zhejiang (38 paragraphs)\n",
      "99. Ministry_of_Defence_(United_Kingdom) (25 paragraphs)\n",
      "100. High-definition_television (46 paragraphs)\n",
      "101. Wood (70 paragraphs)\n",
      "102. Somalis (71 paragraphs)\n",
      "103. Middle_Ages (93 paragraphs)\n",
      "104. Phonology (23 paragraphs)\n",
      "105. Computer (80 paragraphs)\n",
      "106. Black_people (61 paragraphs)\n",
      "107. The_Times (53 paragraphs)\n",
      "108. New_Delhi (52 paragraphs)\n",
      "109. Imamah_(Shia_doctrine) (14 paragraphs)\n",
      "110. Bird_migration (56 paragraphs)\n",
      "111. Atlantic_City,_New_Jersey (47 paragraphs)\n",
      "112. Immunology (18 paragraphs)\n",
      "113. MP3 (62 paragraphs)\n",
      "114. House_music (44 paragraphs)\n",
      "115. Letter_case (12 paragraphs)\n",
      "116. Chihuahua_(state) (66 paragraphs)\n",
      "117. Pitch_(music) (10 paragraphs)\n",
      "118. England_national_football_team (12 paragraphs)\n",
      "119. Houston (48 paragraphs)\n",
      "120. Copper (31 paragraphs)\n",
      "121. Identity_(social_science) (21 paragraphs)\n",
      "122. Himachal_Pradesh (23 paragraphs)\n",
      "123. Communication (12 paragraphs)\n",
      "124. Grape (10 paragraphs)\n",
      "125. Computer_security (25 paragraphs)\n",
      "126. Orthodox_Judaism (25 paragraphs)\n",
      "127. Animal (17 paragraphs)\n",
      "128. Beer (34 paragraphs)\n",
      "129. Race_and_ethnicity_in_the_United_States_Census (12 paragraphs)\n",
      "130. United_States_dollar (32 paragraphs)\n",
      "131. Imperial_College_London (25 paragraphs)\n",
      "132. Gymnastics (23 paragraphs)\n",
      "133. Hanover (21 paragraphs)\n",
      "134. Emotion (44 paragraphs)\n",
      "135. FC_Barcelona (50 paragraphs)\n",
      "136. Everton_F.C. (24 paragraphs)\n",
      "137. Old_English (22 paragraphs)\n",
      "138. Aircraft_carrier (34 paragraphs)\n",
      "139. Federal_Aviation_Administration (12 paragraphs)\n",
      "140. Lancashire (20 paragraphs)\n",
      "141. Mesozoic (16 paragraphs)\n",
      "142. Videoconferencing (21 paragraphs)\n",
      "143. Gregorian_calendar (23 paragraphs)\n",
      "144. Xbox_360 (26 paragraphs)\n",
      "145. Military_history_of_the_United_States (42 paragraphs)\n",
      "146. Hard_rock (25 paragraphs)\n",
      "147. Great_Plains (13 paragraphs)\n",
      "148. Infrared (20 paragraphs)\n",
      "149. Biodiversity (32 paragraphs)\n",
      "150. ASCII (26 paragraphs)\n",
      "151. Digestion (18 paragraphs)\n",
      "152. Federal_Bureau_of_Investigation (45 paragraphs)\n",
      "153. Adolescence (78 paragraphs)\n",
      "154. Antarctica (44 paragraphs)\n",
      "155. Mary_(mother_of_Jesus) (38 paragraphs)\n",
      "156. Melbourne (64 paragraphs)\n",
      "157. John,_King_of_England (76 paragraphs)\n",
      "158. Macintosh (49 paragraphs)\n",
      "159. Anti-aircraft_warfare (70 paragraphs)\n",
      "160. Sanskrit (22 paragraphs)\n",
      "161. Valencia (51 paragraphs)\n",
      "162. General_Electric (16 paragraphs)\n",
      "163. United_States_Army (34 paragraphs)\n",
      "164. Franco-Prussian_War (58 paragraphs)\n",
      "165. Eritrea (40 paragraphs)\n",
      "166. Uranium (40 paragraphs)\n",
      "167. Order_of_the_British_Empire (13 paragraphs)\n",
      "168. Age_of_Enlightenment (83 paragraphs)\n",
      "169. Circadian_rhythm (22 paragraphs)\n",
      "170. Elizabeth_II (45 paragraphs)\n",
      "171. Sexual_orientation (53 paragraphs)\n",
      "172. Dell (45 paragraphs)\n",
      "173. Capital_punishment_in_the_United_States (46 paragraphs)\n",
      "174. Nintendo_Entertainment_System (44 paragraphs)\n",
      "175. Ashkenazi_Jews (50 paragraphs)\n",
      "176. Athanasius_of_Alexandria (36 paragraphs)\n",
      "177. Seattle (52 paragraphs)\n",
      "178. Memory (32 paragraphs)\n",
      "179. Multiracial_American (44 paragraphs)\n",
      "180. Pharmaceutical_industry (44 paragraphs)\n",
      "181. Umayyad_Caliphate (35 paragraphs)\n",
      "182. Asphalt (36 paragraphs)\n",
      "183. Queen_Victoria (42 paragraphs)\n",
      "184. Freemasonry (37 paragraphs)\n",
      "185. Israel (99 paragraphs)\n",
      "186. Hellenistic_period (94 paragraphs)\n",
      "187. Napoleon (89 paragraphs)\n",
      "188. Bill_%26_Melinda_Gates_Foundation (21 paragraphs)\n",
      "189. Northwestern_University (44 paragraphs)\n",
      "190. Hokkien (21 paragraphs)\n",
      "191. Montevideo (70 paragraphs)\n",
      "192. Poultry (31 paragraphs)\n",
      "193. Arsenal_F.C. (34 paragraphs)\n",
      "194. Dutch_language (51 paragraphs)\n",
      "195. Buckingham_Palace (28 paragraphs)\n",
      "196. Incandescent_light_bulb (42 paragraphs)\n",
      "197. Clothing (21 paragraphs)\n",
      "198. Chicago_Cubs (63 paragraphs)\n",
      "199. States_of_Germany (23 paragraphs)\n",
      "200. Korean_War (75 paragraphs)\n",
      "201. Royal_Dutch_Shell (25 paragraphs)\n",
      "202. Copyright_infringement (39 paragraphs)\n",
      "203. Greece (95 paragraphs)\n",
      "204. Mammal (24 paragraphs)\n",
      "205. East_India_Company (31 paragraphs)\n",
      "206. Southeast_Asia (26 paragraphs)\n",
      "207. Professional_wrestling (61 paragraphs)\n",
      "208. Film_speed (26 paragraphs)\n",
      "209. Mexico_City (87 paragraphs)\n",
      "210. Germans (32 paragraphs)\n",
      "211. New_Haven,_Connecticut (66 paragraphs)\n",
      "212. Brigham_Young_University (37 paragraphs)\n",
      "213. Myocardial_infarction (5 paragraphs)\n",
      "214. Department_store (40 paragraphs)\n",
      "215. Intellectual_property (21 paragraphs)\n",
      "216. Florida (35 paragraphs)\n",
      "217. Queen_(band) (59 paragraphs)\n",
      "218. Presbyterianism (38 paragraphs)\n",
      "219. Thuringia (29 paragraphs)\n",
      "220. Predation (26 paragraphs)\n",
      "221. Marvel_Comics (25 paragraphs)\n",
      "222. British_Empire (60 paragraphs)\n",
      "223. Botany (51 paragraphs)\n",
      "224. Madonna_(entertainer) (76 paragraphs)\n",
      "225. London (77 paragraphs)\n",
      "226. Law_of_the_United_States (27 paragraphs)\n",
      "227. Myanmar (73 paragraphs)\n",
      "228. Jews (36 paragraphs)\n",
      "229. Cotton (30 paragraphs)\n",
      "230. Data_compression (26 paragraphs)\n",
      "231. The_Sun_(United_Kingdom) (49 paragraphs)\n",
      "232. Carnival (52 paragraphs)\n",
      "233. Pesticide (26 paragraphs)\n",
      "234. Somerset (37 paragraphs)\n",
      "235. Yale_University (46 paragraphs)\n",
      "236. Late_Middle_Ages (40 paragraphs)\n",
      "237. Ann_Arbor,_Michigan (32 paragraphs)\n",
      "238. Gothic_architecture (40 paragraphs)\n",
      "239. Cubism (39 paragraphs)\n",
      "240. Political_philosophy (21 paragraphs)\n",
      "241. Alloy (22 paragraphs)\n",
      "242. Norfolk_Island (30 paragraphs)\n",
      "243. Edmund_Burke (40 paragraphs)\n",
      "244. Samoa (25 paragraphs)\n",
      "245. Pope_Paul_VI (55 paragraphs)\n",
      "246. George_VI (23 paragraphs)\n",
      "247. Electric_motor (42 paragraphs)\n",
      "248. Switzerland (77 paragraphs)\n",
      "249. Mali (22 paragraphs)\n",
      "250. Nonprofit_organization (24 paragraphs)\n",
      "251. Raleigh,_North_Carolina (34 paragraphs)\n",
      "252. Nutrition (56 paragraphs)\n",
      "253. Crimean_War (62 paragraphs)\n",
      "254. Literature (23 paragraphs)\n",
      "255. Avicenna (35 paragraphs)\n",
      "256. Chinese_characters (56 paragraphs)\n",
      "257. Bermuda (47 paragraphs)\n",
      "258. Nigeria (55 paragraphs)\n",
      "259. Utrecht (27 paragraphs)\n",
      "260. John_von_Neumann (63 paragraphs)\n",
      "261. Molotov%E2%80%93Ribbentrop_Pact (44 paragraphs)\n",
      "262. Capacitor (37 paragraphs)\n",
      "263. History_of_science (67 paragraphs)\n",
      "264. Czech_language (28 paragraphs)\n",
      "265. Digimon (21 paragraphs)\n",
      "266. Glacier (23 paragraphs)\n",
      "267. Planck_constant (24 paragraphs)\n",
      "268. Comcast (27 paragraphs)\n",
      "269. Tuberculosis (31 paragraphs)\n",
      "270. Affirmative_action_in_the_United_States (48 paragraphs)\n",
      "271. FA_Cup (32 paragraphs)\n",
      "272. Alsace (27 paragraphs)\n",
      "273. Baptists (23 paragraphs)\n",
      "274. Child_labour (61 paragraphs)\n",
      "275. North_Carolina (61 paragraphs)\n",
      "276. Heian_period (24 paragraphs)\n",
      "277. On_the_Origin_of_Species (59 paragraphs)\n",
      "278. Dissolution_of_the_Soviet_Union (67 paragraphs)\n",
      "279. Crucifixion_of_Jesus (32 paragraphs)\n",
      "280. Miami (45 paragraphs)\n",
      "281. Supreme_court (23 paragraphs)\n",
      "282. Textual_criticism (32 paragraphs)\n",
      "283. Gramophone_record (85 paragraphs)\n",
      "284. Turner_Classic_Movies (23 paragraphs)\n",
      "285. Hindu_philosophy (25 paragraphs)\n",
      "286. Political_party (27 paragraphs)\n",
      "287. A_cappella (23 paragraphs)\n",
      "288. Dominican_Order (44 paragraphs)\n",
      "289. Eton_College (43 paragraphs)\n",
      "290. Cork_(city) (30 paragraphs)\n",
      "291. Federalism (38 paragraphs)\n",
      "292. Galicia_(Spain) (50 paragraphs)\n",
      "293. Green (27 paragraphs)\n",
      "294. USB (66 paragraphs)\n",
      "295. Sichuan (25 paragraphs)\n",
      "296. Unicode (33 paragraphs)\n",
      "297. Detroit (79 paragraphs)\n",
      "298. Culture (24 paragraphs)\n",
      "299. Sahara (25 paragraphs)\n",
      "300. Rule_of_law (24 paragraphs)\n",
      "301. Tibet (39 paragraphs)\n",
      "302. Exhibition_game (21 paragraphs)\n",
      "303. Strasbourg (34 paragraphs)\n",
      "304. Oklahoma (55 paragraphs)\n",
      "305. History_of_India (78 paragraphs)\n",
      "306. Gamal_Abdel_Nasser (95 paragraphs)\n",
      "307. Pope_John_XXIII (25 paragraphs)\n",
      "308. Time (29 paragraphs)\n",
      "309. European_Central_Bank (28 paragraphs)\n",
      "310. St._John%27s,_Newfoundland_and_Labrador (31 paragraphs)\n",
      "311. PlayStation_3 (44 paragraphs)\n",
      "312. Royal_assent (38 paragraphs)\n",
      "313. Group_(mathematics) (30 paragraphs)\n",
      "314. Central_African_Republic (23 paragraphs)\n",
      "315. Asthma (25 paragraphs)\n",
      "316. LaserDisc (50 paragraphs)\n",
      "317. Annelid (35 paragraphs)\n",
      "318. God (24 paragraphs)\n",
      "319. War_on_Terror (31 paragraphs)\n",
      "320. Labour_Party_(UK) (24 paragraphs)\n",
      "321. Estonia (72 paragraphs)\n",
      "322. Serbo-Croatian (22 paragraphs)\n",
      "323. Alaska (48 paragraphs)\n",
      "324. Karl_Popper (43 paragraphs)\n",
      "325. Mandolin (43 paragraphs)\n",
      "326. Insect (60 paragraphs)\n",
      "327. Race_(human_categorization) (65 paragraphs)\n",
      "328. Paris (94 paragraphs)\n",
      "329. Apollo (52 paragraphs)\n",
      "330. United_States_presidential_election,_2004 (26 paragraphs)\n",
      "331. IBM (26 paragraphs)\n",
      "332. Liberal_Party_of_Australia (26 paragraphs)\n",
      "333. Samurai (50 paragraphs)\n",
      "334. Software_testing (29 paragraphs)\n",
      "335. Glass (26 paragraphs)\n",
      "336. Renewable_energy_commercialization (40 paragraphs)\n",
      "337. Palermo (24 paragraphs)\n",
      "338. Zinc (38 paragraphs)\n",
      "339. Neoclassical_architecture (22 paragraphs)\n",
      "340. CBC_Television (25 paragraphs)\n",
      "341. Appalachian_Mountains (16 paragraphs)\n",
      "342. Energy (33 paragraphs)\n",
      "343. East_Prussia (27 paragraphs)\n",
      "344. Ottoman_Empire (69 paragraphs)\n",
      "345. Philosophy_of_space_and_time (24 paragraphs)\n",
      "346. Neolithic (23 paragraphs)\n",
      "347. Friedrich_Hayek (50 paragraphs)\n",
      "348. Diarrhea (21 paragraphs)\n",
      "349. Madrasa (35 paragraphs)\n",
      "350. Philadelphia (59 paragraphs)\n",
      "351. John_Kerry (52 paragraphs)\n",
      "352. Rajasthan (24 paragraphs)\n",
      "353. Guam (36 paragraphs)\n",
      "354. Empiricism (25 paragraphs)\n",
      "355. Idealism (29 paragraphs)\n",
      "356. Education (34 paragraphs)\n",
      "357. Tennessee (50 paragraphs)\n",
      "358. Post-punk (22 paragraphs)\n",
      "359. Canadian_football (21 paragraphs)\n",
      "360. Seven_Years%27_War (61 paragraphs)\n",
      "361. Richard_Feynman (32 paragraphs)\n",
      "362. Muammar_Gaddafi (76 paragraphs)\n",
      "363. Cyprus (59 paragraphs)\n",
      "364. Steven_Spielberg (49 paragraphs)\n",
      "365. Elevator (52 paragraphs)\n",
      "366. Neptune (36 paragraphs)\n",
      "367. Railway_electrification_system (34 paragraphs)\n",
      "368. Spanish_language_in_the_United_States (22 paragraphs)\n",
      "369. Charleston,_South_Carolina (48 paragraphs)\n",
      "370. Red (48 paragraphs)\n",
      "371. The_Blitz (92 paragraphs)\n",
      "372. Endangered_Species_Act (31 paragraphs)\n",
      "373. Vacuum (30 paragraphs)\n",
      "374. Han_dynasty (65 paragraphs)\n",
      "375. Greeks (44 paragraphs)\n",
      "376. Quran (43 paragraphs)\n",
      "377. Great_power (21 paragraphs)\n",
      "378. Geography_of_the_United_States (21 paragraphs)\n",
      "379. Compact_disc (25 paragraphs)\n",
      "380. Transistor (23 paragraphs)\n",
      "381. Modern_history (88 paragraphs)\n",
      "382. 51st_state (32 paragraphs)\n",
      "383. Antenna_(radio) (74 paragraphs)\n",
      "384. Flowering_plant (23 paragraphs)\n",
      "385. Hyderabad (53 paragraphs)\n",
      "386. Santa_Monica,_California (22 paragraphs)\n",
      "387. Washington_University_in_St._Louis (35 paragraphs)\n",
      "388. Central_Intelligence_Agency (16 paragraphs)\n",
      "389. Pain (33 paragraphs)\n",
      "390. Database (32 paragraphs)\n",
      "391. Tucson,_Arizona (52 paragraphs)\n",
      "392. Armenia (57 paragraphs)\n",
      "393. Bacteria (51 paragraphs)\n",
      "394. Printed_circuit_board (35 paragraphs)\n",
      "395. Premier_League (40 paragraphs)\n",
      "396. Roman_Republic (82 paragraphs)\n",
      "397. Pacific_War (71 paragraphs)\n",
      "398. Richmond,_Virginia (61 paragraphs)\n",
      "399. San_Diego (48 paragraphs)\n",
      "400. Muslim_world (27 paragraphs)\n",
      "401. Iran (60 paragraphs)\n",
      "402. British_Isles (34 paragraphs)\n",
      "403. Association_football (28 paragraphs)\n",
      "404. Georgian_architecture (21 paragraphs)\n",
      "405. Liberia (23 paragraphs)\n",
      "406. Windows_8 (48 paragraphs)\n",
      "407. Swaziland (27 paragraphs)\n",
      "408. Translation (29 paragraphs)\n",
      "409. Airport (25 paragraphs)\n",
      "410. Kievan_Rus%27 (39 paragraphs)\n",
      "411. Super_Nintendo_Entertainment_System (24 paragraphs)\n",
      "412. Sumer (32 paragraphs)\n",
      "413. Tuvalu (61 paragraphs)\n",
      "414. Immaculate_Conception (22 paragraphs)\n",
      "415. Namibia (38 paragraphs)\n",
      "416. Russian_language (31 paragraphs)\n",
      "417. United_States_Air_Force (57 paragraphs)\n",
      "418. Light-emitting_diode (43 paragraphs)\n",
      "419. Bird (62 paragraphs)\n",
      "420. Qing_dynasty (81 paragraphs)\n",
      "421. Indigenous_peoples_of_the_Americas (56 paragraphs)\n",
      "422. Egypt (64 paragraphs)\n",
      "423. Mosaic (64 paragraphs)\n",
      "424. University (27 paragraphs)\n",
      "425. Religion_in_ancient_Rome (81 paragraphs)\n",
      "426. YouTube (34 paragraphs)\n",
      "427. Separation_of_church_and_state_in_the_United_States (41 paragraphs)\n",
      "428. Protestantism (75 paragraphs)\n",
      "429. Bras%C3%ADlia (23 paragraphs)\n",
      "430. Economy_of_Greece (29 paragraphs)\n",
      "431. Party_leaders_of_the_United_States_House_of_Representatives (31 paragraphs)\n",
      "432. Armenians (25 paragraphs)\n",
      "433. Jehovah%27s_Witnesses (46 paragraphs)\n",
      "434. Dwight_D._Eisenhower (90 paragraphs)\n",
      "435. The_Bronx (57 paragraphs)\n",
      "436. Humanism (30 paragraphs)\n",
      "437. Geological_history_of_Earth (26 paragraphs)\n",
      "438. Police (36 paragraphs)\n",
      "439. Punjab,_Pakistan (26 paragraphs)\n",
      "440. Infection (30 paragraphs)\n",
      "441. Hunting (36 paragraphs)\n",
      "442. Kathmandu (58 paragraphs)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train file:\")\n",
    "show_titles('../data/stanford-question-answering-dataset/train-v1.1.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a988669f",
   "metadata": {},
   "source": [
    "Choosing to use articles numbered 4 and 19 from dev, and 31, 63, 105, 125, 150, 158, 178, 230, 294, 296, 331, 334, 390, 394 from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87f86dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted = {}\n",
    "\n",
    "with open('../data/stanford-question-answering-dataset/dev-v1.1.json', 'r', encoding='utf-8') as f:\n",
    "    squad_data = json.load(f)\n",
    "all = squad_data['data']\n",
    "for i, article in enumerate(all):\n",
    "    if i==4 or i==19 :\n",
    "        wanted[article['title']] = article['paragraphs']\n",
    "\n",
    "with open('../data/stanford-question-answering-dataset/train-v1.1.json', 'r', encoding='utf-8') as f:\n",
    "    squad_data = json.load(f)\n",
    "all = squad_data['data']\n",
    "for i, article in enumerate(all):\n",
    "    if i in {30, 62, 104, 124, 149, 157, 177, 229, 293, 295, 330, 333, 389, 393} :\n",
    "        wanted[article['title']] = article['paragraphs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d9bdf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "article_dict = {}\n",
    "context_dict = defaultdict(str)\n",
    "rows = []\n",
    "article_id=0\n",
    "context_id=0\n",
    "for title, contexts in wanted.items():\n",
    "    article_dict[article_id] = title\n",
    "    for i, element in enumerate(contexts):\n",
    "        # for getting paragraphs alone\n",
    "        # context_dict[context_id]=element.get('context')\n",
    "        # for getting whole article text\n",
    "        context_dict[article_id]+=element.get('context')\n",
    "        qas = element.get('qas', [])\n",
    "        for qa in qas:\n",
    "            row = {\n",
    "                'article_id': article_id,\n",
    "                'context_id': context_id,\n",
    "                'question_id': qa.get('id'),\n",
    "                'question': qa.get('question'),\n",
    "                'answers(list of dict)': qa.get('answers') # leave this for later i guess\n",
    "            }\n",
    "            rows.append(row)\n",
    "        context_id+=1\n",
    "    article_id+=1\n",
    "\n",
    "result_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a85e399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>context_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answers(list of dict)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56e16182e3433e1400422e28</td>\n",
       "      <td>What branch of theoretical computer science de...</td>\n",
       "      <td>[{'answer_start': 0, 'text': 'Computational co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56e16182e3433e1400422e29</td>\n",
       "      <td>By what main attribute are computational probl...</td>\n",
       "      <td>[{'answer_start': 175, 'text': 'inherent diffi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56e16182e3433e1400422e2a</td>\n",
       "      <td>What is the term for a task that generally len...</td>\n",
       "      <td>[{'answer_start': 133, 'text': 'computational ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>56e16839cd28a01900c67887</td>\n",
       "      <td>What measure of a computational problem broadl...</td>\n",
       "      <td>[{'answer_start': 46, 'text': 'if its solution...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>56e16839cd28a01900c67888</td>\n",
       "      <td>What method is used to intuitively assess or q...</td>\n",
       "      <td>[{'answer_start': 176, 'text': 'mathematical m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>15</td>\n",
       "      <td>579</td>\n",
       "      <td>57302364b2c2fd1400568927</td>\n",
       "      <td>What type of construction was originally used ...</td>\n",
       "      <td>[{'answer_start': 240, 'text': 'through-hole'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>15</td>\n",
       "      <td>579</td>\n",
       "      <td>57302364b2c2fd1400568928</td>\n",
       "      <td>In what year was the Auto-Sembly process created?</td>\n",
       "      <td>[{'answer_start': 270, 'text': '1949'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>15</td>\n",
       "      <td>579</td>\n",
       "      <td>57302364b2c2fd140056892a</td>\n",
       "      <td>What year was the patent for the Auto-Sembly p...</td>\n",
       "      <td>[{'answer_start': 514, 'text': '1956'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>15</td>\n",
       "      <td>579</td>\n",
       "      <td>57302364b2c2fd1400568929</td>\n",
       "      <td>Although two men developed the Auto-Sembly pro...</td>\n",
       "      <td>[{'answer_start': 539, 'text': 'U.S. Army'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2264</th>\n",
       "      <td>15</td>\n",
       "      <td>579</td>\n",
       "      <td>57302364b2c2fd140056892b</td>\n",
       "      <td>Which part of the Auto-Sembly manufacturing pr...</td>\n",
       "      <td>[{'answer_start': 887, 'text': 'drilling holes'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2265 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      article_id  context_id               question_id  \\\n",
       "0              0           0  56e16182e3433e1400422e28   \n",
       "1              0           0  56e16182e3433e1400422e29   \n",
       "2              0           0  56e16182e3433e1400422e2a   \n",
       "3              0           1  56e16839cd28a01900c67887   \n",
       "4              0           1  56e16839cd28a01900c67888   \n",
       "...          ...         ...                       ...   \n",
       "2260          15         579  57302364b2c2fd1400568927   \n",
       "2261          15         579  57302364b2c2fd1400568928   \n",
       "2262          15         579  57302364b2c2fd140056892a   \n",
       "2263          15         579  57302364b2c2fd1400568929   \n",
       "2264          15         579  57302364b2c2fd140056892b   \n",
       "\n",
       "                                               question  \\\n",
       "0     What branch of theoretical computer science de...   \n",
       "1     By what main attribute are computational probl...   \n",
       "2     What is the term for a task that generally len...   \n",
       "3     What measure of a computational problem broadl...   \n",
       "4     What method is used to intuitively assess or q...   \n",
       "...                                                 ...   \n",
       "2260  What type of construction was originally used ...   \n",
       "2261  In what year was the Auto-Sembly process created?   \n",
       "2262  What year was the patent for the Auto-Sembly p...   \n",
       "2263  Although two men developed the Auto-Sembly pro...   \n",
       "2264  Which part of the Auto-Sembly manufacturing pr...   \n",
       "\n",
       "                                  answers(list of dict)  \n",
       "0     [{'answer_start': 0, 'text': 'Computational co...  \n",
       "1     [{'answer_start': 175, 'text': 'inherent diffi...  \n",
       "2     [{'answer_start': 133, 'text': 'computational ...  \n",
       "3     [{'answer_start': 46, 'text': 'if its solution...  \n",
       "4     [{'answer_start': 176, 'text': 'mathematical m...  \n",
       "...                                                 ...  \n",
       "2260    [{'answer_start': 240, 'text': 'through-hole'}]  \n",
       "2261            [{'answer_start': 270, 'text': '1949'}]  \n",
       "2262            [{'answer_start': 514, 'text': '1956'}]  \n",
       "2263       [{'answer_start': 539, 'text': 'U.S. Army'}]  \n",
       "2264  [{'answer_start': 887, 'text': 'drilling holes'}]  \n",
       "\n",
       "[2265 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "090af322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Computational_complexity_theory',\n",
       " 1: 'Packet_switching',\n",
       " 2: 'Wayback_Machine',\n",
       " 3: 'Web_browser',\n",
       " 4: 'Computer',\n",
       " 5: 'Computer_security',\n",
       " 6: 'ASCII',\n",
       " 7: 'Macintosh',\n",
       " 8: 'Memory',\n",
       " 9: 'Data_compression',\n",
       " 10: 'USB',\n",
       " 11: 'Unicode',\n",
       " 12: 'IBM',\n",
       " 13: 'Software_testing',\n",
       " 14: 'Database',\n",
       " 15: 'Printed_circuit_board'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05de89b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(str,\n",
       "            {0: 'Computational complexity theory is a branch of the theory of computation in theoretical computer science that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each other. A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an algorithm.A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying the amount of resources needed to solve them, such as time and storage. Other complexity measures are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do.Closely related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, it tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kind of problems can, in principle, be solved algorithmically.A computational problem can be viewed as an infinite collection of instances together with a solution for every instance. The input string for a computational problem is referred to as a problem instance, and should not be confused with the problem itself. In computational complexity theory, a problem refers to the abstract question to be solved. In contrast, an instance of this problem is a rather concrete utterance, which can serve as the input for a decision problem. For example, consider the problem of primality testing. The instance is a number (e.g. 15) and the solution is \"yes\" if the number is prime and \"no\" otherwise (in this case \"no\"). Stated another way, the instance is a particular input to the problem, and the solution is the output corresponding to the given input.To further highlight the difference between a problem and an instance, consider the following instance of the decision version of the traveling salesman problem: Is there a route of at most 2000 kilometres passing through all of Germany\\'s 15 largest cities? The quantitative answer to this particular problem instance is of little use for solving other instances of the problem, such as asking for a round trip through all sites in Milan whose total length is at most 10 km. For this reason, complexity theory addresses computational problems and not particular problem instances.When considering computational problems, a problem instance is a string over an alphabet. Usually, the alphabet is taken to be the binary alphabet (i.e., the set {0,1}), and thus the strings are bitstrings. As in a real-world computer, mathematical objects other than bitstrings must be suitably encoded. For example, integers can be represented in binary notation, and graphs can be encoded directly via their adjacency matrices, or by encoding their adjacency lists in binary.Decision problems are one of the central objects of study in computational complexity theory. A decision problem is a special type of computational problem whose answer is either yes or no, or alternately either 1 or 0. A decision problem can be viewed as a formal language, where the members of the language are instances whose output is yes, and the non-members are those instances whose output is no. The objective is to decide, with the aid of an algorithm, whether a given input string is a member of the formal language under consideration. If the algorithm deciding this problem returns the answer yes, the algorithm is said to accept the input string, otherwise it is said to reject the input.An example of a decision problem is the following. The input is an arbitrary graph. The problem consists in deciding whether the given graph is connected, or not. The formal language associated with this decision problem is then the set of all connected graphs—of course, to obtain a precise definition of this language, one has to decide how graphs are encoded as binary strings.A function problem is a computational problem where a single output (of a total function) is expected for every input, but the output is more complex than that of a decision problem, that is, it isn\\'t just yes or no. Notable examples include the traveling salesman problem and the integer factorization problem.It is tempting to think that the notion of function problems is much richer than the notion of decision problems. However, this is not really the case, since function problems can be recast as decision problems. For example, the multiplication of two integers can be expressed as the set of triples (a, b, c) such that the relation a × b = c holds. Deciding whether a given triple is a member of this set corresponds to solving the problem of multiplying two numbers.To measure the difficulty of solving a computational problem, one may wish to see how much time the best algorithm requires to solve the problem. However, the running time may, in general, depend on the instance. In particular, larger instances will require more time to solve. Thus the time required to solve a problem (or the space required, or any measure of complexity) is calculated as a function of the size of the instance. This is usually taken to be the size of the input in bits. Complexity theory is interested in how algorithms scale with an increase in the input size. For instance, in the problem of finding whether a graph is connected, how much more time does it take to solve a problem for a graph with 2n vertices compared to the time taken for a graph with n vertices?If the input size is n, the time taken can be expressed as a function of n. Since the time taken on different inputs of the same size can be different, the worst-case time complexity T(n) is defined to be the maximum time taken over all inputs of size n. If T(n) is a polynomial in n, then the algorithm is said to be a polynomial time algorithm. Cobham\\'s thesis says that a problem can be solved with a feasible amount of resources if it admits a polynomial time algorithm.A Turing machine is a mathematical model of a general computing machine. It is a theoretical device that manipulates symbols contained on a strip of tape. Turing machines are not intended as a practical computing technology, but rather as a thought experiment representing a computing machine—anything from an advanced supercomputer to a mathematician with a pencil and paper. It is believed that if a problem can be solved by an algorithm, there exists a Turing machine that solves the problem. Indeed, this is the statement of the Church–Turing thesis. Furthermore, it is known that everything that can be computed on other models of computation known to us today, such as a RAM machine, Conway\\'s Game of Life, cellular automata or any programming language can be computed on a Turing machine. Since Turing machines are easy to analyze mathematically, and are believed to be as powerful as any other model of computation, the Turing machine is the most commonly used model in complexity theory.A deterministic Turing machine is the most basic Turing machine, which uses a fixed set of rules to determine its future actions. A probabilistic Turing machine is a deterministic Turing machine with an extra supply of random bits. The ability to make probabilistic decisions often helps algorithms solve problems more efficiently. Algorithms that use random bits are called randomized algorithms. A non-deterministic Turing machine is a deterministic Turing machine with an added feature of non-determinism, which allows a Turing machine to have multiple possible future actions from a given state. One way to view non-determinism is that the Turing machine branches into many possible computational paths at each step, and if it solves the problem in any of these branches, it is said to have solved the problem. Clearly, this model is not meant to be a physically realizable model, it is just a theoretically interesting abstract machine that gives rise to particularly interesting complexity classes. For examples, see non-deterministic algorithm.Many types of Turing machines are used to define complexity classes, such as deterministic Turing machines, probabilistic Turing machines, non-deterministic Turing machines, quantum Turing machines, symmetric Turing machines and alternating Turing machines. They are all equally powerful in principle, but when resources (such as time or space) are bounded, some of these may be more powerful than others.Many machine models different from the standard multi-tape Turing machines have been proposed in the literature, for example random access machines. Perhaps surprisingly, each of these models can be converted to another without providing any extra computational power. The time and memory consumption of these alternate models may vary. What all these models have in common is that the machines operate deterministically.However, some computational problems are easier to analyze in terms of more unusual resources. For example, a non-deterministic Turing machine is a computational model that is allowed to branch out to check many different possibilities at once. The non-deterministic Turing machine has very little to do with how we physically want to compute algorithms, but its branching exactly captures many of the mathematical models we want to analyze, so that non-deterministic time is a very important resource in analyzing computational problems.For a precise definition of what it means to solve a problem using a given amount of time and space, a computational model such as the deterministic Turing machine is used. The time required by a deterministic Turing machine M on input x is the total number of state transitions, or steps, the machine makes before it halts and outputs the answer (\"yes\" or \"no\"). A Turing machine M is said to operate within time f(n), if the time required by M on each input of length n is at most f(n). A decision problem A can be solved in time f(n) if there exists a Turing machine operating in time f(n) that solves the problem. Since complexity theory is interested in classifying problems based on their difficulty, one defines sets of problems based on some criteria. For instance, the set of problems solvable within time f(n) on a deterministic Turing machine is then denoted by DTIME(f(n)).Analogous definitions can be made for space requirements. Although time and space are the most well-known complexity resources, any complexity measure can be viewed as a computational resource. Complexity measures are very generally defined by the Blum complexity axioms. Other complexity measures used in complexity theory include communication complexity, circuit complexity, and decision tree complexity.The best, worst and average case complexity refer to three different ways of measuring the time complexity (or any other complexity measure) of different inputs of the same size. Since some inputs of size n may be faster to solve than others, we define the following complexities:For example, consider the deterministic sorting algorithm quicksort. This solves the problem of sorting a list of integers that is given as the input. The worst-case is when the input is sorted or sorted in reverse order, and the algorithm takes time O(n2) for this case. If we assume that all possible permutations of the input list are equally likely, the average time taken for sorting is O(n log n). The best case occurs when each pivoting divides the list in half, also needing O(n log n) time.To classify the computation time (or similar resources, such as space consumption), one is interested in proving upper and lower bounds on the minimum amount of time required by the most efficient algorithm solving a given problem. The complexity of an algorithm is usually taken to be its worst-case complexity, unless specified otherwise. Analyzing a particular algorithm falls under the field of analysis of algorithms. To show an upper bound T(n) on the time complexity of a problem, one needs to show only that there is a particular algorithm with running time at most T(n). However, proving lower bounds is much more difficult, since lower bounds make a statement about all possible algorithms that solve a given problem. The phrase \"all possible algorithms\" includes not just the algorithms known today, but any algorithm that might be discovered in the future. To show a lower bound of T(n) for a problem requires showing that no algorithm can have time complexity lower than T(n).Upper and lower bounds are usually stated using the big O notation, which hides constant factors and smaller terms. This makes the bounds independent of the specific details of the computational model used. For instance, if T(n) = 7n2 + 15n + 40, in big O notation one would write T(n) = O(n2).Of course, some complexity classes have complicated definitions that do not fit into this framework. Thus, a typical complexity class has a definition like the following:But bounding the computation time above by some concrete function f(n) often yields complexity classes that depend on the chosen machine model. For instance, the language {xx | x is any binary string} can be solved in linear time on a multi-tape Turing machine, but necessarily requires quadratic time in the model of single-tape Turing machines. If we allow polynomial variations in running time, Cobham-Edmonds thesis states that \"the time complexities in any two reasonable and general models of computation are polynomially related\" (Goldreich 2008, Chapter 1.2). This forms the basis for the complexity class P, which is the set of decision problems solvable by a deterministic Turing machine within polynomial time. The corresponding set of function problems is FP.Many important complexity classes can be defined by bounding the time or space used by the algorithm. Some important complexity classes of decision problems defined in this manner are the following:Other important complexity classes include BPP, ZPP and RP, which are defined using probabilistic Turing machines; AC and NC, which are defined using Boolean circuits; and BQP and QMA, which are defined using quantum Turing machines. #P is an important complexity class of counting problems (not decision problems). Classes like IP and AM are defined using Interactive proof systems. ALL is the class of all decision problems.For the complexity classes defined in this way, it is desirable to prove that relaxing the requirements on (say) computation time indeed defines a bigger set of problems. In particular, although DTIME(n) is contained in DTIME(n2), it would be interesting to know if the inclusion is strict. For time and space requirements, the answer to such questions is given by the time and space hierarchy theorems respectively. They are called hierarchy theorems because they induce a proper hierarchy on the classes defined by constraining the respective resources. Thus there are pairs of complexity classes such that one is properly included in the other. Having deduced such proper set inclusions, we can proceed to make quantitative statements about how much more additional time or space is needed in order to increase the number of problems that can be solved.The time and space hierarchy theorems form the basis for most separation results of complexity classes. For instance, the time hierarchy theorem tells us that P is strictly contained in EXPTIME, and the space hierarchy theorem tells us that L is strictly contained in PSPACE.Many complexity classes are defined using the concept of a reduction. A reduction is a transformation of one problem into another problem. It captures the informal notion of a problem being at least as difficult as another problem. For instance, if a problem X can be solved using an algorithm for Y, X is no more difficult than Y, and we say that X reduces to Y. There are many different types of reductions, based on the method of reduction, such as Cook reductions, Karp reductions and Levin reductions, and the bound on the complexity of reductions, such as polynomial-time reductions or log-space reductions.The most commonly used reduction is a polynomial-time reduction. This means that the reduction process takes polynomial time. For example, the problem of squaring an integer can be reduced to the problem of multiplying two integers. This means an algorithm for multiplying two integers can be used to square an integer. Indeed, this can be done by giving the same input to both inputs of the multiplication algorithm. Thus we see that squaring is not more difficult than multiplication, since squaring can be reduced to multiplication.This motivates the concept of a problem being hard for a complexity class. A problem X is hard for a class of problems C if every problem in C can be reduced to X. Thus no problem in C is harder than X, since an algorithm for X allows us to solve any problem in C. Of course, the notion of hard problems depends on the type of reduction being used. For complexity classes larger than P, polynomial-time reductions are commonly used. In particular, the set of problems that are hard for NP is the set of NP-hard problems.If a problem X is in C and hard for C, then X is said to be complete for C. This means that X is the hardest problem in C. (Since many problems could be equally hard, one might say that X is one of the hardest problems in C.) Thus the class of NP-complete problems contains the most difficult problems in NP, in the sense that they are the ones most likely not to be in P. Because the problem P = NP is not solved, being able to reduce a known NP-complete problem, Π2, to another problem, Π1, would indicate that there is no known polynomial-time solution for Π1. This is because a polynomial-time solution to Π1 would yield a polynomial-time solution to Π2. Similarly, because all NP problems can be reduced to the set, finding an NP-complete problem that can be solved in polynomial time would mean that P = NP.The complexity class P is often seen as a mathematical abstraction modeling those computational tasks that admit an efficient algorithm. This hypothesis is called the Cobham–Edmonds thesis. The complexity class NP, on the other hand, contains many problems that people would like to solve efficiently, but for which no efficient algorithm is known, such as the Boolean satisfiability problem, the Hamiltonian path problem and the vertex cover problem. Since deterministic Turing machines are special non-deterministic Turing machines, it is easily observed that each problem in P is also member of the class NP.The question of whether P equals NP is one of the most important open questions in theoretical computer science because of the wide implications of a solution. If the answer is yes, many important problems can be shown to have more efficient solutions. These include various types of integer programming problems in operations research, many problems in logistics, protein structure prediction in biology, and the ability to find formal proofs of pure mathematics theorems. The P versus NP problem is one of the Millennium Prize Problems proposed by the Clay Mathematics Institute. There is a US$1,000,000 prize for resolving the problem.It was shown by Ladner that if P ≠ NP then there exist problems in NP that are neither in P nor NP-complete. Such problems are called NP-intermediate problems. The graph isomorphism problem, the discrete logarithm problem and the integer factorization problem are examples of problems believed to be NP-intermediate. They are some of the very few NP problems not known to be in P or to be NP-complete.The graph isomorphism problem is the computational problem of determining whether two finite graphs are isomorphic. An important unsolved problem in complexity theory is whether the graph isomorphism problem is in P, NP-complete, or NP-intermediate. The answer is not known, but it is believed that the problem is at least not NP-complete. If graph isomorphism is NP-complete, the polynomial time hierarchy collapses to its second level. Since it is widely believed that the polynomial hierarchy does not collapse to any finite level, it is believed that graph isomorphism is not NP-complete. The best algorithm for this problem, due to Laszlo Babai and Eugene Luks has run time 2O(√(n log(n))) for graphs with n vertices.The integer factorization problem is the computational problem of determining the prime factorization of a given integer. Phrased as a decision problem, it is the problem of deciding whether the input has a factor less than k. No efficient integer factorization algorithm is known, and this fact forms the basis of several modern cryptographic systems, such as the RSA algorithm. The integer factorization problem is in NP and in co-NP (and even in UP and co-UP). If the problem is NP-complete, the polynomial time hierarchy will collapse to its first level (i.e., NP will equal co-NP). The best known algorithm for integer factorization is the general number field sieve, which takes time O(e(64/9)1/3(n.log 2)1/3(log (n.log 2))2/3) to factor an n-bit integer. However, the best known quantum algorithm for this problem, Shor\\'s algorithm, does run in polynomial time. Unfortunately, this fact doesn\\'t say much about where the problem lies with respect to non-quantum complexity classes.Many known complexity classes are suspected to be unequal, but this has not been proved. For instance P ⊆ NP ⊆ PP ⊆ PSPACE, but it is possible that P = PSPACE. If P is not equal to NP, then P is not equal to PSPACE either. Since there are many known complexity classes between P and PSPACE, such as RP, BPP, PP, BQP, MA, PH, etc., it is possible that all these complexity classes collapse to one class. Proving that any of these classes are unequal would be a major breakthrough in complexity theory.Along the same lines, co-NP is the class containing the complement problems (i.e. problems with the yes/no answers reversed) of NP problems. It is believed that NP is not equal to co-NP; however, it has not yet been proven. It has been shown that if these two complexity classes are not equal then P is not equal to NP.Similarly, it is not known if L (the set of all problems that can be solved in logarithmic space) is strictly contained in P or equal to P. Again, there are many complexity classes between the two, such as NL and NC, and it is not known if they are distinct or equal classes.Problems that can be solved in theory (e.g., given large but finite time), but which in practice take too long for their solutions to be useful, are known as intractable problems. In complexity theory, problems that lack polynomial-time solutions are considered to be intractable for more than the smallest inputs. In fact, the Cobham–Edmonds thesis states that only those problems that can be solved in polynomial time can be feasibly computed on some computational device. Problems that are known to be intractable in this sense include those that are EXPTIME-hard. If NP is not the same as P, then the NP-complete problems are also intractable in this sense. To see why exponential-time algorithms might be unusable in practice, consider a program that makes 2n operations before halting. For small n, say 100, and assuming for the sake of example that the computer does 1012 operations each second, the program would run for about 4 × 1010 years, which is the same order of magnitude as the age of the universe. Even with a much faster computer, the program would only be useful for very small instances and in that sense the intractability of a problem is somewhat independent of technological progress. Nevertheless, a polynomial time algorithm is not always practical. If its running time is, say, n15, it is unreasonable to consider it efficient and it is still useless except on small instances.What intractability means in practice is open to debate. Saying that a problem is not in P does not imply that all large cases of the problem are hard or even that most of them are. For example, the decision problem in Presburger arithmetic has been shown not to be in P, yet algorithms have been written that solve the problem in reasonable times in most cases. Similarly, algorithms can solve the NP-complete knapsack problem over a wide range of sizes in less than quadratic time and SAT solvers routinely handle large instances of the NP-complete Boolean satisfiability problem.Before the actual research explicitly devoted to the complexity of algorithmic problems started off, numerous foundations were laid out by various researchers. Most influential among these was the definition of Turing machines by Alan Turing in 1936, which turned out to be a very robust and flexible simplification of a computer.As Fortnow & Homer (2003) point out, the beginning of systematic studies in computational complexity is attributed to the seminal paper \"On the Computational Complexity of Algorithms\" by Juris Hartmanis and Richard Stearns (1965), which laid out the definitions of time and space complexity and proved the hierarchy theorems. Also, in 1965 Edmonds defined a \"good\" algorithm as one with running time bounded by a polynomial of the input size.Earlier papers studying problems solvable by Turing machines with specific bounded resources include  John Myhill\\'s definition of linear bounded automata (Myhill 1960), Raymond Smullyan\\'s study of rudimentary sets (1961), as well as Hisao Yamada\\'s paper on real-time computations (1962). Somewhat earlier, Boris Trakhtenbrot (1956), a pioneer in the field from the USSR, studied another specific complexity measure. As he remembers:Even though some proofs of complexity-theoretic theorems regularly assume some concrete choice of input encoding, one tries to keep the discussion abstract enough to be independent of the choice of encoding. This can be achieved by ensuring that different representations can be transformed into each other efficiently.In 1967, Manuel Blum developed an axiomatic complexity theory based on his axioms and proved an important result, the so-called, speed-up theorem. The field really began to flourish in 1971 when the US researcher Stephen Cook and, working independently, Leonid Levin in the USSR, proved that there exist practically relevant problems that are NP-complete. In 1972, Richard Karp took this idea a leap forward with his landmark paper, \"Reducibility Among Combinatorial Problems\", in which he showed that 21 diverse combinatorial and graph theoretical problems, each infamous for its computational intractability, are NP-complete.',\n",
       "             1: \"Starting in the late 1950s, American computer scientist Paul Baran developed the concept Distributed Adaptive Message Block Switching with the goal to provide a fault-tolerant, efficient routing method for telecommunication messages as part of a research program at the RAND Corporation, funded by the US Department of Defense. This concept contrasted and contradicted the theretofore established principles of pre-allocation of network bandwidth, largely fortified by the development of telecommunications in the Bell System. The new concept found little resonance among network implementers until the independent work of Donald Davies at the National Physical Laboratory (United Kingdom) (NPL) in the late 1960s. Davies is credited with coining the modern name packet switching and inspiring numerous packet switching networks in Europe in the decade following, including the incorporation of the concept in the early ARPANET in the United States.Packet switching contrasts with another principal networking paradigm, circuit switching, a method which pre-allocates dedicated network bandwidth specifically for each communication session, each having a constant bit rate and latency between nodes. In cases of billable services, such as cellular communication services, circuit switching is characterized by a fee per unit of connection time, even when no data is transferred, while packet switching may be characterized by a fee per unit of information transmitted, such as characters, packets, or messages.Packet mode communication may be implemented with or without intermediate forwarding nodes (packet switches or routers). Packets are normally forwarded by intermediate network nodes asynchronously using first-in, first-out buffering, but may be forwarded according to some scheduling discipline for fair queuing, traffic shaping, or for differentiated or guaranteed quality of service, such as weighted fair queuing or leaky bucket. In case of a shared physical medium (such as radio or 10BASE5), the packets may be delivered according to a multiple access scheme.Baran developed the concept of distributed adaptive message block switching during his research at the RAND Corporation for the US Air Force into survivable communications networks, first presented to the Air Force in the summer of 1961 as briefing B-265, later published as RAND report P-2626 in 1962, and finally in report RM 3420 in 1964. Report P-2626 described a general architecture for a large-scale, distributed, survivable communications network. The work focuses on three key ideas: use of a decentralized network with multiple paths between any two points, dividing user messages into message blocks, later called packets, and delivery of these messages by store and forward switching.Starting in 1965, Donald Davies at the National Physical Laboratory, UK, independently developed the same message routing methodology as developed by Baran. He called it packet switching, a more accessible name than Baran's, and proposed to build a nationwide network in the UK. He gave a talk on the proposal in 1966, after which a person from the Ministry of Defence (MoD) told him about Baran's work. A member of Davies' team (Roger Scantlebury) met Lawrence Roberts at the 1967 ACM Symposium on Operating System Principles and suggested it for use in the ARPANET.In connectionless mode each packet includes complete addressing information. The packets are routed individually, sometimes resulting in different paths and out-of-order delivery. Each packet is labeled with a destination address, source address, and port numbers. It may also be labeled with the sequence number of the packet. This precludes the need for a dedicated path to help the packet find its way to its destination, but means that much more information is needed in the packet header, which is therefore larger, and this information needs to be looked up in power-hungry content-addressable memory. Each packet is dispatched and may go via different routes; potentially, the system has to do as much work for every packet as the connection-oriented system has to do in connection set-up, but with less information as to the application's requirements. At the destination, the original message/data is reassembled in the correct order, based on the packet sequence number. Thus a virtual connection, also known as a virtual circuit or byte stream is provided to the end-user by a transport layer protocol, although intermediate network nodes only provides a connectionless network layer service.Connection-oriented transmission requires a setup phase in each involved node before any packet is transferred to establish the parameters of communication. The packets include a connection identifier rather than address information and are negotiated between endpoints so that they are delivered in order and with error checking. Address information is only transferred to each node during the connection set-up phase, when the route to the destination is discovered and an entry is added to the switching table in each network node through which the connection passes. The signaling protocols used allow the application to specify its requirements and discover link parameters. Acceptable values for service parameters may be negotiated. Routing a packet requires the node to look up the connection id in a table. The packet header can be small, as it only needs to contain this code and any information, such as length, timestamp, or sequence number, which is different for different packets.Both X.25 and Frame Relay provide connection-oriented operations. But X.25 does it at the network layer of the OSI Model. Frame Relay does it at level two, the data link layer. Another major difference between X.25 and Frame Relay is that X.25 requires a handshake between the communicating parties before any user packets are transmitted. Frame Relay does not define any such handshakes. X.25 does not define any operations inside the packet network. It only operates at the user-network-interface (UNI). Thus, the network provider is free to use any procedure it wishes inside the network. X.25 does specify some limited re-transmission procedures at the UNI, and its link layer protocol (LAPB) provides conventional HDLC-type link management procedures. Frame Relay is a modified version of ISDN's layer two protocol, LAPD and LAPB. As such, its integrity operations pertain only between nodes on a link, not end-to-end. Any retransmissions must be carried out by higher layer protocols. The X.25 UNI protocol is part of the X.25 protocol suite, which consists of the lower three layers of the OSI Model. It was widely used at the UNI for packet switching networks during the 1980s and early 1990s, to provide a standardized interface into and out of packet networks. Some implementations used X.25 within the network as well, but its connection-oriented features made this setup cumbersome and inefficient. Frame relay operates principally at layer two of the OSI Model. However, its address field (the Data Link Connection ID, or DLCI) can be used at the OSI network layer, with a minimum set of procedures. Thus, it rids itself of many X.25 layer 3 encumbrances, but still has the DLCI as an ID beyond a node-to-node layer two link protocol. The simplicity of Frame Relay makes it faster and more efficient than X.25. Because Frame relay is a data link layer protocol, like X.25 it does not define internal network routing operations. For X.25 its packet IDs---the virtual circuit and virtual channel numbers have to be correlated to network addresses. The same is true for Frame Relays DLCI. How this is done is up to the network provider. Frame Relay, by virtue of having no network layer procedures is connection-oriented at layer two, by using the HDLC/LAPD/LAPB Set Asynchronous Balanced Mode (SABM). X.25 connections are typically established for each communication session, but it does have a feature allowing a limited amount of traffic to be passed across the UNI without the connection-oriented handshake. For a while, Frame Relay was used to interconnect LANs across wide area networks. However, X.25 and well as Frame Relay have been supplanted by the Internet Protocol (IP) at the network layer, and the Asynchronous Transfer Mode (ATM) and or versions of Multi-Protocol Label Switching (MPLS) at layer two. A typical configuration is to run IP over ATM or a version of MPLS. <Uyless Black, X.25 and Related Protocols, IEEE Computer Society, 1991> <Uyless Black, Frame Relay Networks, McGraw-Hill, 1998> <Uyless Black, MPLS and Label Switching Networks, Prentice Hall, 2001> < Uyless Black, ATM, Volume I, Prentice Hall, 1995>ARPANET and SITA HLN became operational in 1969. Before the introduction of X.25 in 1973, about twenty different network technologies had been developed. Two fundamental differences involved the division of functions and tasks between the hosts at the edge of the network and the network core. In the datagram system, the hosts have the responsibility to ensure orderly delivery of packets. The User Datagram Protocol (UDP) is an example of a datagram protocol. In the virtual call system, the network guarantees sequenced delivery of data to the host. This results in a simpler host interface with less functionality than in the datagram model. The X.25 protocol suite uses this network type.AppleTalk was a proprietary suite of networking protocols developed by Apple Inc. in 1985 for Apple Macintosh computers. It was the primary protocol used by Apple devices through the 1980s and 90s. AppleTalk included features that allowed local area networks to be established ad hoc without the requirement for a centralized router or server. The AppleTalk system automatically assigned addresses, updated the distributed namespace, and configured any required inter-network routing. It was a plug-n-play system.The CYCLADES packet switching network was a French research network designed and directed by Louis Pouzin. First demonstrated in 1973, it was developed to explore alternatives to the early ARPANET design and to support network research generally. It was the first network to make the hosts responsible for reliable delivery of data, rather than the network itself, using unreliable datagrams and associated end-to-end protocol mechanisms. Concepts of this network influenced later ARPANET architecture.DECnet is a suite of network protocols created by Digital Equipment Corporation, originally released in 1975 in order to connect two PDP-11 minicomputers. It evolved into one of the first peer-to-peer network architectures, thus transforming DEC into a networking powerhouse in the 1980s. Initially built with three layers, it later (1982) evolved into a seven-layer OSI-compliant networking protocol. The DECnet protocols were designed entirely by Digital Equipment Corporation. However, DECnet Phase II (and later) were open standards with published specifications, and several implementations were developed outside DEC, including one for Linux.In 1965, at the instigation of Warner Sinback, a data network based on this voice-phone network was designed to connect GE's four computer sales and service centers (Schenectady, Phoenix, Chicago, and Phoenix) to facilitate a computer time-sharing service, apparently the world's first commercial online service. (In addition to selling GE computers, the centers were computer service bureaus, offering batch processing services. They lost money from the beginning, and Sinback, a high-level marketing manager, was given the job of turning the business around. He decided that a time-sharing system, based on Kemney's work at Dartmouth—which used a computer on loan from GE—could be profitable. Warner was right.)Merit Network, Inc., an independent non-profit 501(c)(3) corporation governed by Michigan's public universities, was formed in 1966 as the Michigan Educational Research Information Triad to explore computer networking between three of Michigan's public universities as a means to help the state's educational and economic development. With initial support from the State of Michigan and the National Science Foundation (NSF), the packet-switched network was first demonstrated in December 1971 when an interactive host to host connection was made between the IBM mainframe computer systems at the University of Michigan in Ann Arbor and Wayne State University in Detroit. In October 1972 connections to the CDC mainframe at Michigan State University in East Lansing completed the triad. Over the next several years in addition to host to host interactive connections the network was enhanced to support terminal to host connections, host to host batch connections (remote job submission, remote printing, batch file transfer), interactive file transfer, gateways to the Tymnet and Telenet public data networks, X.25 host attachments, gateways to X.25 data networks, Ethernet attached hosts, and eventually TCP/IP and additional public universities in Michigan join the network. All of this set the stage for Merit's role in the NSFNET project starting in the mid-1980s.Telenet was the first FCC-licensed public data network in the United States. It was founded by former ARPA IPTO director Larry Roberts as a means of making ARPANET technology public. He had tried to interest AT&T in buying the technology, but the monopoly's reaction was that this was incompatible with their future. Bolt, Beranack and Newman (BBN) provided the financing. It initially used ARPANET technology but changed the host interface to X.25 and the terminal interface to X.29. Telenet designed these protocols and helped standardize them in the CCITT. Telenet was incorporated in 1973 and started operations in 1975. It went public in 1979 and was then sold to GTE.Tymnet was an international data communications network headquartered in San Jose, CA that utilized virtual call packet switched technology and used X.25, SNA/SDLC, BSC and ASCII interfaces to connect host computers (servers)at thousands of large companies, educational institutions, and government agencies. Users typically connected via dial-up connections or dedicated async connections. The business consisted of a large public network that supported dial-up users and a private network business that allowed government agencies and large companies (mostly banks and airlines) to build their own dedicated networks. The private networks were often connected via gateways to the public network to reach locations not on the private network. Tymnet was also connected to dozens of other public networks in the U.S. and internationally via X.25/X.75 gateways. (Interesting note: Tymnet was not named after Mr. Tyme. Another employee suggested the name.)  There were two kinds of X.25 networks. Some such as DATAPAC and TRANSPAC were initially implemented with an X.25 external interface. Some older networks such as TELENET and TYMNET were modified to provide a X.25 host interface in addition to older host connection schemes. DATAPAC was developed by Bell Northern Research which was a joint venture of Bell Canada (a common carrier) and Northern Telecom (a telecommunications equipment supplier). Northern Telecom sold several DATAPAC clones to foreign PTTs including the Deutsche Bundespost. X.75 and X.121 allowed the interconnection of national X.25 networks. A user or host could call a host on a foreign network by including the DNIC of the remote network as part of the destination address.[citation needed]AUSTPAC was an Australian public X.25 network operated by Telstra. Started by Telecom Australia in the early 1980s, AUSTPAC was Australia's first public packet-switched data network, supporting applications such as on-line betting, financial applications — the Australian Tax Office made use of AUSTPAC — and remote terminal access to academic institutions, who maintained their connections to AUSTPAC up until the mid-late 1990s in some cases. Access can be via a dial-up terminal to a PAD, or, by linking a permanent X.25 node to the network.[citation needed]Datanet 1 was the public switched data network operated by the Dutch PTT Telecom (now known as KPN). Strictly speaking Datanet 1 only referred to the network and the connected users via leased lines (using the X.121 DNIC 2041), the name also referred to the public PAD service Telepad (using the DNIC 2049). And because the main Videotex service used the network and modified PAD devices as infrastructure the name Datanet 1 was used for these services as well. Although this use of the name was incorrect all these services were managed by the same people within one department of KPN contributed to the confusion.The Computer Science Network (CSNET) was a computer network funded by the U.S. National Science Foundation (NSF) that began operation in 1981. Its purpose was to extend networking benefits, for computer science departments at academic and research institutions that could not be directly connected to ARPANET, due to funding or authorization limitations. It played a significant role in spreading awareness of, and access to, national networking and was a major milestone on the path to development of the global Internet.Internet2 is a not-for-profit United States computer networking consortium led by members from the research and education communities, industry, and government. The Internet2 community, in partnership with Qwest, built the first Internet2 Network, called Abilene, in 1998 and was a prime investor in the National LambdaRail (NLR) project. In 2006, Internet2 announced a partnership with Level 3 Communications to launch a brand new nationwide network, boosting its capacity from 10 Gbit/s to 100 Gbit/s. In October, 2007, Internet2 officially retired Abilene and now refers to its new, higher capacity network as the Internet2 Network.The National Science Foundation Network (NSFNET) was a program of coordinated, evolving projects sponsored by the National Science Foundation (NSF) beginning in 1985 to promote advanced research and education networking in the United States. NSFNET was also the name given to several nationwide backbone networks operating at speeds of 56 kbit/s, 1.5 Mbit/s (T1), and 45 Mbit/s (T3) that were constructed to support NSF's networking initiatives from 1985-1995. Initially created to link researchers to the nation's NSF-funded supercomputing centers, through further public funding and private industry partnerships it developed into a major part of the Internet backbone.The Very high-speed Backbone Network Service (vBNS) came on line in April 1995 as part of a National Science Foundation (NSF) sponsored project to provide high-speed interconnection between NSF-sponsored supercomputing centers and select access points in the United States. The network was engineered and operated by MCI Telecommunications under a cooperative agreement with the NSF. By 1998, the vBNS had grown to connect more than 100 universities and research and engineering institutions via 12 national points of presence with DS-3 (45 Mbit/s), OC-3c (155 Mbit/s), and OC-12c (622 Mbit/s) links on an all OC-12c backbone, a substantial engineering feat for that time. The vBNS installed one of the first ever production OC-48c (2.5 Gbit/s) IP links in February 1999 and went on to upgrade the entire backbone to OC-48c.\",\n",
       "             2: 'The Wayback Machine is a digital archive of the World Wide Web and other information on the Internet created by the Internet Archive, a nonprofit organization, based in San Francisco, California, United States. It was set up by Brewster Kahle and Bruce Gilliat, and is maintained with content from Alexa Internet. The service enables users to see archived versions of web pages across time, which the archive calls a \"three dimensional index.\"Since 1996, they have been archiving cached pages of web sites onto their large cluster of Linux nodes. They revisit sites every few weeks or months and archive a new version if the content has changed. Sites can also be captured on the fly by visitors who are offered a link to do so. The intent is to capture and archive content that otherwise would be lost whenever a site is changed or closed down. Their grand vision is to archive the entire Internet.The name Wayback Machine was chosen as a droll reference to a plot device in an animated cartoon series, The Rocky and Bullwinkle Show. In one of the animated cartoon\\'s component segments, Peabody\\'s Improbable History, lead characters Mr. Peabody and Sherman routinely used a time machine called the \"WABAC machine\" (pronounced way-back) to witness, participate in, and, more often than not, alter famous events in history.In 1996 Brewster Kahle, with Bruce Gilliat, developed software to crawl and download all publicly accessible World Wide Web pages, the Gopher hierarchy, the Netnews (Usenet) bulletin board system, and downloadable software. The information collected by these \"crawlers\" does not include all the information available on the Internet, since much of the data is restricted by the publisher or stored in databases that are not accessible. These \"crawlers\" also respect the robots exclusion standard for websites whose owners opt for them not to appear in search results or be cached. To overcome inconsistencies in partially cached web sites, Archive-It.org was developed in 2005 by the Internet Archive as a means of allowing institutions and content creators to voluntarily harvest and preserve collections of digital content, and create digital archives.Information had been kept on digital tape for five years, with Kahle occasionally allowing researchers and scientists to tap into the clunky database. When the archive reached its fifth anniversary, it was unveiled and opened to the public in a ceremony at the University of California, Berkeley.Snapshots usually become available more than six months after they are archived or, in some cases, even later; it can take twenty-four months or longer.  The frequency of snapshots is variable, so not all tracked web site updates are recorded. Sometimes there are intervals of several weeks or years between snapshots.After August 2008 sites had to be listed on the Open Directory in order to be included. According to Jeff Kaplan of the Internet Archive in November 2010, other sites were still being archived, but more recent captures would become visible only after the next major indexing, an infrequent operation.As of 2009[update], the Wayback Machine contained approximately three petabytes of data and was growing at a rate of 100 terabytes each month; the growth rate reported in 2003 was 12 terabytes/month. The data is stored on PetaBox rack systems manufactured by Capricorn Technologies.In 2009, the Internet Archive migrated its customized storage architecture to Sun Open Storage, and hosts a new data center in a Sun Modular Datacenter on Sun Microsystems\\' California campus.In 2011 a new, improved version of the Wayback Machine, with an updated interface and fresher index of archived content, was made available for public testing.In March 2011, it was said on the Wayback Machine forum that \"The Beta of the new Wayback Machine has a more complete and up-to-date index of all crawled materials into 2010, and will continue to be updated regularly. The index driving the classic Wayback Machine only has a little bit of material past 2008, and no further index updates are planned, as it will be phased out this year\".In October 2013, the company announced the \"Save a Page\" feature which allows any Internet user to archive the contents of a URL. This became a threat of abuse by the service for hosting malicious binaries.In a 2009 case, Netbula, LLC v. Chordiant Software Inc., defendant Chordiant filed a motion to compel Netbula to disable the robots.txt file on its web site that was causing the Wayback Machine to retroactively remove access to previous versions of pages it had archived from Nebula\\'s site, pages that Chordiant believed would support its case.Netbula objected to the motion on the ground that defendants were asking to alter Netbula\\'s web site and that they should have subpoenaed Internet Archive for the pages directly. An employee of Internet Archive filed a sworn statement supporting Chordiant\\'s motion, however, stating that it could not produce the web pages by any other means \"without considerable burden, expense and disruption to its operations.\"Magistrate Judge Howard Lloyd in the Northern District of California, San Jose Division, rejected Netbula\\'s arguments and ordered them to disable the robots.txt blockage temporarily in order to allow Chordiant to retrieve the archived pages that they sought.In an October 2004 case, Telewizja Polska USA, Inc. v. Echostar Satellite, No. 02 C 3293, 65 Fed. R. Evid. Serv. 673 (N.D. Ill. Oct. 15, 2004), a litigant attempted to use the Wayback Machine archives as a source of admissible evidence, perhaps for the first time. Telewizja Polska is the provider of TVP Polonia and EchoStar operates the Dish Network. Prior to the trial proceedings, EchoStar indicated that it intended to offer Wayback Machine snapshots as proof of the past content of Telewizja Polska\\'s web site. Telewizja Polska brought a motion in limine to suppress the snapshots on the grounds of hearsay and unauthenticated source, but Magistrate Judge Arlander Keys rejected Telewizja Polska\\'s assertion of hearsay and denied TVP\\'s motion in limine to exclude the evidence at trial. At the trial, however, district Court Judge Ronald Guzman, the trial judge, overruled Magistrate Keys\\' findings,[citation needed] and held that neither the affidavit of the Internet Archive employee nor the underlying pages (i.e., the Telewizja Polska website) were admissible as evidence. Judge Guzman reasoned that the employee\\'s affidavit contained both hearsay and inconclusive supporting statements, and the purported web page printouts were not self-authenticating.[citation needed]Provided some additional requirements are met (e.g. providing an authoritative statement of the archivist), the United States patent office and the European Patent Office will accept date stamps from the Internet Archive as evidence of when a given Web page was accessible to the public. These dates are used to determine if a Web page is available as prior art for instance in examining a patent application.There are technical limitations to archiving a web site, and as a consequence, it is possible for opposing parties in litigation to misuse the results provided by web site archives. This problem can be exacerbated by the practice of submitting screen shots of web pages in complaints, answers, or expert witness reports, when the underlying links are not exposed and therefore, can contain errors. For example, archives such as the Wayback Machine do not fill out forms and therefore, do not include the contents of non-RESTful e-commerce databases in their archives.In Europe the Wayback Machine could be interpreted as violating copyright laws. Only the content creator can decide where their content is published or duplicated, so the Archive would have to delete pages from its system upon request of the creator. The exclusion policies for the Wayback Machine may be found in the FAQ section of the site. The Wayback Machine also retroactively respects robots.txt files, i.e., pages that currently are blocked to robots on the live web temporarily will be made unavailable from the archives as well.In late 2002, the Internet Archive removed various sites that were critical of Scientology from the Wayback Machine. An error message stated that this was in response to a \"request by the site owner.\" Later, it was clarified that lawyers from the Church of Scientology had demanded the removal and that the site owners did not want their material removed.In 2003, Harding Earley Follmer & Frailey defended a client from a trademark dispute using the Archive\\'s Wayback Machine. The attorneys were able to demonstrate that the claims made by the plaintiff were invalid, based on the content of their web site from several years prior. The plaintiff, Healthcare Advocates, then amended their complaint to include the Internet Archive, accusing the organization of copyright infringement as well as violations of the DMCA and the Computer Fraud and Abuse Act. Healthcare Advocates claimed that, since they had installed a robots.txt file on their web site, even if after the initial lawsuit was filed, the Archive should have removed all previous copies of the plaintiff web site from the Wayback Machine. The lawsuit was settled out of court.Robots.txt is used as part of the Robots Exclusion Standard, a voluntary protocol the Internet Archive respects that disallows bots from indexing certain pages delineated by its creator as off-limits. As a result, the Internet Archive has rendered unavailable a number of web sites that now are inaccessible through the Wayback Machine. Currently, the Internet Archive applies robots.txt rules retroactively; if a site blocks the Internet Archive, such as Healthcare Advocates, any previously archived pages from the domain are rendered unavailable as well. In cases of blocked sites, only the robots.txt file is archived.The Internet Archive states, however, \"Sometimes a website owner will contact us directly and ask us to stop crawling or archiving a site. We comply with these requests.\" In addition, the web site says: \"The Internet Archive is not interested in preserving or offering access to Web sites or other Internet documents of persons who do not want their materials in the collection.\"In December 2005, activist Suzanne Shell filed suit demanding Internet Archive pay her US $100,000 for archiving her web site profane-justice.org between 1999 and 2004. Internet Archive filed a declaratory judgment action in the United States District Court for the Northern District of California on January 20, 2006, seeking a judicial determination that Internet Archive did not violate Shell\\'s copyright. Shell responded and brought a countersuit against Internet Archive for archiving her site, which she alleges is in violation of her terms of service. On February 13, 2007, a judge for the United States District Court for the District of Colorado dismissed all counterclaims except breach of contract. The Internet Archive did not move to dismiss copyright infringement claims Shell asserted arising out of its copying activities, which would also go forward.On April 25, 2007, Internet Archive and Suzanne Shell jointly announced the settlement of their lawsuit. The Internet Archive said it \"...has no interest in including materials in the Wayback Machine of persons who do not wish to have their Web content archived. We recognize that Ms. Shell has a valid and enforceable copyright in her Web site and we regret that the inclusion of her Web site in the Wayback Machine resulted in this litigation.\" Shell said, \"I respect the historical value of Internet Archive\\'s goal. I never intended to interfere with that goal nor cause it any harm.\"In 2013–14 a pornographic actor was trying to remove archived images of himself, first by sending multiple DMCA requests to the Archive and then in the Federal Court of Canada.',\n",
       "             3: 'A web browser (commonly referred to as a browser) is a software application for retrieving, presenting, and traversing information resources on the World Wide Web. An information resource is identified by a Uniform Resource Identifier (URI/URL) and may be a web page, image, video or other piece of content. Hyperlinks present in resources enable users easily to navigate their browsers to related resources.Although browsers are primarily intended to use the World Wide Web, they can also be used to access information provided by web servers in private networks or files in file systems.The first web browser was invented in 1990 by Sir Tim Berners-Lee. Berners-Lee is the director of the World Wide Web Consortium (W3C), which oversees the Web\\'s continued development, and is also the founder of the World Wide Web Foundation. His browser was called WorldWideWeb and later renamed Nexus.In 1993, browser software was further innovated by Marc Andreessen with the release of Mosaic, \"the world\\'s first popular browser\", which made the World Wide Web system easy to use and more accessible to the average person. Andreesen\\'s browser sparked the internet boom of the 1990s. The introduction of Mosaic in 1993 – one of the first graphical web browsers – led to an explosion in web use. Andreessen, the leader of the Mosaic team at National Center for Supercomputing Applications (NCSA), soon started his own company, named Netscape, and released the Mosaic-influenced Netscape Navigator in 1994, which quickly became the world\\'s most popular browser, accounting for 90% of all web use at its peak (see usage share of web browsers).Microsoft responded with its Internet Explorer in 1995, also heavily influenced by Mosaic, initiating the industry\\'s first browser war. Bundled with Windows, Internet Explorer gained dominance in the web browser market; Internet Explorer usage share peaked at over 95% by 2002.Opera debuted in 1996; it has never achieved widespread use, having less than 2% browser usage share as of February 2012 according to Net Applications. Its Opera-mini version has an additive share, in April 2011 amounting to 1.1% of overall browser use, but focused on the fast-growing mobile phone web browser market, being preinstalled on over 40 million phones. It is also available on several other embedded systems, including Nintendo\\'s Wii video game console.In 1998, Netscape launched what was to become the Mozilla Foundation in an attempt to produce a competitive browser using the open source software model. That browser would eventually evolve into Firefox, which developed a respectable following while still in the beta stage of development; shortly after the release of Firefox 1.0 in late 2004, Firefox (all versions) accounted for 7% of browser use. As of August 2011, Firefox has a 28% usage share.Apple\\'s Safari had its first beta release in January 2003; as of April 2011, it had a dominant share of Apple-based web browsing, accounting for just over 7% of the entire browser market.The most recent major entrant to the browser market is Chrome, first released in September 2008. Chrome\\'s take-up has increased significantly year by year, by doubling its usage share from 8% to 16% by August 2011. This increase seems largely to be at the expense of Internet Explorer, whose share has tended to decrease from month to month. In December 2011, Chrome overtook Internet Explorer 8 as the most widely used web browser but still had lower usage than all versions of Internet Explorer combined. Chrome\\'s user-base continued to grow and in May 2012, Chrome\\'s usage passed the usage of all versions of Internet Explorer combined. By April 2014, Chrome\\'s usage had hit 45%.Internet Explorer, on the other hand, was bundled free with the Windows operating system (and was also downloadable free), and therefore it was funded partly by the sales of Windows to computer manufacturers and direct to users. Internet Explorer also used to be available for the Mac. It is likely that releasing IE for the Mac was part of Microsoft\\'s overall strategy to fight threats to its quasi-monopoly platform dominance - threats such as web standards and Java - by making some web developers, or at least their managers, assume that there was \"no need\" to develop for anything other than Internet Explorer. In this respect, IE may have contributed to Windows and Microsoft applications sales in another way, through \"lock-in\" to Microsoft\\'s browser.In January 2009, the European Commission announced it would investigate the bundling of Internet Explorer with Windows operating systems from Microsoft, saying \"Microsoft\\'s tying of Internet Explorer to the Windows operating system harms competition between web browsers, undermines product innovation and ultimately reduces consumer choice.\" Microsoft Corp v CommissionSafari and Mobile Safari were likewise always included with OS X and iOS respectively, so, similarly, they were originally funded by sales of Apple computers and mobile devices, and formed part of the overall Apple experience to customers.Today, most commercial web browsers are paid by search engine companies to make their engine default, or to include them as another option. For example, Google pays Mozilla, the maker of Firefox, to make Google Search the default search engine in Firefox. Mozilla makes enough money from this deal that it does not need to charge users for Firefox. In addition, Google Search is also (as one would expect) the default search engine in Google Chrome. Users searching for websites or items on the Internet would be led to Google\\'s search results page, increasing ad revenue and which funds development at Google and of Google Chrome.The primary purpose of a web browser is to bring information resources to the user (\"retrieval\" or \"fetching\"), allowing them to view the information (\"display\", \"rendering\"), and then access other information (\"navigation\", \"following links\").This process begins when the user inputs a Uniform Resource Locator (URL), for example http://en.wikipedia.org/, into the browser. The prefix of the URL, the Uniform Resource Identifier or URI, determines how the URL will be interpreted. The most commonly used kind of URI starts with http: and identifies a resource to be retrieved over the Hypertext Transfer Protocol (HTTP). Many browsers also support a variety of other prefixes, such as https: for HTTPS, ftp: for the File Transfer Protocol, and file: for local files. Prefixes that the web browser cannot directly handle are often handed off to another application entirely. For example, mailto: URIs are usually passed to the user\\'s default e-mail application, and news: URIs are passed to the user\\'s default newsgroup reader.In the case of http, https, file, and others, once the resource has been retrieved the web browser will display it. HTML and associated content (image files, formatting information such as CSS, etc.) is passed to the browser\\'s layout engine to be transformed from markup to an interactive document, a process known as \"rendering\". Aside from HTML, web browsers can generally display any kind of content that can be part of a web page. Most browsers can display images, audio, video, and XML files, and often have plug-ins to support Flash applications and Java applets. Upon encountering a file of an unsupported type or a file that is set up to be downloaded rather than displayed, the browser prompts the user to save the file to disk.Information resources may contain hyperlinks to other information resources. Each link contains the URI of a resource to go to. When a link is clicked, the browser navigates to the resource indicated by the link\\'s target URI, and the process of bringing content to the user begins again.Available web browsers range in features from minimal, text-based user interfaces with bare-bones support for HTML to rich user interfaces supporting a wide variety of file formats and protocols. Browsers which include additional components to support e-mail, Usenet news, and Internet Relay Chat (IRC), are sometimes referred to as \"Internet suites\" rather than merely \"web browsers\".All major web browsers allow the user to open multiple information resources at the same time, either in different browser windows or in different tabs of the same window. Major browsers also include pop-up blockers to prevent unwanted windows from \"popping up\" without the user\\'s consent.A browser extension is a computer program that extends the functionality of a web browser. Every major web browser supports the development of browser extensions.Most web browsers can display a list of web pages that the user has bookmarked so that the user can quickly return to them. Bookmarks are also called \"Favorites\" in Internet Explorer. In addition, all major web browsers have some form of built-in web feed aggregator. In Firefox, web feeds are formatted as \"live bookmarks\" and behave like a folder of bookmarks corresponding to recent entries in the feed. In Opera, a more traditional feed reader is included which stores and displays the contents of the feed.Most browsers support HTTP Secure and offer quick and easy ways to delete the web cache, download history, form and search history, cookies, and browsing history. For a comparison of the current security vulnerabilities of browsers, see comparison of web browsers.Early web browsers supported only a very simple version of HTML. The rapid development of proprietary web browsers led to the development of non-standard dialects of HTML, leading to problems with interoperability. Modern web browsers support a combination of standards-based and de facto HTML and XHTML, which should be rendered in the same way by all browsers.Web browsers consist of a user interface, layout engine, rendering engine, JavaScript interpreter, UI backend, networking component and data persistence component. These components achieve different functionalities of a web browser and together provide all capabilities of a web browser.',\n",
       "             4: 'Conventionally, a computer consists of at least one processing element, typically a central processing unit (CPU), and some form of memory. The processing element carries out arithmetic and logic operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices allow information to be retrieved from an external source, and the result of operations saved and retrieved.Mechanical analog computers started appearing in the first century and were later used in the medieval era for astronomical calculations. In World War II, mechanical analog computers were used for specialized military applications such as calculating torpedo aiming. During this time the first electronic digital computers were developed. Originally they were the size of a large room, consuming as much power as several hundred modern personal computers (PCs).Modern computers based on integrated circuits are millions to billions of times more capable than the early machines, and occupy a fraction of the space. Computers are small enough to fit into mobile devices, and mobile computers can be powered by small batteries. Personal computers in their various forms are icons of the Information Age and are generally considered as \"computers\". However, the embedded computers found in many devices from MP3 players to fighter aircraft and from electronic toys to industrial robots are the most numerous.The first known use of the word \"computer\" was in 1613 in a book called The Yong Mans Gleanings by English writer Richard Braithwait: \"I haue read the truest computer of Times, and the best Arithmetician that euer breathed, and he reduceth thy dayes into a short number.\" It referred to a person who carried out calculations, or computations. The word continued with the same meaning until the middle of the 20th century. From the end of the 19th century the word began to take on its more familiar meaning, a machine that carries out computations.Devices have been used to aid computation for thousands of years, mostly using one-to-one correspondence with fingers. The earliest counting device was probably a form of tally stick. Later record keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented counts of items, probably livestock or grains, sealed in hollow unbaked clay containers. The use of counting rods is one example.The abacus was initially used for arithmetic tasks. The Roman abacus was used in Babylonia as early as 2400 BC. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European counting house, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.The Antikythera mechanism is believed to be the earliest mechanical analog \"computer\", according to Derek J. de Solla Price. It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to circa 100 BC. Devices of a level of complexity comparable to that of the Antikythera mechanism would not reappear until a thousand years later.Many mechanical aids to calculation and measurement were constructed for astronomical and navigation use. The planisphere was a star chart invented by Abū Rayhān al-Bīrūnī in the early 11th century. The astrolabe was invented in the Hellenistic world in either the 1st or 2nd centuries BC and is often attributed to Hipparchus. A combination of the planisphere and dioptra, the astrolabe was effectively an analog computer capable of working out several different kinds of problems in spherical astronomy. An astrolabe incorporating a mechanical calendar computer and gear-wheels was invented by Abi Bakr of Isfahan, Persia in 1235. Abū Rayhān al-Bīrūnī invented the first mechanical geared lunisolar calendar astrolabe, an early fixed-wired knowledge processing machine with a gear train and gear-wheels, circa 1000 AD.The sector, a calculating instrument used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots, was developed in the late 16th century and found application in gunnery, surveying and navigation.The slide rule was invented around 1620–1630, shortly after the publication of the concept of the logarithm. It is a hand-operated analog computer for doing multiplication and division. As slide rule development progressed, added scales provided reciprocals, squares and square roots, cubes and cube roots, as well as transcendental functions such as logarithms and exponentials, circular and hyperbolic trigonometry and other functions. Aviation is one of the few fields where slide rules are still in widespread use, particularly for solving time–distance problems in light aircraft. To save space and for ease of reading, these are typically circular devices rather than the classic linear slide rule shape. A popular example is the E6B.In the 1770s Pierre Jaquet-Droz, a Swiss watchmaker, built a mechanical doll (automata) that could write holding a quill pen. By switching the number and order of its internal wheels different letters, and hence different messages, could be produced. In effect, it could be mechanically \"programmed\" to read instructions. Along with two other complex machines, the doll is at the Musée d\\'Art et d\\'Histoire of Neuchâtel, Switzerland, and still operates.The tide-predicting machine invented by Sir William Thomson in 1872 was of great utility to navigation in shallow waters. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location.The differential analyser, a mechanical analog computer designed to solve differential equations by integration, used wheel-and-disc mechanisms to perform the integration. In 1876 Lord Kelvin had already discussed the possible construction of such calculators, but he had been stymied by the limited output torque of the ball-and-disk integrators. In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output. The torque amplifier was the advance that allowed these machines to work. Starting in the 1920s, Vannevar Bush and others developed mechanical differential analyzers.Charles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the \"father of the computer\", he conceptualized and invented the first mechanical computer in the early 19th century. After working on his revolutionary difference engine, designed to aid in navigational calculations, in 1833 he realized that a much more general design, an Analytical Engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. The Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete.The machine was about a century ahead of its time. All the parts for his machine had to be made by hand — this was a major problem for a device with thousands of parts. Eventually, the project was dissolved with the decision of the British Government to cease funding. Babbage\\'s failure to complete the analytical engine can be chiefly attributed to difficulties not only of politics and financing, but also to his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. Nevertheless, his son, Henry Babbage, completed a simplified version of the analytical engine\\'s computing unit (the mill) in 1888. He gave a successful demonstration of its use in computing tables in 1906.The first modern analog computer was a tide-predicting machine, invented by Sir William Thomson in 1872. The differential analyser, a mechanical analog computer designed to solve differential equations by integration using wheel-and-disc mechanisms, was conceptualized in 1876 by James Thomson, the brother of the more famous Lord Kelvin.The art of mechanical analog computing reached its zenith with the differential analyzer, built by H. L. Hazen and Vannevar Bush at MIT starting in 1927. This built on the mechanical integrators of James Thomson and the torque amplifiers invented by H. W. Nieman. A dozen of these devices were built before their obsolescence became obvious.By the 1950s the success of digital electronic computers had spelled the end for most analog computing machines, but analog computers remain in use in some specialized applications such as education (control systems) and aircraft (slide rule).The principle of the modern computer was first described by mathematician and pioneering computer scientist Alan Turing, who set out the idea in his seminal 1936 paper, On Computable Numbers. Turing reformulated Kurt Gödel\\'s 1931 results on the limits of proof and computation, replacing Gödel\\'s universal arithmetic-based formal language with the formal and simple hypothetical devices that became known as Turing machines. He proved that some such machine would be capable of performing any conceivable mathematical computation if it were representable as an algorithm. He went on to prove that there was no solution to the Entscheidungsproblem by first showing that the halting problem for Turing machines is undecidable: in general, it is not possible to decide algorithmically whether a given Turing machine will ever halt.He also introduced the notion of a \\'Universal Machine\\' (now known as a Universal Turing machine), with the idea that such a machine could perform the tasks of any other machine, or in other words, it is provably capable of computing anything that is computable by executing a program stored on tape, allowing the machine to be programmable. Von Neumann acknowledged that the central concept of the modern computer was due to this paper. Turing machines are to this day a central object of study in theory of computation. Except for the limitations imposed by their finite memory stores, modern computers are said to be Turing-complete, which is to say, they have algorithm execution capability equivalent to a universal Turing machine.By 1938 the United States Navy had developed an electromechanical analog computer small enough to use aboard a submarine. This was the Torpedo Data Computer, which used trigonometry to solve the problem of firing a torpedo at a moving target. During World War II similar devices were developed in other countries as well.Early digital computers were electromechanical; electric switches drove mechanical relays to perform the calculation. These devices had a low operating speed and were eventually superseded by much faster all-electric computers, originally using vacuum tubes. The Z2, created by German engineer Konrad Zuse in 1939, was one of the earliest examples of an electromechanical relay computer.In 1941, Zuse followed his earlier machine up with the Z3, the world\\'s first working electromechanical programmable, fully automatic digital computer. The Z3 was built with 2000 relays, implementing a 22 bit word length that operated at a clock frequency of about 5–10 Hz. Program code was supplied on punched film while data could be stored in 64 words of memory or supplied from the keyboard. It was quite similar to modern machines in some respects, pioneering numerous advances such as floating point numbers. Replacement of the hard-to-implement decimal system (used in Charles Babbage\\'s earlier design) by the simpler binary system meant that Zuse\\'s machines were easier to build and potentially more reliable, given the technologies available at that time. The Z3 was Turing complete.Purely electronic circuit elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. The engineer Tommy Flowers, working at the Post Office Research Station in London in the 1930s, began to explore the possible use of electronics for the telephone exchange. Experimental equipment that he built in 1934 went into operation 5 years later, converting a portion of the telephone exchange network into an electronic data processing system, using thousands of vacuum tubes. In the US, John Vincent Atanasoff and Clifford E. Berry of Iowa State University developed and tested the Atanasoff–Berry Computer (ABC) in 1942, the first \"automatic electronic digital computer\". This design was also all-electronic and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory.During World War II, the British at Bletchley Park achieved a number of successes at breaking encrypted German military communications. The German encryption machine, Enigma, was first attacked with the help of the electro-mechanical bombes. To crack the more sophisticated German Lorenz SZ 40/42 machine, used for high-level Army communications, Max Newman and his colleagues commissioned Flowers to build the Colossus. He spent eleven months from early February 1943 designing and building the first Colossus. After a functional test in December 1943, Colossus was shipped to Bletchley Park, where it was delivered on 18 January 1944 and attacked its first message on 5 February.Colossus was the world\\'s first electronic digital programmable computer. It used a large number of valves (vacuum tubes). It had paper-tape input and was capable of being configured to perform a variety of boolean logical operations on its data, but it was not Turing-complete. Nine Mk II Colossi were built (The Mk I was converted to a Mk II making ten machines in total). Colossus Mark I contained 1500 thermionic valves (tubes), but Mark II with 2400 valves, was both 5 times faster and simpler to operate than Mark 1, greatly speeding the decoding process.The US-built ENIAC (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the US. Although the ENIAC was similar to the Colossus it was much faster and more flexible. It was unambiguously a Turing-complete device and could compute any problem that would fit into its memory. Like the Colossus, a \"program\" on the ENIAC was defined by the states of its patch cables and switches, a far cry from the stored program electronic machines that came later. Once a program was written, it had to be mechanically set into the machine with manual resetting of plugs and switches.It combined the high speed of electronics with the ability to be programmed for many complex problems. It could add or subtract 5000 times a second, a thousand times faster than any other machine. It also had modules to multiply, divide, and square root. High speed memory was limited to 20 words (about 80 bytes). Built under the direction of John Mauchly and J. Presper Eckert at the University of Pennsylvania, ENIAC\\'s development and construction lasted from 1943 to full operation at the end of 1945. The machine was huge, weighing 30 tons, using 200 kilowatts of electric power and contained over 18,000 vacuum tubes, 1,500 relays, and hundreds of thousands of resistors, capacitors, and inductors.Early computing machines had fixed programs. Changing its function required the re-wiring and re-structuring of the machine. With the proposal of the stored-program computer this changed. A stored-program computer includes by design an instruction set and can store in memory a set of instructions (a program) that details the computation. The theoretical basis for the stored-program computer was laid by Alan Turing in his 1936 paper. In 1945 Turing joined the National Physical Laboratory and began work on developing an electronic stored-program digital computer. His 1945 report ‘Proposed Electronic Calculator’ was the first specification for such a device. John von Neumann at the University of Pennsylvania, also circulated his First Draft of a Report on the EDVAC in 1945.The Manchester Small-Scale Experimental Machine, nicknamed Baby, was the world\\'s first stored-program computer. It was built at the Victoria University of Manchester by Frederic C. Williams, Tom Kilburn and Geoff Tootill, and ran its first program on 21 June 1948. It was designed as a testbed for the Williams tube the first random-access digital storage device. Although the computer was considered \"small and primitive\" by the standards of its time, it was the first working machine to contain all of the elements essential to a modern electronic computer. As soon as the SSEM had demonstrated the feasibility of its design, a project was initiated at the university to develop it into a more usable computer, the Manchester Mark 1.The Mark 1 in turn quickly became the prototype for the Ferranti Mark 1, the world\\'s first commercially available general-purpose computer. Built by Ferranti, it was delivered to the University of Manchester in February 1951. At least seven of these later machines were delivered between 1953 and 1957, one of them to Shell labs in Amsterdam. In October 1947, the directors of British catering company J. Lyons & Company decided to take an active role in promoting the commercial development of computers. The LEO I computer became operational in April 1951  and ran the world\\'s first regular routine office computer job.The bipolar transistor was invented in 1947. From 1955 onwards transistors replaced vacuum tubes in computer designs, giving rise to the \"second generation\" of computers. Compared to vacuum tubes, transistors have many advantages: they are smaller, and require less power than vacuum tubes, so give off less heat. Silicon junction transistors were much more reliable than vacuum tubes and had longer, indefinite, service life. Transistorized computers could contain tens of thousands of binary logic circuits in a relatively compact space.At the University of Manchester, a team under the leadership of Tom Kilburn designed and built a machine using the newly developed transistors instead of valves. Their first transistorised computer and the first in the world, was operational by 1953, and a second version was completed there in April 1955. However, the machine did make use of valves to generate its 125 kHz clock waveforms and in the circuitry to read and write on its magnetic drum memory, so it was not the first completely transistorized computer. That distinction goes to the Harwell CADET of 1955, built by the electronics division of the Atomic Energy Research Establishment at Harwell.The next great advance in computing power came with the advent of the integrated circuit. The idea of the integrated circuit was first conceived by a radar scientist working for the Royal Radar Establishment of the Ministry of Defence, Geoffrey W.A. Dummer. Dummer presented the first public description of an integrated circuit at the Symposium on Progress in Quality Electronic Components in Washington, D.C. on 7 May 1952.The first practical ICs were invented by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor. Kilby recorded his initial ideas concerning the integrated circuit in July 1958, successfully demonstrating the first working integrated example on 12 September 1958. In his patent application of 6 February 1959, Kilby described his new device as \"a body of semiconductor material ... wherein all the components of the electronic circuit are completely integrated\". Noyce also came up with his own idea of an integrated circuit half a year later than Kilby. His chip solved many practical problems that Kilby\\'s had not. Produced at Fairchild Semiconductor, it was made of silicon, whereas Kilby\\'s chip was made of germanium.This new development heralded an explosion in the commercial and personal use of computers and led to the invention of the microprocessor. While the subject of exactly which device was the first microprocessor is contentious, partly due to lack of agreement on the exact definition of the term \"microprocessor\", it is largely undisputed that the first single-chip microprocessor was the Intel 4004, designed and realized by Ted Hoff, Federico Faggin, and Stanley Mazor at Intel.With the continued miniaturization of computing resources, and advancements in portable battery life, portable computers grew in popularity in the 2000s. The same developments that spurred the growth of laptop computers and other portable computers allowed manufacturers to integrate computing resources into cellular phones. These so-called smartphones and tablets run on a variety of operating systems and have become the dominant computing device on the market, with manufacturers reporting having shipped an estimated 237 million devices in 2Q 2013.In practical terms, a computer program may be just a few instructions or extend to many millions of instructions, as do the programs for word processors and web browsers for example. A typical modern computer can execute billions of instructions per second (gigaflops) and rarely makes a mistake over many years of operation. Large computer programs consisting of several million instructions may take teams of programmers years to write, and due to the complexity of the task almost certainly contain errors.Program execution might be likened to reading a book. While a person will normally read each word and line in sequence, they may at times jump back to an earlier place in the text or skip sections that are not of interest. Similarly, a computer may sometimes go back and repeat the instructions in some section of the program over and over again until some internal condition is met. This is called the flow of control within the program and it is what allows the computer to perform tasks repeatedly without human intervention.In most computers, individual instructions are stored as machine code with each instruction being given a unique number (its operation code or opcode for short). The command to add two numbers together would have one opcode; the command to multiply them would have a different opcode, and so on. The simplest computers are able to perform any of a handful of different instructions; the more complex computers have several hundred to choose from, each with a unique numerical code. Since the computer\\'s memory is able to store numbers, it can also store the instruction codes. This leads to the important fact that entire programs (which are just lists of these instructions) can be represented as lists of numbers and can themselves be manipulated inside the computer in the same way as numeric data. The fundamental concept of storing programs in the computer\\'s memory alongside the data they operate on is the crux of the von Neumann, or stored program[citation needed], architecture. In some cases, a computer might store some or all of its program in memory that is kept separate from the data it operates on. This is called the Harvard architecture after the Harvard Mark I computer. Modern von Neumann computers display some traits of the Harvard architecture in their designs, such as in CPU caches.While it is possible to write computer programs as long lists of numbers (machine language) and while this technique was used with many early computers, it is extremely tedious and potentially error-prone to do so in practice, especially for complicated programs. Instead, each basic instruction can be given a short name that is indicative of its function and easy to remember – a mnemonic such as ADD, SUB, MULT or JUMP. These mnemonics are collectively known as a computer\\'s assembly language. Converting programs written in assembly language into something the computer can actually understand (machine language) is usually done by a computer program called an assembler.Programming languages provide various ways of specifying programs for computers to run. Unlike natural languages, programming languages are designed to permit no ambiguity and to be concise. They are purely written languages and are often difficult to read aloud. They are generally either translated into machine code by a compiler or an assembler before being run, or translated directly at run time by an interpreter. Sometimes programs are executed by a hybrid method of the two techniques.Machine languages and the assembly languages that represent them (collectively termed low-level programming languages) tend to be unique to a particular type of computer. For instance, an ARM architecture computer (such as may be found in a PDA or a hand-held videogame) cannot understand the machine language of an Intel Pentium or the AMD Athlon 64 computer that might be in a PC.Though considerably easier than in machine language, writing long programs in assembly language is often difficult and is also error prone. Therefore, most practical programs are written in more abstract high-level programming languages that are able to express the needs of the programmer more conveniently (and thereby help reduce programmer error). High level languages are usually \"compiled\" into machine language (or sometimes into assembly language and then into machine language) using another computer program called a compiler. High level languages are less related to the workings of the target computer than assembly language, and more related to the language and structure of the problem(s) to be solved by the final program. It is therefore often possible to use different compilers to translate the same high level language program into the machine language of many different types of computer. This is part of the means by which software like video games may be made available for different computer architectures such as personal computers and various video game consoles.These 4G languages are less procedural than 3G languages. The benefit of 4GL is that it provides ways to obtain information without requiring the direct help of a programmer. Example of 4GL is SQL.Errors in computer programs are called \"bugs\". They may be benign and not affect the usefulness of the program, or have only subtle effects. But in some cases, they may cause the program or the entire system to \"hang\", becoming unresponsive to input such as mouse clicks or keystrokes, to completely fail, or to crash. Otherwise benign bugs may sometimes be harnessed for malicious intent by an unscrupulous user writing an exploit, code designed to take advantage of a bug and disrupt a computer\\'s proper execution. Bugs are usually not the fault of the computer. Since computers merely execute the instructions they are given, bugs are nearly always the result of programmer error or an oversight made in the program\\'s design.Admiral Grace Hopper, an American computer scientist and developer of the first compiler, is credited for having first used the term \"bugs\" in computing after a dead moth was found shorting a relay in the Harvard Mark II computer in September 1947.A general purpose computer has four main components: the arithmetic logic unit (ALU), the control unit, the memory, and the input and output devices (collectively termed I/O). These parts are interconnected by buses, often made of groups of wires.Inside each of these parts are thousands to trillions of small electrical circuits which can be turned off or on by means of an electronic switch. Each circuit represents a bit (binary digit) of information so that when the circuit is on it represents a \"1\", and when off it represents a \"0\" (in positive logic representation). The circuits are arranged in logic gates so that one or more of the circuits may control the state of one or more of the other circuits.The control unit (often called a control system or central controller) manages the computer\\'s various components; it reads and interprets (decodes) the program instructions, transforming them into control signals that activate other parts of the computer. Control systems in advanced computers may change the order of execution of some instructions to improve performance.A key component common to all CPUs is the program counter, a special memory cell (a register) that keeps track of which location in memory the next instruction is to be read from.Since the program counter is (conceptually) just another set of memory cells, it can be changed by calculations done in the ALU. Adding 100 to the program counter would cause the next instruction to be read from a place 100 locations further down the program. Instructions that modify the program counter are often known as \"jumps\" and allow for loops (instructions that are repeated by the computer) and often conditional instruction execution (both examples of control flow).The sequence of operations that the control unit goes through to process an instruction is in itself like a short computer program, and indeed, in some more complex CPU designs, there is another yet smaller computer called a microsequencer, which runs a microcode program that causes all of these events to happen.The control unit, ALU, and registers are collectively known as a central processing unit (CPU). Early CPUs were composed of many separate components but since the mid-1970s CPUs have typically been constructed on a single integrated circuit called a microprocessor.The set of arithmetic operations that a particular ALU supports may be limited to addition and subtraction, or might include multiplication, division, trigonometry functions such as sine, cosine, etc., and square roots. Some can only operate on whole numbers (integers) whilst others use floating point to represent real numbers, albeit with limited precision. However, any computer that is capable of performing just the simplest operations can be programmed to break down the more complex operations into simple steps that it can perform. Therefore, any computer can be programmed to perform any arithmetic operation—although it will take more time to do so if its ALU does not directly support the operation. An ALU may also compare numbers and return boolean truth values (true or false) depending on whether one is equal to, greater than or less than the other (\"is 64 greater than 65?\").Logic operations involve Boolean logic: AND, OR, XOR, and NOT. These can be useful for creating complicated conditional statements and processing boolean logic.Superscalar computers may contain multiple ALUs, allowing them to process several instructions simultaneously. Graphics processors and computers with SIMD and MIMD features often contain ALUs that can perform arithmetic on vectors and matrices.A computer\\'s memory can be viewed as a list of cells into which numbers can be placed or read. Each cell has a numbered \"address\" and can store a single number. The computer can be instructed to \"put the number 123 into the cell numbered 1357\" or to \"add the number that is in cell 1357 to the number that is in cell 2468 and put the answer into cell 1595.\" The information stored in memory may represent practically anything. Letters, numbers, even computer instructions can be placed into memory with equal ease. Since the CPU does not differentiate between different types of information, it is the software\\'s responsibility to give significance to what the memory sees as nothing but a series of numbers.In almost all modern computers, each memory cell is set up to store binary numbers in groups of eight bits (called a byte). Each byte is able to represent 256 different numbers (28 = 256); either from 0 to 255 or −128 to +127. To store larger numbers, several consecutive bytes may be used (typically, two, four or eight). When negative numbers are required, they are usually stored in two\\'s complement notation. Other arrangements are possible, but are usually not seen outside of specialized applications or historical contexts. A computer can store any kind of information in memory if it can be represented numerically. Modern computers have billions or even trillions of bytes of memory.The CPU contains a special set of memory cells called registers that can be read and written to much more rapidly than the main memory area. There are typically between two and one hundred registers depending on the type of CPU. Registers are used for the most frequently needed data items to avoid having to access main memory every time data is needed. As data is constantly being worked on, reducing the need to access main memory (which is often slow compared to the ALU and control units) greatly increases the computer\\'s speed.RAM can be read and written to anytime the CPU commands it, but ROM is preloaded with data and software that never changes, therefore the CPU can only read from it. ROM is typically used to store the computer\\'s initial start-up instructions. In general, the contents of RAM are erased when the power to the computer is turned off, but ROM retains its data indefinitely. In a PC, the ROM contains a specialized program called the BIOS that orchestrates loading the computer\\'s operating system from the hard disk drive into RAM whenever the computer is turned on or reset. In embedded computers, which frequently do not have disk drives, all of the required software may be stored in ROM. Software stored in ROM is often called firmware, because it is notionally more like hardware than software. Flash memory blurs the distinction between ROM and RAM, as it retains its data when turned off but is also rewritable. It is typically much slower than conventional ROM and RAM however, so its use is restricted to applications where high speed is unnecessary.In more sophisticated computers there may be one or more RAM cache memories, which are slower than registers but faster than main memory. Generally computers with this sort of cache are designed to move frequently needed data into the cache automatically, often without the need for any intervention on the programmer\\'s part.I/O is the means by which a computer exchanges information with the outside world. Devices that provide input or output to the computer are called peripherals. On a typical personal computer, peripherals include input devices like the keyboard and mouse, and output devices such as the display and printer. Hard disk drives, floppy disk drives and optical disc drives serve as both input and output devices. Computer networking is another form of I/O.While a computer may be viewed as running one gigantic program stored in its main memory, in some systems it is necessary to give the appearance of running several programs simultaneously. This is achieved by multitasking i.e. having the computer switch rapidly between running each program in turn.One means by which this is done is with a special signal called an interrupt, which can periodically cause the computer to stop executing instructions where it was and do something else instead. By remembering where it was executing prior to the interrupt, the computer can return to that task later. If several programs are running \"at the same time\". then the interrupt generator might be causing several hundred interrupts per second, causing a program switch each time. Since modern computers typically execute instructions several orders of magnitude faster than human perception, it may appear that many programs are running at the same time even though only one is ever executing in any given instant. This method of multitasking is sometimes termed \"time-sharing\" since each program is allocated a \"slice\" of time in turn.Seemingly, multitasking would cause a computer that is switching between several programs to run more slowly, in direct proportion to the number of programs it is running, but most programs spend much of their time waiting for slow input/output devices to complete their tasks. If a program is waiting for the user to click on the mouse or press a key on the keyboard, then it will not take a \"time slice\" until the event it is waiting for has occurred. This frees up time for other programs to execute so that many programs may be run simultaneously without unacceptable speed loss.Some computers are designed to distribute their work across several CPUs in a multiprocessing configuration, a technique once employed only in large and powerful machines such as supercomputers, mainframe computers and servers. Multiprocessor and multi-core (multiple CPUs on a single integrated circuit) personal and laptop computers are now widely available, and are being increasingly used in lower-end markets as a result.Supercomputers in particular often have highly unique architectures that differ significantly from the basic stored-program architecture and from general purpose computers. They often feature thousands of CPUs, customized high-speed interconnects, and specialized computing hardware. Such designs tend to be useful only for specialized tasks due to the large scale of program organization required to successfully utilize most of the available resources at once. Supercomputers usually see usage in large-scale simulation, graphics rendering, and cryptography applications, as well as with other so-called \"embarrassingly parallel\" tasks.Computers have been used to coordinate information between multiple locations since the 1950s. The U.S. military\\'s SAGE system was the first large-scale example of such a system, which led to a number of special-purpose commercial systems such as Sabre.In the 1970s, computer engineers at research institutions throughout the United States began to link their computers together using telecommunications technology. The effort was funded by ARPA (now DARPA), and the computer network that resulted was called the ARPANET. The technologies that made the Arpanet possible spread and evolved.In time, the network spread beyond academic and military institutions and became known as the Internet. The emergence of networking involved a redefinition of the nature and boundaries of the computer. Computer operating systems and applications were modified to include the ability to define and access the resources of other computers on the network, such as peripheral devices, stored information, and the like, as extensions of the resources of an individual computer. Initially these facilities were available primarily to people working in high-tech environments, but in the 1990s the spread of applications like e-mail and the World Wide Web, combined with the development of cheap, fast networking technologies like Ethernet and ADSL saw computer networking become almost ubiquitous. In fact, the number of computers that are networked is growing phenomenally. A very large proportion of personal computers regularly connect to the Internet to communicate and receive information. \"Wireless\" networking, often utilizing mobile phone networks, has meant networking is becoming increasingly ubiquitous even in mobile computing environments.The ability to store and execute lists of instructions called programs makes computers extremely versatile, distinguishing them from calculators. The Church–Turing thesis is a mathematical statement of this versatility: any computer with a minimum capability (being Turing-complete) is, in principle, capable of performing the same tasks that any other computer can perform. Therefore, any type of computer (netbook, supercomputer, cellular automaton, etc.) is able to perform the same computational tasks, given enough time and storage capacity.A computer does not need to be electronic, nor even have a processor, nor RAM, nor even a hard disk. While popular usage of the word \"computer\" is synonymous with a personal electronic computer, the modern definition of a computer is literally: \"A device that computes, especially a programmable [usually] electronic machine that performs high-speed mathematical or logical operations or that assembles, stores, correlates, or otherwise processes information.\" Any device which processes information qualifies as a computer, especially if the processing is purposeful.[citation needed]Historically, computers evolved from mechanical computers and eventually from vacuum tubes to transistors. However, conceptually computational systems as flexible as a personal computer can be built out of almost anything. For example, a computer can be made out of billiard balls (billiard ball computer); an often quoted example.[citation needed] More realistically, modern computers are made out of transistors made of photolithographed semiconductors.There is active research to make computers out of many promising new types of technology, such as optical computers, DNA computers, neural computers, and quantum computers. Most computers are universal, and are able to calculate any computable function, and are limited only by their memory capacity and operating speed. However different designs of computers can give very different performance for particular problems; for example quantum computers can potentially break some modern encryption algorithms (by quantum factoring) very quickly.A computer will solve problems in exactly the way it is programmed to, without regard to efficiency, alternative solutions, possible shortcuts, or possible errors in the code. Computer programs that learn and adapt are part of the emerging field of artificial intelligence and machine learning.The term hardware covers all of those parts of a computer that are tangible objects. Circuits, displays, power supplies, cables, keyboards, printers and mice are all hardware.Software refers to parts of the computer which do not have a material form, such as programs, data, protocols, etc. When software is stored in hardware that cannot easily be modified (such as BIOS ROM in an IBM PC compatible), it is sometimes called \"firmware\".Firmware is the technology which has the combination of both hardware and software such as BIOS chip inside a computer. This chip (hardware) is located on the motherboard and has the BIOS set up (software) stored in it.When unprocessed data is sent to the computer with the help of input devices, the data is processed and sent to output devices. The input devices may be hand-operated or automated. The act of processing is mainly regulated by the CPU. Some examples of hand-operated input devices are:',\n",
       "             5: 'Computer security, also known as cybersecurity or IT security, is the protection of information systems from theft or damage to the hardware, the software, and to the information on them, as well as from disruption or misdirection of the services they provide. It includes controlling physical access to the hardware, as well as protecting against harm that may come via network access, data and code injection, and due to malpractice by operators, whether intentional, accidental, or due to them being tricked into deviating from secure procedures.Denial of service attacks are designed to make a machine or network resource unavailable to its intended users. Attackers can deny service to individual victims, such as by deliberately entering a wrong password enough consecutive times to cause the victim account to be locked, or they may overload the capabilities of a machine or network and block all users at once. While a network attack from a single IP address can be blocked by adding a new firewall rule, many forms of Distributed denial of service (DDoS) attacks are possible, where the attack comes from a large number of points – and defending is much more difficult. Such attacks can originate from the zombie computers of a botnet, but a range of other techniques are possible including reflection and amplification attacks, where innocent systems are fooled into sending traffic to the victim.If access is gained to a car\\'s internal controller area network, it is possible to disable the brakes and turn the steering wheel. Computerized engine timing, cruise control, anti-lock brakes, seat belt tensioners, door locks, airbags and advanced driver assistance systems make these disruptions possible, and self-driving cars go even further. Connected cars may use wifi and bluetooth to communicate with onboard consumer devices, and the cell phone network to contact concierge and emergency assistance services or get navigational or entertainment information; each of these networks is a potential entry point for malware or an attacker. Researchers in 2011 were even able to use a malicious compact disc in a car\\'s stereo system as a successful attack vector, and cars with built-in voice recognition or remote assistance features have onboard microphones which could be used for eavesdropping.However, relatively few organisations maintain computer systems with effective detection systems, and fewer still have organised response mechanisms in place. As result, as Reuters points out: \"Companies for the first time report they are losing more through electronic theft of data than physical stealing of assets\". The primary obstacle to effective eradication of cyber crime could be traced to excessive reliance on firewalls and other automated \"detection\" systems. Yet it is basic evidence gathering by using packet capture appliances that puts criminals behind bars.One use of the term \"computer security\" refers to technology that is used to implement secure operating systems. In the 1980s the United States Department of Defense (DoD) used the \"Orange Book\" standards, but the current international standard ISO/IEC 15408, \"Common Criteria\" defines a number of progressively more stringent Evaluation Assurance Levels. Many common operating systems meet the EAL4 standard of being \"Methodically Designed, Tested and Reviewed\", but the formal verification required for the highest levels means that they are uncommon. An example of an EAL6 (\"Semiformally Verified Design and Tested\") system is Integrity-178B, which is used in the Airbus A380 and several military jets.China\\'s network security and information technology leadership team was established February 27, 2014. The leadership team is tasked with national security and long-term development and co-ordination of major issues related to network security and information technology. Economic, political, cultural, social and military fields as related to network security and information technology strategy, planning and major macroeconomic policy are being researched. The promotion of national network security and information technology law are constantly under study for enhanced national security capabilities.Eavesdropping is the act of surreptitiously listening to a private conversation, typically between hosts on a network. For instance, programs such as Carnivore and NarusInsight have been used by the FBI and NSA to eavesdrop on the systems of internet service providers. Even machines that operate as a closed system (i.e., with no contact to the outside world) can be eavesdropped upon via monitoring the faint electro-magnetic transmissions generated by the hardware; TEMPEST is a specification by the NSA referring to these attacks.Desktop computers and laptops are commonly infected with malware either to gather passwords or financial account information, or to construct a botnet to attack another target. Smart phones, tablet computers, smart watches, and other mobile devices such as Quantified Self devices like activity trackers have also become targets and many of these have sensors such as cameras, microphones, GPS receivers, compasses, and accelerometers which could be exploited, and may collect personal information, including sensitive health information. Wifi, Bluetooth, and cell phone network on any of these devices could be used as attack vectors, and sensors might be remotely activated after a successful breach.Within computer systems, two of many security models capable of enforcing privilege separation are access control lists (ACLs) and capability-based security. Using ACLs to confine programs has been proven to be insecure in many situations, such as if the host computer can be tricked into indirectly allowing restricted file access, an issue known as the confused deputy problem. It has also been shown that the promise of ACLs of giving access to an object to only one person can never be guaranteed in practice. Both of these problems are resolved by capabilities. This does not mean practical flaws exist in all ACL-based systems, but only that the designers of certain utilities must take responsibility to ensure that they do not introduce flaws.[citation needed]In 1994, over a hundred intrusions were made by unidentified crackers into the Rome Laboratory, the US Air Force\\'s main command and research facility. Using trojan horses, hackers were able to obtain unrestricted access to Rome\\'s networking systems and remove traces of their activities. The intruders were able to obtain classified files, such as air tasking order systems data and furthermore able to penetrate connected networks of National Aeronautics and Space Administration\\'s Goddard Space Flight Center, Wright-Patterson Air Force Base, some Defense contractors, and other private sector organizations, by posing as a trusted Rome center user.In July of 2015, a hacker group known as \"The Impact Team\" successfully breached the extramarital relationship website Ashley Madison. The group claimed that they had taken not only company data but user data as well. After the breach, The Impact Team dumped emails from the company\\'s CEO, to prove their point, and threatened to dump customer data unless the website was taken down permanently. With this initial data release, the group stated “Avid Life Media has been instructed to take Ashley Madison and Established Men offline permanently in all forms, or we will release all customer records, including profiles with all the customers\\' secret sexual fantasies and matching credit card transactions, real names and addresses, and employee documents and emails. The other websites may stay online.”  When Avid Life Media, the parent company that created the Ashley Madison website, did not take the site offline, The Impact Group released two more compressed files, one 9.7GB and the second 20GB. After the second data dump, Avid Life Media CEO Noel Biderman resigned, but the website remained functional.The question of whether the government should intervene or not in the regulation of the cyberspace is a very polemical one. Indeed, for as long as it has existed and by definition, the cyberspace is a virtual space free of any government intervention. Where everyone agree that an improvement on cybersecurity is more than vital, is the government the best actor to solve this issue? Many government officials and experts think that the government should step in and that there is a crucial need for regulation, mainly due to the failure of the private sector to solve efficiently the cybersecurity problem. R. Clarke said during a panel discussion at the RSA Security Conference in San Francisco, he believes that the \"industry only responds when you threaten regulation. If industry doesn\\'t respond (to the threat), you have to follow through.\" On the other hand, executives from the private sector agree that improvements are necessary, but think that the government intervention would affect their ability to innovate efficiently.On October 3, 2010, Public Safety Canada unveiled Canada’s Cyber Security Strategy, following a Speech from the Throne commitment to boost the security of Canadian cyberspace. The aim of the strategy is to strengthen Canada’s \"cyber systems and critical infrastructure sectors, support economic growth and protect Canadians as they connect to each other and to the world.\" Three main pillars define the strategy: securing government systems, partnering to secure vital cyber systems outside the federal government, and helping Canadians to be secure online. The strategy involves multiple departments and agencies across the Government of Canada. The Cyber Incident Management Framework for Canada outlines these responsibilities, and provides a plan for coordinated response between government and other partners in the event of a cyber incident. The Action Plan 2010–2015 for Canada\\'s Cyber Security Strategy outlines the ongoing implementation of the strategy.Computers control functions at many utilities, including coordination of telecommunications, the power grid, nuclear power plants, and valve opening and closing in water and gas networks. The Internet is a potential attack vector for such machines if connected, but the Stuxnet worm demonstrated that even equipment controlled by computers not connected to the Internet can be vulnerable to physical damage caused by malicious commands sent to industrial equipment (in that case uranium enrichment centrifuges) which are infected via removable media. In 2014, the Computer Emergency Readiness Team, a division of the Department of Homeland Security, investigated 79 hacking incidents at energy companies.Today, computer security comprises mainly \"preventive\" measures, like firewalls or an exit procedure. A firewall can be defined as a way of filtering network data between a host or a network and another network, such as the Internet, and can be implemented as software running on the machine, hooking into the network stack (or, in the case of most UNIX-based operating systems such as Linux, built into the operating system kernel) to provide real time filtering and blocking. Another implementation is a so-called physical firewall which consists of a separate machine filtering network traffic. Firewalls are common amongst machines that are permanently connected to the Internet.Serious financial damage has been caused by security breaches, but because there is no standard model for estimating the cost of an incident, the only data available is that which is made public by the organizations involved. \"Several computer security consulting firms produce estimates of total worldwide losses attributable to virus and worm attacks and to hostile digital acts in general. The 2003 loss estimates by these firms range from $13 billion (worms and viruses only) to $226 billion (for all forms of covert attacks). The reliability of these estimates is often challenged; the underlying methodology is basically anecdotal.\"While hardware may be a source of insecurity, such as with microchip vulnerabilities maliciously introduced during the manufacturing process, hardware-based or assisted computer security also offers an alternative to software-only computer security. Using devices and methods such as dongles, trusted platform modules, intrusion-aware cases, drive locks, disabling USB ports, and mobile-enabled access may be considered more secure due to the physical access (or sophisticated backdoor access) required in order to be compromised. Each of these is covered in more detail below.Public Safety Canada’s Canadian Cyber Incident Response Centre (CCIRC) is responsible for mitigating and responding to threats to Canada’s critical infrastructure and cyber systems. The CCIRC provides support to mitigate cyber threats, technical support to respond and recover from targeted cyber attacks, and provides online tools for members of Canada’s critical infrastructure sectors. The CCIRC posts regular cyber security bulletins on the Public Safety Canada website. The CCIRC also operates an online reporting tool where individuals and organizations can report a cyber incident. Canada\\'s Cyber Security Strategy is part of a larger, integrated approach to critical infrastructure protection, and functions as a counterpart document to the National Strategy and Action Plan for Critical Infrastructure.This has led to new terms such as cyberwarfare and cyberterrorism. More and more critical infrastructure is being controlled via computer programs that, while increasing efficiency, exposes new vulnerabilities. The test will be to see if governments and corporations that control critical systems such as energy, communications and other information will be able to prevent attacks before they occur. As Jay Cross, the chief scientist of the Internet Time Group, remarked, \"Connectedness begets vulnerability.\"On September 27, 2010, Public Safety Canada partnered with STOP.THINK.CONNECT, a coalition of non-profit, private sector, and government organizations dedicated to informing the general public on how to protect themselves online. On February 4, 2014, the Government of Canada launched the Cyber Security Cooperation Program. The program is a $1.5 million five-year initiative aimed at improving Canada’s cyber systems through grants and contributions to projects in support of this objective. Public Safety Canada aims to begin an evaluation of Canada\\'s Cyber Security Strategy in early 2015. Public Safety Canada administers and routinely updates the GetCyberSafe portal for Canadian citizens, and carries out Cyber Security Awareness Month during October.An unauthorized user gaining physical access to a computer is most likely able to directly download data from it. They may also compromise security by making operating system modifications, installing software worms, keyloggers, or covert listening devices. Even when the system is protected by standard security measures, these may be able to be by passed by booting another operating system or tool from a CD-ROM or other bootable media. Disk encryption and Trusted Platform Module are designed to prevent these attacks.Clickjacking, also known as \"UI redress attack or User Interface redress attack\", is a malicious technique in which an attacker tricks a user into clicking on a button or link on another webpage while the user intended to click on the top level page. This is done using multiple transparent or opaque layers. The attacker is basically \"hijacking\" the clicks meant for the top level page and routing them to some other irrelevant page, most likely owned by someone else. A similar technique can be used to hijack keystrokes. Carefully drafting a combination of stylesheets, iframes, buttons and text boxes, a user can be led into believing that they are typing the password or other information on some authentic webpage while it is being channeled into an invisible frame controlled by the attacker.In 1988, only 60,000 computers were connected to the Internet, and most were mainframes, minicomputers and professional workstations. On November 2, 1988, many started to slow down, because they were running a malicious code that demanded processor time and that spread itself to other computers – the first internet \"computer worm\". The software was traced back to 23-year-old Cornell University graduate student Robert Tappan Morris, Jr. who said \\'he wanted to count how many machines were connected to the Internet\\'.In 2013 and 2014, a Russian/Ukrainian hacking ring known as \"Rescator\" broke into Target Corporation computers in 2013, stealing roughly 40 million credit cards, and then Home Depot computers in 2014, stealing between 53 and 56 million credit card numbers. Warnings were delivered at both corporations, but ignored; physical security breaches using self checkout machines are believed to have played a large role. \"The malware utilized is absolutely unsophisticated and uninteresting,\" says Jim Walter, director of threat intelligence operations at security technology company McAfee – meaning that the heists could have easily been stopped by existing antivirus software had administrators responded to the warnings. The size of the thefts has resulted in major attention from state and Federal United States authorities and the investigation is ongoing.Berlin starts National Cyber Defense Initiative: On June 16, 2011, the German Minister for Home Affairs, officially opened the new German NCAZ (National Center for Cyber Defense) Nationales Cyber-Abwehrzentrum located in Bonn. The NCAZ closely cooperates with BSI (Federal Office for Information Security) Bundesamt für Sicherheit in der Informationstechnik, BKA (Federal Police Organisation) Bundeskriminalamt (Deutschland), BND (Federal Intelligence Service) Bundesnachrichtendienst, MAD (Military Intelligence Service) Amt für den Militärischen Abschirmdienst and other national organisations in Germany taking care of national security aspects. According to the Minister the primary task of the new organisation founded on February 23, 2011, is to detect and prevent attacks against the national infrastructure and mentioned incidents like Stuxnet.',\n",
       "             6: 'Originally based on the English alphabet, ASCII encodes 128 specified characters into seven-bit integers as shown by the ASCII chart on the right. The characters encoded are numbers 0 to 9, lowercase letters a to z, uppercase letters A to Z, basic punctuation symbols, control codes that originated with Teletype machines, and a space. For example, lowercase j would become binary 1101010 and decimal 106. ASCII includes definitions for 128 characters: 33 are non-printing control characters (many now obsolete) that affect how text and space are processed and 95 printable characters, including the space (which is considered an invisible graphic:223).The code itself was patterned so that most control codes were together, and all graphic codes were together, for ease of identification. The first two columns (32 positions) were reserved for control characters.:220, 236\\u2009§\\u20098,9) The \"space\" character had to come before graphics to make sorting easier, so it became position 20hex;:237\\u2009§\\u200910 for the same reason, many special signs commonly used as separators were placed before digits. The committee decided it was important to support uppercase 64-character alphabets, and chose to pattern ASCII so it could be reduced easily to a usable 64-character set of graphic codes,:228, 237\\u2009§\\u200914 as was done in the DEC SIXBIT code. Lowercase letters were therefore not interleaved with uppercase. To keep options available for lowercase letters and other graphics, the special and numeric codes were arranged before the letters, and the letter A was placed in position 41hex to match the draft of the corresponding British standard.:238\\u2009§\\u200918 The digits 0–9 were arranged so they correspond to values in binary prefixed with 011, making conversion with binary-coded decimal straightforward.ASCII was incorporated into the Unicode character set as the first 128 symbols, so the 7-bit ASCII characters have the same numeric codes in both sets. This allows UTF-8 to be backward compatible with 7-bit ASCII, as a UTF-8 file containing only ASCII characters is identical to an ASCII file containing the same sequence of characters. Even more importantly, forward compatibility is ensured as software that recognizes only 7-bit ASCII characters as special and does not alter bytes with the highest bit set (as is often done to support 8-bit ASCII extensions such as ISO-8859-1) will preserve UTF-8 data unchanged.When a Teletype 33 ASR equipped with the automatic paper tape reader received a Control-S (XOFF, an abbreviation for transmit off), it caused the tape reader to stop; receiving Control-Q (XON, \"transmit on\") caused the tape reader to resume. This technique became adopted by several early computer operating systems as a \"handshaking\" signal warning a sender to stop transmission because of impending overflow; it persists to this day in many systems as a manual output control technique. On some systems Control-S retains its meaning but Control-Q is replaced by a second Control-S to resume output. The 33 ASR also could be configured to employ Control-R (DC2) and Control-T (DC4) to start and stop the tape punch; on some units equipped with this function, the corresponding control character lettering on the keycap above the letter was TAPE and TAPE respectively.DEC operating systems (OS/8, RT-11, RSX-11, RSTS, TOPS-10, etc.) used both characters to mark the end of a line so that the console device (originally Teletype machines) would work. By the time so-called \"glass TTYs\" (later called CRTs or terminals) came along, the convention was so well established that backward compatibility necessitated continuing the convention. When Gary Kildall cloned RT-11 to create CP/M he followed established DEC convention. Until the introduction of PC DOS in 1981, IBM had no hand in this because their 1970s operating systems used EBCDIC instead of ASCII and they were oriented toward punch-card input and line printer output on which the concept of carriage return was meaningless. IBM\\'s PC DOS (also marketed as MS-DOS by Microsoft) inherited the convention by virtue of being a clone of CP/M, and Windows inherited it from MS-DOS.C trigraphs were created to solve this problem for ANSI C, although their late introduction and inconsistent implementation in compilers limited their use. Many programmers kept their computers on US-ASCII, so plain-text in Swedish, German etc. (for example, in e-mail or Usenet) contained \"{, }\" and similar variants in the middle of words, something those programmers got used to. For example, a Swedish programmer mailing another programmer asking if they should go for lunch, could get \"N{ jag har sm|rg}sar.\" as the answer, which should be \"Nä jag har smörgåsar.\" meaning \"No I\\'ve got sandwiches.\"The X3.2 subcommittee designed ASCII based on the earlier teleprinter encoding systems. Like other character encodings, ASCII specifies a correspondence between digital bit patterns and character symbols (i.e. graphemes and control characters). This allows digital devices to communicate with each other and to process, store, and communicate character-oriented information such as written language. Before ASCII was developed, the encodings in use included 26 alphabetic characters, 10 numerical digits, and from 11 to 25 special graphic symbols. To include all these, and control characters compatible with the Comité Consultatif International Téléphonique et Télégraphique (CCITT) International Telegraph Alphabet No. 2 (ITA2) standard, Fieldata, and early EBCDIC, more than 64 codes were required for ASCII.ASCII itself was first used commercially during 1963 as a seven-bit teleprinter code for American Telephone & Telegraph\\'s TWX (TeletypeWriter eXchange) network. TWX originally used the earlier five-bit ITA2, which was also used by the competing Telex teleprinter system. Bob Bemer introduced features such as the escape sequence. His British colleague Hugh McGregor Ross helped to popularize this work –  according to Bemer, \"so much so that the code that was to become ASCII was first called the Bemer-Ross Code in Europe\". Because of his extensive work on ASCII, Bemer has been called \"the father of ASCII.\"For example, character 10 represents the \"line feed\" function (which causes a printer to advance its paper), and character 8 represents \"backspace\". RFC 2822 refers to control characters that do not include carriage return, line feed or white space as non-whitespace control characters. Except for the control characters that prescribe elementary line-oriented formatting, ASCII does not define any mechanism for describing the structure or appearance of text within a document. Other schemes, such as markup languages, address page and document layout and formatting.Some software assigned special meanings to ASCII characters sent to the software from the terminal. Operating systems from Digital Equipment Corporation, for example, interpreted DEL as an input character as meaning \"remove previously-typed input character\", and this interpretation also became common in Unix systems. Most other systems used BS for that meaning and used DEL to mean \"remove the character at the cursor\".[citation needed] That latter interpretation is the most common now.[citation needed]Computers attached to the ARPANET included machines running operating systems such as TOPS-10 and TENEX using CR-LF line endings, machines running operating systems such as Multics using LF line endings, and machines running operating systems such as OS/360 that represented lines as a character count followed by the characters of the line and that used EBCDIC rather than ASCII. The Telnet protocol defined an ASCII \"Network Virtual Terminal\" (NVT), so that connections between hosts with different line-ending conventions and character sets could be supported by transmitting a standard text format over the network. Telnet used ASCII along with CR-LF line endings, and software using other conventions would translate between the local conventions and the NVT. The File Transfer Protocol adopted the Telnet protocol, including use of the Network Virtual Terminal, for use when transmitting commands and transferring data in the default ASCII mode. This adds complexity to implementations of those protocols, and to other network protocols, such as those used for E-mail and the World Wide Web, on systems not using the NVT\\'s CR-LF line-ending convention.From early in its development, ASCII was intended to be just one of several national variants of an international character code standard, ultimately published as ISO/IEC 646 (1972), which would share most characters in common but assign other locally useful characters to several code points reserved for \"national use.\" However, the four years that elapsed between the publication of ASCII-1963 and ISO\\'s first acceptance of an international recommendation during 1967 caused ASCII\\'s choices for the national use characters to seem to be de facto standards for the world, causing confusion and incompatibility once other countries did begin to make their own assignments to these code points.Most early home computer systems developed their own 8-bit character sets containing line-drawing and game glyphs, and often filled in some or all of the control characters from 0–31 with more graphics. Kaypro CP/M computers used the \"upper\" 128 characters for the Greek alphabet. The IBM PC defined code page 437, which replaced the control-characters with graphic symbols such as smiley faces, and mapped additional graphic characters to the upper 128 positions. Operating systems such as DOS supported these code pages, and manufacturers of IBM PCs supported them in hardware. Digital Equipment Corporation developed the Multinational Character Set (DEC-MCS) for use in the popular VT220 terminal as one of the first extensions designed more for international languages than for block graphics. The Macintosh defined Mac OS Roman and Postscript also defined a set, both of these contained both international letters and typographic punctuation marks instead of graphics, more like modern character sets.ASCII (i/ˈæski/ ASS-kee), abbreviated from American Standard Code for Information Interchange, is a character-encoding scheme (the IANA prefers the name US-ASCII). ASCII codes represent text in computers, communications equipment, and other devices that use text. Most modern character-encoding schemes are based on ASCII, though they support many additional characters. ASCII was the most common character encoding on the World Wide Web until December 2007, when it was surpassed by UTF-8, which is fully backward compatibe to ASCII.The committee debated the possibility of a shift function (like in ITA2), which would allow more than 64 codes to be represented by a six-bit code. In a shifted code, some character codes determine choices between options for the following character codes. It allows compact encoding, but is less reliable for data transmission as an error in transmitting the shift code typically makes a long part of the transmission unreadable. The standards committee decided against shifting, and so ASCII required at least a seven-bit code.:215, 236\\u2009§\\u20094Many more of the control codes have been given meanings quite different from their original ones. The \"escape\" character (ESC, code 27), for example, was intended originally to allow sending other control characters as literals instead of invoking their meaning. This is the same meaning of \"escape\" encountered in URL encodings, C language strings, and other systems where certain characters have a reserved meaning. Over time this meaning has been co-opted and has eventually been changed. In modern use, an ESC sent to the terminal usually indicates the start of a command sequence, usually in the form of a so-called \"ANSI escape code\" (or, more properly, a \"Control Sequence Introducer\") beginning with ESC followed by a \"[\" (left-bracket) character. An ESC sent from the terminal is most often used as an out-of-band character used to terminate an operation, as in the TECO and vi text editors. In graphical user interface (GUI) and windowing systems, ESC generally causes an application to abort its current operation or to exit (terminate) altogether.Older operating systems such as TOPS-10, along with CP/M, tracked file length only in units of disk blocks and used Control-Z (SUB) to mark the end of the actual text in the file. For this reason, EOF, or end-of-file, was used colloquially and conventionally as a three-letter acronym for Control-Z instead of SUBstitute. The end-of-text code (ETX), also known as Control-C, was inappropriate for a variety of reasons, while using Z as the control code to end a file is analogous to it ending the alphabet and serves as a very convenient mnemonic aid. A historically common and still prevalent convention uses the ETX code convention to interrupt and halt a program via an input data stream, usually from a keyboard.ASCII developed from telegraphic codes. Its first commercial use was as a seven-bit teleprinter code promoted by Bell data services. Work on the ASCII standard began on October 6, 1960, with the first meeting of the American Standards Association\\'s (ASA) X3.2 subcommittee. The first edition of the standard was published during 1963, underwent a major revision during 1967, and experienced its most recent update during 1986. Compared to earlier telegraph codes, the proposed Bell code and ASCII were both ordered for more convenient sorting (i.e., alphabetization) of lists, and added features for devices other than teleprinters.The committee considered an eight-bit code, since eight bits (octets) would allow two four-bit patterns to efficiently encode two digits with binary-coded decimal. However, it would require all data transmission to send eight bits when seven could suffice. The committee voted to use a seven-bit code to minimize costs associated with data transmission. Since perforated tape at the time could record eight bits in one position, it also allowed for a parity bit for error checking if desired.:217, 236\\u2009§\\u20095 Eight-bit machines (with octets as the native data type) that did not use parity checking typically set the eighth bit to 0.With the other special characters and control codes filled in, ASCII was published as ASA X3.4-1963, leaving 28 code positions without any assigned meaning, reserved for future standardization, and one unassigned control code.:66, 245 There was some debate at the time whether there should be more control characters rather than the lowercase alphabet.:435 The indecision did not last long: during May 1963 the CCITT Working Party on the New Telegraph Alphabet proposed to assign lowercase characters to columns 6 and 7, and International Organization for Standardization TC 97 SC 2 voted during October to incorporate the change into its draft standard. The X3.2.4 task group voted its approval for the change to ASCII at its May 1963 meeting. Locating the lowercase letters in columns 6 and 7 caused the characters to differ in bit pattern from the upper case by a single bit, which simplified case-insensitive character matching and the construction of keyboards and printers.Other international standards bodies have ratified character encodings such as ISO/IEC 646 that are identical or nearly identical to ASCII, with extensions for characters outside the English alphabet and symbols used outside the United States, such as the symbol for the United Kingdom\\'s pound sterling (£). Almost every country needed an adapted version of ASCII, since ASCII suited the needs of only the USA and a few other countries. For example, Canada had its own version that supported French characters. Other adapted encodings include ISCII (India), VISCII (Vietnam), and YUSCII (Yugoslavia). Although these encodings are sometimes referred to as ASCII, true ASCII is defined strictly only by the ANSI standard.Probably the most influential single device on the interpretation of these characters was the Teletype Model 33 ASR, which was a printing terminal with an available paper tape reader/punch option. Paper tape was a very popular medium for long-term program storage until the 1980s, less costly and in some ways less fragile than magnetic tape. In particular, the Teletype Model 33 machine assignments for codes 17 (Control-Q, DC1, also known as XON), 19 (Control-S, DC3, also known as XOFF), and 127 (Delete) became de facto standards. The Model 33 was also notable for taking the description of Control-G (BEL, meaning audibly alert the operator) literally as the unit contained an actual bell which it rang when it received a BEL character. Because the keytop for the O key also showed a left-arrow symbol (from ASCII-1963, which had this character instead of underscore), a noncompliant use of code 15 (Control-O, Shift In) interpreted as \"delete previous character\" was also adopted by many early timesharing systems but eventually became neglected.The inherent ambiguity of many control characters, combined with their historical usage, created problems when transferring \"plain text\" files between systems. The best example of this is the newline problem on various operating systems. Teletype machines required that a line of text be terminated with both \"Carriage Return\" (which moves the printhead to the beginning of the line) and \"Line Feed\" (which advances the paper one line without moving the printhead). The name \"Carriage Return\" comes from the fact that on a manual typewriter the carriage holding the paper moved while the position where the typebars struck the ribbon remained stationary. The entire carriage had to be pushed (returned) to the right in order to position the left margin of the paper for the next line.Many of the non-alphanumeric characters were positioned to correspond to their shifted position on typewriters; an important subtlety is that these were based on mechanical typewriters, not electric typewriters. Mechanical typewriters followed the standard set by the Remington No. 2 (1878), the first typewriter with a shift key, and the shifted values of 23456789- were \"#$%_&\\'() –  early typewriters omitted 0 and 1, using O (capital letter o) and l (lowercase letter L) instead, but 1! and 0) pairs became standard once 0 and 1 became common. Thus, in ASCII !\"#$% were placed in second column, rows 1–5, corresponding to the digits 1–5 in the adjacent column. The parentheses could not correspond to 9 and 0, however, because the place corresponding to 0 was taken by the space character. This was accommodated by removing _ (underscore) from 6 and shifting the remaining characters left, which corresponded to many European typewriters that placed the parentheses with 8 and 9. This discrepancy from typewriters led to bit-paired keyboards, notably the Teletype Model 33, which used the left-shifted layout corresponding to ASCII, not to traditional mechanical typewriters. Electric typewriters, notably the more recently introduced IBM Selectric (1961), used a somewhat different layout that has become standard on computers—\\u200b\\u200bfollowing the IBM PC (1981), especially Model M (1984)—\\u200b\\u200band thus shift values for symbols on modern keyboards do not correspond as closely to the ASCII table as earlier keyboards did. The /? pair also dates to the No. 2, and the ,< .> pairs were used on some keyboards (others, including the No. 2, did not shift , (comma) or . (full stop) so they could be used in uppercase without unshifting). However, ASCII split the ;: pair (dating to No. 2), and rearranged mathematical symbols (varied conventions, commonly -* =+) to :* ;+ -=.Code 127 is officially named \"delete\" but the Teletype label was \"rubout\". Since the original standard did not give detailed interpretation for most control codes, interpretations of this code varied. The original Teletype meaning, and the intent of the standard, was to make it an ignored character, the same as NUL (all zeroes). This was useful specifically for paper tape, because punching the all-ones bit pattern on top of an existing mark would obliterate it. Tapes designed to be \"hand edited\" could even be produced with spaces of extra NULs (blank tape) so that a block of characters could be \"rubbed out\" and then replacements put into the empty space.Unfortunately, requiring two characters to mark the end of a line introduces unnecessary complexity and questions as to how to interpret each character when encountered alone. To simplify matters plain text data streams, including files, on Multics used line feed (LF) alone as a line terminator. Unix and Unix-like systems, and Amiga systems, adopted this convention from Multics. The original Macintosh OS, Apple DOS, and ProDOS, on the other hand, used carriage return (CR) alone as a line terminator; however, since Apple replaced these operating systems with the Unix-based OS X operating system, they now use line feed (LF) as well.',\n",
       "             7: 'The Macintosh, however, was expensive, which hindered its ability to be competitive in a market already dominated by the Commodore 64 for consumers, as well as the IBM Personal Computer and its accompanying clone market for businesses. Macintosh systems still found success in education and desktop publishing and kept Apple as the second-largest PC manufacturer for the next decade. In the 1990s, improvements in the rival Wintel platform, notably with the introduction of Windows 3.0, then Windows 95, gradually took market share from the more expensive Macintosh systems. The performance advantage of 68000-based Macintosh systems was eroded by Intel\\'s Pentium, and in 1994 Apple was relegated to third place as Compaq became the top PC manufacturer. Even after a transition to the superior PowerPC-based Power Macintosh (later renamed the PowerMac, in line with the PowerBook series) line in 1994, the falling prices of commodity PC components and the release of Windows 95 saw the Macintosh user base decline.Smith\\'s first Macintosh board was built to Raskin\\'s design specifications: it had 64 kilobytes (kB) of RAM, used the Motorola 6809E microprocessor, and was capable of supporting a 256×256-pixel black-and-white bitmap display. Bud Tribble, a member of the Mac team, was interested in running the Apple Lisa\\'s graphical programs on the Macintosh, and asked Smith whether he could incorporate the Lisa\\'s Motorola 68000 microprocessor into the Mac while still keeping the production cost down. By December 1980, Smith had succeeded in designing a board that not only used the 68000, but increased its speed from 5 MHz to 8 MHz; this board also had the capacity to support a 384×256-pixel display. Smith\\'s design used fewer RAM chips than the Lisa, which made production of the board significantly more cost-efficient. The final Mac design was self-contained and had the complete QuickDraw picture language and interpreter in 64 kB of ROM – far more than most other computers; it had 128 kB of RAM, in the form of sixteen 64 kilobit (kb) RAM chips soldered to the logicboard. Though there were no memory slots, its RAM was expandable to 512 kB by means of soldering sixteen IC sockets to accept 256 kb RAM chips in place of the factory-installed chips. The final product\\'s screen was a 9-inch, 512x342 pixel monochrome display, exceeding the size of the planned screen.Apple spent $2.5 million purchasing all 39 advertising pages in a special, post-election issue of Newsweek, and ran a \"Test Drive a Macintosh\" promotion, in which potential buyers with a credit card could take home a Macintosh for 24 hours and return it to a dealer afterwards. While 200,000 people participated, dealers disliked the promotion, the supply of computers was insufficient for demand, and many were returned in such a bad condition that they could no longer be sold. This marketing campaign caused CEO John Sculley to raise the price from US$1,995 to US$2,495 (about $5,200 when adjusted for inflation in 2010). The computer sold well, nonetheless, reportedly outselling the IBM PCjr which also began shipping early that year. By April 1984 the company sold 50,000 Macintoshes, and hoped for 70,000 by early May and almost 250,000 by the end of the year.Apple released the Macintosh Plus on January 10, 1986, for a price of US$2,600. It offered one megabyte of RAM, easily expandable to four megabytes by the use of socketed RAM boards. It also featured a SCSI parallel interface, allowing up to seven peripherals—such as hard drives and scanners—to be attached to the machine. Its floppy drive was increased to an 800 kB capacity. The Mac Plus was an immediate success and remained in production, unchanged, until October 15, 1990; on sale for just over four years and ten months, it was the longest-lived Macintosh in Apple\\'s history. In September 1986, Apple introduced the Macintosh Programmer\\'s Workshop, or MPW, an application that allowed software developers to create software for Macintosh on Macintosh, rather than cross compiling from a Lisa. In August 1987, Apple unveiled HyperCard and MultiFinder, which added cooperative multitasking to the Macintosh. Apple began bundling both with every Macintosh.With the new Motorola 68030 processor came the Macintosh IIx in 1988, which had benefited from internal improvements, including an on-board MMU. It was followed in 1989 by the Macintosh IIcx, a more compact version with fewer slots  and a version of the Mac SE powered by the 16 MHz 68030, the Macintosh SE/30. Later that year, the Macintosh IIci, running at 25 MHz, was the first Mac to be \"32-bit clean.\" This allowed it to natively support more than 8 MB of RAM, unlike its predecessors, which had \"32-bit dirty\" ROMs (8 of the 32 bits available for addressing were used for OS-level flags). System 7 was the first Macintosh operating system to support 32-bit addressing. The following year, the Macintosh IIfx, starting at US$9,900, was unveiled. Apart from its fast 40 MHz 68030 processor, it had significant internal architectural improvements, including faster memory and two Apple II CPUs (6502s) dedicated to I/O processing.As for Mac OS, System 7 was a 32-bit rewrite from Pascal to C++ that introduced virtual memory and improved the handling of color graphics, as well as memory addressing, networking, and co-operative multitasking. Also during this time, the Macintosh began to shed the \"Snow White\" design language, along with the expensive consulting fees they were paying to Frogdesign. Apple instead brought the design work in-house by establishing the Apple Industrial Design Group, becoming responsible for crafting a new look for all Apple products.When Steve Jobs returned to Apple in 1997 following the company\\'s purchase of NeXT, he ordered that the OS that had been previewed as version 7.7 be branded Mac OS 8 (in place of the never-to-appear Copland OS). Since Apple had licensed only System 7 to third parties, this move effectively ended the clone line. The decision caused significant financial losses for companies like Motorola, who produced the StarMax; Umax, who produced the SuperMac; and Power Computing, who offered several lines of Mac clones, including the PowerWave, PowerTower, and PowerTower Pro. These companies had invested substantial resources in creating their own Mac-compatible hardware. Apple bought out Power Computing\\'s license, but allowed Umax to continue selling Mac clones until their license expired, as they had a sizeable presence in the lower-end segment that Apple did not. In September 1997 Apple extended Umax\\' license allowing them to sell clones with Mac OS 8, the only clone maker to do so, but with the restriction that they only sell low-end systems. Without the higher profit margins of high-end systems, however, Umax judged this would not be profitable and exited the Mac clone market in May 1998, having lost USD$36 million on the program.Mac OS continued to evolve up to version 9.2.2, including retrofits such as the addition of a nanokernel and support for Multiprocessing Services 2.0 in Mac OS 8.6, though its dated architecture made replacement necessary. Initially developed in the Pascal programming language, it was substantially rewritten in C++ for System 7. From its beginnings on an 8 MHz machine with 128 KB of RAM, it had grown to support Apple\\'s latest 1 GHz G4-equipped Macs. Since its architecture was laid down, features that were already common on Apple\\'s competition, like preemptive multitasking and protected memory, had become feasible on the kind of hardware Apple manufactured. As such, Apple introduced Mac OS X, a fully overhauled Unix-based successor to Mac OS 9. OS X uses Darwin, XNU, and Mach as foundations, and is based on NeXTSTEP. It was released to the public in September 2000, as the Mac OS X Public Beta, featuring a revamped user interface called \"Aqua\". At US$29.99, it allowed adventurous Mac users to sample Apple\\'s new operating system and provide feedback for the actual release. The initial version of Mac OS X, 10.0 \"Cheetah\", was released on March 24, 2001. Older Mac OS applications could still run under early Mac OS X versions, using an environment called \"Classic\". Subsequent releases of Mac OS X included 10.1 \"Puma\" (2001), 10.2 \"Jaguar\" (2002), 10.3 \"Panther\" (2003) and 10.4 \"Tiger\" (2005).The current Mac product family uses Intel x86-64 processors. Apple introduced an emulator during the transition from PowerPC chips (called Rosetta), much as it did during the transition from Motorola 68000 architecture a decade earlier. The Macintosh is the only mainstream computer platform to have successfully transitioned to a new CPU architecture, and has done so twice. All current Mac models ship with at least 8 GB of RAM as standard other than the 1.4 GHz Mac Mini, MacBook Pro (without Retina Display), and MacBook Air. Current Mac computers use ATI Radeon or nVidia GeForce graphics cards as well as Intel graphics built into the main CPU. All current Macs (except for the MacBook Pro without Retina Display) do not ship with an optical media drive that includes a dual-function DVD/CD burner. Apple refers to this as a SuperDrive. Current Macs include two standard data transfer ports: USB and Thunderbolt (except for the MacBook (2015 version), which only has a USB-C port and headphone port). MacBook Pro, iMac, MacBook Air, and Mac Mini computers now also feature the \"Thunderbolt\" port, which Apple says can transfer data at speeds up to 10 gigabits per second. USB was introduced in the 1998 iMac G3 and is ubiquitous today, while FireWire is mainly reserved for high-performance devices such as hard drives or video cameras. Starting with the then-new iMac G5, released in October 2005, Apple started to include built-in iSight cameras on appropriate models, and a media center interface called Front Row that can be operated by an Apple Remote or keyboard for accessing media stored on the computer. Front Row has been discontinued as of 2011, however, and the Apple Remote is no longer bundled with new Macs.Originally, the hardware architecture was so closely tied to the Mac OS operating system that it was impossible to boot an alternative operating system. The most common workaround, is to boot into Mac OS and then to hand over control to a Mac OS-based bootloader application. Used even by Apple for A/UX and MkLinux, this technique is no longer necessary since the introduction of Open Firmware-based PCI Macs, though it was formerly used for convenience on many Old World ROM systems due to bugs in the firmware implementation.[citation needed] Now, Mac hardware boots directly from Open Firmware in most PowerPC-based Macs or EFI in all Intel-based Macs.In 1982, Regis McKenna was brought in to shape the marketing and launch of the Macintosh. Later the Regis McKenna team grew to include Jane Anderson, Katie Cadigan and Andy Cunningham, who eventually led the Apple account for the agency. Cunningham and Anderson were the primary authors of the Macintosh launch plan. The launch of the Macintosh pioneered many different tactics that are used today in launching technology products, including the \"multiple exclusive,\" event marketing (credited to John Sculley, who brought the concept over from Pepsi), creating a mystique around a product and giving an inside look into a product\\'s creation.Compaq, who had previously held the third place spot among PC manufacturers during the 1980s and early-mid 1990s, initiated a successful price war in 1994 that vaulted them to the biggest by the year end, overtaking a struggling IBM and relegating Apple to third place. Apple\\'s market share further struggled due to the release of the Windows 95 operating system, which unified Microsoft\\'s formerly separate MS-DOS and Windows products. Windows 95 significantly enhanced the multimedia capability and performance of IBM PC compatible computers, and brought the capabilities of Windows to parity with the Mac OS GUI.Statistics from late 2003 indicate that Apple had 2.06 percent of the desktop share in the United States that had increased to 2.88 percent by Q4 2004. As of October 2006, research firms IDC and Gartner reported that Apple\\'s market share in the U.S. had increased to about 6 percent. Figures from December 2006, showing a market share around 6 percent (IDC) and 6.1 percent (Gartner) are based on a more than 30 percent increase in unit sale from 2005 to 2006. The installed base of Mac computers is hard to determine, with numbers ranging from 5% (estimated in 2009) to 16% (estimated in 2005).The Macintosh SE was released at the same time as the Macintosh II for $2900 (or $3900 with hard drive), as the first compact Mac with a 20 MB internal hard drive and an expansion slot. The SE\\'s expansion slot was located inside the case along with the CRT, potentially exposing an upgrader to high voltage. For this reason, Apple recommended users bring their SE to an authorized Apple dealer to have upgrades performed. The SE also updated Jerry Manock and Terry Oyama\\'s original design and shared the Macintosh II\\'s Snow White design language, as well as the new Apple Desktop Bus (ADB) mouse and keyboard that had first appeared on the Apple IIGS some months earlier.In recent years, Apple has seen a significant boost in sales of Macs. This has been attributed, in part, to the success of the iPod and the iPhone, a halo effect whereby satisfied iPod or iPhone owners purchase more Apple products, and Apple has since capitalized on that with the iCloud cloud service that allows users to seamlessly sync data between these devices and Macs. Nonetheless, like other personal computer manufacturers, the Macintosh lines have been hurt by consumer trend towards smartphones and tablet computers (particularly Apple\\'s own iPhone and iPad, respectively) as the computing devices of choice among consumers.In response, Apple introduced a range of relatively inexpensive Macs in October 1990. The Macintosh Classic, essentially a less expensive version of the Macintosh SE, was the least expensive Mac offered until early 2001. The 68020-powered Macintosh LC, in its distinctive \"pizza box\" case, offered color graphics and was accompanied by a new, low-cost 512×384 pixel monitor. The Macintosh IIsi was essentially a 20 MHz IIci with only one expansion slot. All three machines sold well, although Apple\\'s profit margin on them was considerably lower than that on earlier models.Starting in 2006, Apple\\'s industrial design shifted to favor aluminum, which was used in the construction of the first MacBook Pro. Glass was added in 2008 with the introduction of the unibody MacBook Pro. These materials are billed as environmentally friendly. The iMac, MacBook Pro, MacBook Air, and Mac Mini lines currently all use aluminum enclosures, and are now made of a single unibody. Chief designer Jonathan Ive continues to guide products towards a minimalist and simple feel, including eliminating of replaceable batteries in notebooks. Multi-touch gestures from the iPhone\\'s interface have been applied to the Mac line in the form of touch pads on notebooks and the Magic Mouse and Magic Trackpad for desktops.The Macintosh project was begun in 1979 by Jef Raskin, an Apple employee who envisioned an easy-to-use, low-cost computer for the average consumer. He wanted to name the computer after his favorite type of apple, the McIntosh, but the spelling was changed to \"Macintosh\" for legal reasons as the original was the same spelling as that used by McIntosh Laboratory, Inc., the audio equipment manufacturer. Steve Jobs requested that McIntosh Laboratory give Apple a release for the name with its changed spelling so that Apple could use it, but the request was denied, forcing Apple to eventually buy the rights to use the name. (A 1984 Byte Magazine article suggested Apple changed the spelling only after \"early users\" misspelled \"McIntosh\". However, Jef Raskin had adopted the Macintosh spelling by 1981, when the Macintosh computer was still a single prototype machine in the lab. This explanation further clashes with the first explanation given above that the change was made for \"legal reasons.\")After the Lisa\\'s announcement, John Dvorak discussed rumors of a mysterious \"MacIntosh\" project at Apple in February 1983. The company announced the Macintosh 128K—manufactured at an Apple factory in Fremont, California—in October 1983, followed by an 18-page brochure included with various magazines in December. The Macintosh was introduced by a US$1.5 million Ridley Scott television commercial, \"1984\". It most notably aired during the third quarter of Super Bowl XVIII on January 22, 1984, and is now considered a \"watershed event\" and a \"masterpiece.\" Regis McKenna called the ad \"more successful than the Mac itself.\" \"1984\" used an unnamed heroine to represent the coming of the Macintosh (indicated by a Picasso-style picture of the computer on her white tank top) as a means of saving humanity from the \"conformity\" of IBM\\'s attempts to dominate the computer industry. The ad alludes to George Orwell\\'s novel, Nineteen Eighty-Four, which described a dystopian future ruled by a televised \"Big Brother.\"The Macintosh (/ˈmækᵻntɒʃ/ MAK-in-tosh; branded as Mac since 1997) is a series of personal computers (PCs) designed, developed, and marketed by Apple Inc. Steve Jobs introduced the original Macintosh computer on January 24, 1984. This was the first mass-market personal computer featuring an integral graphical user interface and mouse. This first model was later renamed to \"Macintosh 128k\" for uniqueness amongst a populous family of subsequently updated models which are also based on Apple\\'s same proprietary architecture. Since 1998, Apple has largely phased out the Macintosh name in favor of \"Mac\", though the product family has been nicknamed \"Mac\" or \"the Mac\" since the development of the first model.In 1985, the combination of the Mac, Apple\\'s LaserWriter printer, and Mac-specific software like Boston Software\\'s MacPublisher and Aldus PageMaker enabled users to design, preview, and print page layouts complete with text and graphics—an activity to become known as desktop publishing. Initially, desktop publishing was unique to the Macintosh, but eventually became available for other platforms. Later, applications such as Macromedia FreeHand, QuarkXPress, and Adobe\\'s Photoshop and Illustrator strengthened the Mac\\'s position as a graphics computer and helped to expand the emerging desktop publishing market.Raskin was authorized to start hiring for the project in September 1979, and he immediately asked his long-time colleague, Brian Howard, to join him. His initial team would eventually consist of himself, Howard, Joanna Hoffman, Burrell Smith, and Bud Tribble. The rest of the original Mac team would include Bill Atkinson, Bob Belleville, Steve Capps, George Crow, Donn Denman, Chris Espinosa, Andy Hertzfeld, Bruce Horn, Susan Kare, Larry Kenyon, and Caroline Rose with Steve Jobs leading the project.From 2001 to 2008, Mac sales increased continuously on an annual basis. Apple reported worldwide sales of 3.36 million Macs during the 2009 holiday season. As of Mid-2011, the Macintosh continues to enjoy rapid market share increase in the US, growing from 7.3% of all computer shipments in 2010 to 9.3% in 2011. According to IDC\\'s quarterly PC tracker, globally, in 3rd quarter of 2014, Apple\\'s PC market share increased 5.7 percent year over year, with record sales of 5.5 million units. Apple now sits in the number five spot, with a global market share of about 6% during 2014, behind Lenovo, HP, Dell and Acer.In 1987, Apple spun off its software business as Claris. It was given the code and rights to several applications, most notably MacWrite, MacPaint, and MacProject. In the late 1980s, Claris released a number of revamped software titles; the result was the \"Pro\" series, including MacDraw Pro, MacWrite Pro, and FileMaker Pro. To provide a complete office suite, Claris purchased the rights to the Informix Wingz spreadsheet program on the Mac, renaming it Claris Resolve, and added the new presentation software Claris Impact. By the early 1990s, Claris applications were shipping with the majority of consumer-level Macintoshes and were extremely popular. In 1991, Claris released ClarisWorks, which soon became their second best-selling application. When Claris was reincorporated back into Apple in 1998, ClarisWorks was renamed AppleWorks beginning with version 5.0.Two days after \"1984\" aired, the Macintosh went on sale, and came bundled with two applications designed to show off its interface: MacWrite and MacPaint. It was first demonstrated by Steve Jobs in the first of his famous Mac keynote speeches, and though the Mac garnered an immediate, enthusiastic following, some labeled it a mere \"toy.\" Because the operating system was designed largely around the GUI, existing text-mode and command-driven applications had to be redesigned and the programming code rewritten. This was a time-consuming task that many software developers chose not to undertake, and could be regarded as a reason for an initial lack of software for the new system. In April 1984, Microsoft\\'s MultiPlan migrated over from MS-DOS, with Microsoft Word following in January 1985. In 1985, Lotus Software introduced Lotus Jazz for the Macintosh platform after the success of Lotus 1-2-3 for the IBM PC, although it was largely a flop. Apple introduced the Macintosh Office suite the same year with the \"Lemmings\" ad. Infamous for insulting its own potential customers, the ad was not successful.Apple has generally dominated the premium PC market, having a 91 percent market share for PCs priced at more than $1,000 in 2009, according to NPD. The Macintosh took 45 percent of operating profits in the PC industry during Q4 2012, compared to 13 percent for Dell, seven percent for Hewlett Packard, six percent for Lenovo and Asus, and one percent for Acer. While sales of the Macintosh have largely held steady, in comparison to Apple\\'s sales of the iPhone and iPad which increased significantly during the 2010s, Macintosh computers still enjoy high margins on a per unit basis, with the majority being their MacBooks that are focused on the ultraportable niche that is the most profitable and only growing segment of PCs. It also helped that the Macintosh lineup is simple, updated on a yearly schedule, and consistent across both Apple retail stores, and authorized resellers where they have a special \"store within a store\" section to distinguish them from Windows PCs. In contrast, Windows PC manufacturers generally have a wide range of offerings, selling only a portion through retail with a full selection on the web, and often with limited-time or region-specific models. The Macintosh ranked third on the \"list of intended brands for desktop purchases\" for the 2011 holiday season, then moved up to second in 2012 by displacing Hewlett Packard, and in 2013 took the top spot ahead of Dell.The Macintosh\\'s minimal memory became apparent, even compared with other personal computers in 1984, and could not be expanded easily. It also lacked a hard disk drive or the means to easily attach one. Many small companies sprang up to address the memory issue. Suggestions revolved around either upgrading the memory to 512 KB or removing the computer\\'s 16 memory chips and replacing them with larger-capacity chips, a tedious and difficult operation. In October 1984, Apple introduced the Macintosh 512K, with quadruple the memory of the original, at a price of US$3,195. It also offered an upgrade for 128k Macs that involved replacing the logic board.In 1988, Apple sued Microsoft and Hewlett-Packard on the grounds that they infringed Apple\\'s copyrighted GUI, citing (among other things) the use of rectangular, overlapping, and resizable windows. After four years, the case was decided against Apple, as were later appeals. Apple\\'s actions were criticized by some in the software community, including the Free Software Foundation (FSF), who felt Apple was trying to monopolize on GUIs in general, and boycotted GNU software for the Macintosh platform for seven years.Furthermore, Apple had created too many similar models that confused potential buyers. At one point, its product lineup was subdivided into Classic, LC, II, Quadra, Performa, and Centris models, with essentially the same computer being sold under a number of different names. These models competed against Macintosh clones, hardware manufactured by third parties that ran Apple\\'s System 7. This succeeded in increasing the Macintosh\\'s market share somewhat, and provided cheaper hardware for consumers, but hurt Apple financially as existing Apple customers began to buy cheaper clones which cannibalized the sales of Apple\\'s higher-margin Macintosh systems, yet Apple still shouldered the burden of developing the Mac OS platform.In early 2001, Apple began shipping computers with CD-RW drives and emphasized the Mac\\'s ability to play DVDs by including DVD-ROM and DVD-RAM drives as standard. Steve Jobs admitted that Apple had been \"late to the party\" on writable CD technology, but felt that Macs could become a \"digital hub\" that linked and enabled an \"emerging digital lifestyle\". Apple would later introduce an update to its iTunes music player software that enabled it to burn CDs, along with a controversial \"Rip, Mix, Burn\" advertising campaign that some felt encouraged media piracy. This accompanied the release of the iPod, Apple\\'s first successful handheld device. Apple continued to launch products, such as the unsuccessful Power Mac G4 Cube, the education-oriented eMac, and the titanium (and later aluminium) PowerBook G4 laptop for professionals.It was not long until Apple released their first portable computer, the Macintosh Portable in 1989. Although due to considerable design issues, it was soon replaced in 1991 with the first of the PowerBook line: the PowerBook 100, a miniaturized portable; the 16 MHz 68030 PowerBook 140; and the 25 MHz 68030 PowerBook 170. They were the first portable computers with the keyboard behind a palm rest and a built-in pointing device (a trackball) in front of the keyboard. The 1993 PowerBook 165c was Apple\\'s first portable computer to feature a color screen, displaying 256 colors with 640 x 400-pixel resolution. The second generation of PowerBooks, the 68040-equipped 500 series, introduced trackpads, integrated stereo speakers, and built-in Ethernet to the laptop form factor in 1994.In 2001, Apple introduced Mac OS X, based on Darwin and NEXTSTEP; its new features included the Dock and the Aqua user interface. During the transition, Apple included a virtual machine subsystem known as Classic, allowing users to run Mac OS 9 applications under Mac OS X 10.4 and earlier on PowerPC machines. Apple introduced Mac OS X 10.8 in February, and it was made available in the summer of 2012. Mountain Lion includes many new features, such as Mission Control, the Mac App Store (available to Mac OS X v10.6.6 \"Snow Leopard.\" users by software update), Launchpad, an application viewer and launcher akin to the iOS Home Screen, and Resume, a feature similar to the hibernate function found in Microsoft Windows. The most recent version is OS X El Capitan . In addition to Mavericks, all new Macs are bundled with assorted Apple-produced applications, including iLife, the Safari web browser and the iTunes media player. Apple introduced Mavericks at WWDC 2013 in June, and released it on October 15 of that year. It is free of charge to everyone running Snow Leopard or later and is compatible with most Macs from 2007 and later. Mavericks brought a lot of the iOS apps, functions, and feel to the Mac as well as better multi display support, iBooks, Maps, app nap, and other upgrades to improve performance and battery life.In 2000, Apple released the Power Mac G4 Cube, their first desktop since the discontinued Power Macintosh G3, to slot between the iMac G3 and the Power Mac G4. Even with its innovative design, it was initially priced US$200 higher than the comparably-equipped and more-expandable base Power Mac G4, while also not including a monitor, making it too expensive and resulting in slow sales. Apple sold just 29,000 Cubes in Q4 of 2000 which was one third of expectations, compared to 308,000 Macs during that same quarter, and Cube sales dropped to 12,000 units in Q1 of 2001. A price drop and hardware upgrades could not offset the earlier perception of the Cube\\'s reduced value compared to the iMac and Power Mac G4 lineup, and it was discontinued in July 2001.Historically, Mac OS X enjoyed a near-absence of the types of malware and spyware that affect Microsoft Windows users. Mac OS X has a smaller usage share compared to Microsoft Windows (roughly 5% and 92%, respectively), but it also has traditionally more secure UNIX roots. Worms, as well as potential vulnerabilities, were noted in February 2006, which led some industry analysts and anti-virus companies to issue warnings that Apple\\'s Mac OS X is not immune to malware. Increasing market share coincided with additional reports of a variety of attacks. Apple releases security updates for its software. In early 2011, Mac OS X experienced a large increase in malware attacks, and malware such as Mac Defender, MacProtector, and MacGuard were seen as an increasing problem for Mac users. At first, the malware installer required the user to enter the administrative password, but later versions were able to install without user input. Initially, Apple support staff were instructed not to assist in the removal of the malware or admit the existence of the malware issue, but as the malware spread, a support document was issued. Apple announced an OS X update to fix the problem. An estimated 100,000 users were affected.By March 2011, the market share of OS X in North America had increased to slightly over 14%. Whether the size of the Mac\\'s market share and installed base is relevant, and to whom, is a hotly debated issue. Industry pundits have often called attention to the Mac\\'s relatively small market share to predict Apple\\'s impending doom, particularly in the early and mid-1990s when the company\\'s future seemed bleakest. Others argue that market share is the wrong way to judge the Mac\\'s success. Apple has positioned the Mac as a higher-end personal computer, and so it may be misleading to compare it to a budget PC. Because the overall market for personal computers has grown rapidly, the Mac\\'s increasing sales numbers are effectively swamped by the industry\\'s expanding sales volume as a whole. Apple\\'s small market share, then, gives the impression that fewer people are using Macs than did ten years ago, when exactly the opposite is true. Soaring sales of the iPhone and iPad mean that the portion of Apple\\'s profits represented by the Macintosh has declined in 2010, dropping to 24% from 46% two years earlier. Others try to de-emphasize market share, citing that it is rarely brought up in other industries. Regardless of the Mac\\'s market share, Apple has remained profitable since Steve Jobs\\' return and the company\\'s subsequent reorganization. Notably, a report published in the first quarter of 2008 found that Apple had a 14% market share in the personal computer market in the US, including 66% of all computers over $1,000. Market research indicates that Apple draws its customer base from a higher-income demographic than the mainstream personal computer market.Notwithstanding these technical and commercial successes on the Macintosh platform, their systems remained fairly expensive, making them less competitive in light of the falling costs of components that made IBM PC compatibles cheaper and accelerated their adoption. In 1989, Jean-Louis Gassée had steadfastly refused to lower the profit margins on Mac computers, then there was a component shortage that rocked the exponentially-expanding PC industry that year, forcing Apple USA head Allan Loren to cut prices which dropped Apple\\'s margins. Microsoft Windows 3.0 was released in May 1990, the first iteration of Windows which had a feature set and performance comparable to the significantly costlier Macintosh. Furthermore, Apple had created too many similar models that confused potential buyers; at one point the product lineup was subdivided into Classic, LC, II, Quadra, Performa, and Centris models, with essentially the same computer being sold under a number of different names.Starting in 2002, Apple moved to eliminate CRT displays from its product line as part of aesthetic design and space-saving measures with the iMac G4. However, the new iMac with its flexible LCD flat-panel monitor was considerably more expensive on its debut than the preceding iMac G3, largely due to the higher cost of the LCD technology at the time. In order to keep the Macintosh affordable for the education market and due to obsolescence of the iMac G3, Apple created the eMac in April 2002 as the intended successor; however the eMac\\'s CRT made it relatively bulky and somewhat outdated, while its all-in-one construction meant it could not be expanded to meet consumer demand for larger monitors. The iMac G4\\'s relatively high prices were approaching that of laptops which were portable and had higher resolution LCD screens. Meanwhile, Windows PC manufacturers could offer desktop configurations with LCD flat panel monitors at prices comparable to the eMac and at much lower cost than the iMac G4. The flop of the Power Mac G4 Cube, along with the more expensive iMac G4 and heavy eMac, meant that Macintosh desktop sales never reached the market share attained by the previous iMac G3. For the next half-decade while Macintosh sales held steady, it would instead be the iPod portable music player and iTunes music download service that would drive Apple\\'s sales growth.The sales breakdown of the Macintosh have seen sales of desktop Macs stayed mostly constant while being surpassed by that of Mac notebooks whose sales rate has grown considerably; seven out of ten Macs sold were laptops in 2009, a ratio projected to rise to three out of four by 2010. The change in sales of form factors is due to the desktop iMac moving from affordable (iMac G3) to upscale (iMac G4) and subsequent releases are considered premium all-in-ones. By contrast the MSRP of the MacBook laptop lines have dropped through successive generations such that the MacBook Air and MacBook Pro constitute the lowest price of entry to a Mac, with the exception of the even more inexpensive Mac Mini (the only sub-$1000 offering from Apple, albeit without a monitor and keyboard), not surprisingly the MacBooks are the top-selling form factors of the Macintosh platform today. The use of Intel microprocessors has helped Macs more directly compete with their Windows counterparts on price and performance, and by the 2010s Apple was receiving Intel\\'s latest CPUs first before other PC manufacturers.In 1998, after the return of Steve Jobs, Apple consolidated its multiple consumer-level desktop models into the all-in-one iMac G3, which became a commercial success and revitalized the brand. Since their transition to Intel processors in 2006, the complete lineup is entirely based on said processors and associated systems. Its current lineup comprises three desktops (the all-in-one iMac, entry-level Mac mini, and the Mac Pro tower graphics workstation), and four laptops (the MacBook, MacBook Air, MacBook Pro, and MacBook Pro with Retina display). Its Xserve server was discontinued in 2011 in favor of the Mac Mini and Mac Pro.Burrel\\'s innovative design, which combined the low production cost of an Apple II with the computing power of Lisa\\'s CPU, the Motorola 68K, received the attention of Steve Jobs, co-founder of Apple. Realizing that the Macintosh was more marketable than the Lisa, he began to focus his attention on the project. Raskin left the team in 1981 over a personality conflict with Jobs. Team member Andy Hertzfeld said that the final Macintosh design is closer to Jobs\\' ideas than Raskin\\'s. After hearing of the pioneering GUI technology being developed at Xerox PARC, Jobs had negotiated a visit to see the Xerox Alto computer and its Smalltalk development tools in exchange for Apple stock options. The Lisa and Macintosh user interfaces were influenced by technology seen at Xerox PARC and were combined with the Macintosh group\\'s own ideas. Jobs also commissioned industrial designer Hartmut Esslinger to work on the Macintosh line, resulting in the \"Snow White\" design language; although it came too late for the earliest Macs, it was implemented in most other mid- to late-1980s Apple computers. However, Jobs\\' leadership at the Macintosh project did not last; after an internal power struggle with new CEO John Sculley, Jobs resigned from Apple in 1985. He went on to found NeXT, another computer company targeting the education market, and did not return until 1997, when Apple acquired NeXT.Jobs stated during the Macintosh\\'s introduction \"we expect Macintosh to become the third industry standard\", after the Apple II and IBM PC. Although outselling every other computer, it did not meet expectations during the first year, especially among business customers. Only about ten applications including MacWrite and MacPaint were widely available, although many non-Apple software developers participated in the introduction and Apple promised that 79 companies including Lotus, Digital Research, and Ashton-Tate were creating products for the new computer. After one year, it had less than one quarter of the software selection available compared to the IBM PC—including only one word processor, two databases, and one spreadsheet—although Apple had sold 280,000 Macintoshes compared to IBM\\'s first year sales of fewer than 100,000 PCs.Updated Motorola CPUs made a faster machine possible, and in 1987 Apple took advantage of the new Motorola technology and introduced the Macintosh II at $5500, powered by a 16 MHz Motorola 68020 processor. The primary improvement in the Macintosh II was Color QuickDraw in ROM, a color version of the graphics language which was the heart of the machine. Among the many innovations in Color QuickDraw were the ability to handle any display size, any color depth, and multiple monitors. The Macintosh II marked the start of a new direction for the Macintosh, as now for the first time it had an open architecture with several NuBus expansion slots, support for color graphics and external monitors, and a modular design similar to that of the IBM PC. It had an internal hard drive and a power supply with a fan, which was initially fairly loud. One third-party developer sold a device to regulate fan speed based on a heat sensor, but it voided the warranty. Later Macintosh computers had quieter power supplies and hard drives.Microsoft Windows 3.0 was released in May 1990, and according to a common saying at the time \"Windows was not as good as Macintosh, but it was good enough for the average user\". Though still a graphical wrapper that relied upon MS-DOS, 3.0 was the first iteration of Windows which had a feature set and performance comparable to the much more expensive Macintosh platform. It also did not help matters that during the previous year Jean-Louis Gassée had steadfastly refused to lower the profit margins on Mac computers. Finally, there was a component shortage that rocked the exponentially-expanding PC industry in 1989, forcing Apple USA head Allan Loren to cut prices which dropped Apple\\'s margins.Intel had tried unsuccessfully to push Apple to migrate the Macintosh platform to Intel chips. Apple concluded that Intel\\'s CISC (Complex Instruction Set Computer) architecture ultimately would not be able to compete against RISC (Reduced Instruction Set Computer) processors. While the Motorola 68040 offered the same features as the Intel 80486 and could on a clock-for-clock basis significantly outperform the Intel chip, the 486 had the ability to be clocked significantly faster without suffering from overheating problems, especially the clock-doubled i486DX2 which ran the CPU logic at twice the external bus speed, giving such equipped IBM compatible systems a significant performance lead over their Macintosh equivalents. Apple\\'s product design and engineering didn\\'t help matters as they restricted the use of the \\'040 to their expensive Quadras for a time while the 486 was readily available to OEMs as well as enthusiasts who put together their own machines. In late 1991, as the higher-end Macintosh desktop lineup transitioned to the \\'040, Apple was unable to offer the \\'040 in their top-of-the-line PowerBooks until early 1994 with the PowerBook 500 series, several years after the first 486-powered IBM compatible laptops hit the market which cost Apple considerable sales. In 1993 Intel rolled out the Pentium processors as the successor to the 486, while the Motorola 68050 was never released, leaving the Macintosh platform a generation behind IBM compatibles in the latest CPU technology. In 1994, Apple abandoned Motorola CPUs for the RISC PowerPC architecture developed by the AIM alliance of Apple Computer, IBM, and Motorola. The Power Macintosh line, the first to use the new chips, proved to be highly successful, with over a million PowerPC units sold in nine months. However, in the long run, spurning Intel for the PowerPC was a mistake as the commoditization of Intel-architecture chips meant Apple couldn\\'t compete on price against \"the Dells of the world\".In 1998, Apple introduced its new iMac which, like the original 128K Mac, was an all-in-one computer. Its translucent plastic case, originally Bondi blue and later various additional colors, is considered an industrial design landmark of the late 1990s. The iMac did away with most of Apple\\'s standard (and usually proprietary) connections, such as SCSI and ADB, in favor of two USB ports. It replaced a floppy disk drive with a CD-ROM drive for installing software, but was incapable of writing to CDs or other media without external third-party hardware. The iMac proved to be phenomenally successful, with 800,000 units sold in 139 days. It made the company an annual profit of US$309 million, Apple\\'s first profitable year since Michael Spindler took over as CEO in 1995. This aesthetic was applied to the Power Macintosh and later the iBook, Apple\\'s first consumer-level laptop computer, filling the missing quadrant of Apple\\'s \"four-square product matrix\" (desktop and portable products for both consumers and professionals). More than 140,000 pre-orders were placed before it started shipping in September, and by October proved to be a large success.Apple discontinued the use of PowerPC microprocessors in 2006. At WWDC 2005, Steve Jobs announced this transition, revealing that Mac OS X was always developed to run on both the Intel and PowerPC architectures. All new Macs now use x86 processors made by Intel, and some were renamed as a result. Intel-based Macs running OS X 10.6 and below (support has been discontinued since 10.7) can run pre-existing software developed for PowerPC using an emulator called Rosetta, although at noticeably slower speeds than native programs. However, the Classic environment is unavailable on the Intel architecture. Intel chips introduced the potential to run the Microsoft Windows operating system natively on Apple hardware, without emulation software such as Virtual PC. In March 2006, a group of hackers announced that they were able to run Windows XP on an Intel-based Mac. The group released their software as open source and has posted it for download on their website. On April 5, 2006, Apple announced the availability of the public beta of Boot Camp, software that allows owners of Intel-based Macs to install Windows XP on their machines; later versions added support for Windows Vista and Windows 7. Classic was discontinued in Mac OS X 10.5, and Boot Camp became a standard feature on Intel-based Macs.Apple was initially reluctant to embrace mice with multiple buttons and scroll wheels. Macs did not natively support pointing devices that featured multiple buttons, even from third parties, until Mac OS X arrived in 2001. Apple continued to offer only single button mice, in both wired and Bluetooth wireless versions, until August 2005, when it introduced the Mighty Mouse. While it looked like a traditional one-button mouse, it actually had four buttons and a scroll ball, capable of independent x- and y-axis movement. A Bluetooth version followed in July 2006. In October 2009, Apple introduced the Magic Mouse, which uses multi-touch gesture recognition (similar to that of the iPhone) instead of a physical scroll wheel or ball. It is available only in a wireless configuration, but the wired Mighty Mouse (re-branded as \"Apple Mouse\") is still available as an alternative. Since 2010, Apple has also offered the Magic Trackpad as a means to control Macintosh desktop computers in a way similar to laptops.Following the release of Intel-based Macs, third-party platform virtualization software such as Parallels Desktop, VMware Fusion, and VirtualBox began to emerge. These programs allow users to run Microsoft Windows or previously Windows-only software on Macs at near native speed. Apple also released Boot Camp and Mac-specific Windows drivers that help users to install Windows XP or Vista and natively dual boot between Mac OS X and Windows. Though not condoned by Apple, it is possible to run the Linux operating system using Boot camp or other virtualization workarounds. Unlike most PCs, however, Macs are unable to run many legacy PC operating systems. In particular, Intel-based macs lack the A20 gate.Although the PC market declined, Apple still managed to ship 2.8 million MacBooks in Q2 2012 (the majority of which are the MacBook Air) compared to 500,000 total Ultrabooks, although there were dozens of Ultrabooks from various manufacturers on the market while Apple only offered 11-inch and 13-inch models of the MacBook Air. The Air has been the best-selling ultra-portable in certain countries over Windows Ultrabooks, particularly the United States. While several Ultrabooks were able to claim individual distinctions such as being the lightest or thinnest, the Air was regarded by reviewers as the best all-around subnotebook/ultraportable in regard to \"OS X experience, full keyboard, superior trackpad, Thunderbolt connector and the higher-quality, all-aluminum unibody construction\". The Air was among the first to receive Intel\\'s latest CPUs before other PC manufacturers, and OS X has gained market share on Windows in recent years. Through July 1, 2013, the MacBook Air took in 56 percent of all Ultrabook sales in the United States, although being one of the higher-priced competitors, though several Ultrabooks with better features were often more expensive than the MacBook Air. The competitive pricing of MacBooks was particularly effective when rivals charged more for seemingly equivalent Ultrabooks, as this contradicted the established \"elitist aura\" perception that Apple products cost more but were higher quality, which made these most expensive Ultrabooks seem exorbitant no matter how valid their higher prices were.',\n",
       "             8: 'In psychology, memory is the process in which information is encoded, stored, and retrieved. Encoding allows information from the outside world to be sensed in the form of chemical and physical stimuli. In the first stage the information must be changed so that it may be put into the encoding process. Storage is the second memory stage or process. This entails that information is maintained over short periods of time. Finally the third process is the retrieval of information that has been stored. Such information must be located and returned to the consciousness. Some retrieval attempts may be effortless due to the type of information, and other attempts to remember stored information may be more demanding for various reasons.Short-term memory is believed to rely mostly on an acoustic code for storing information, and to a lesser extent a visual code. Conrad (1964) found that test subjects had more difficulty recalling collections of letters that were acoustically similar (e.g. E, P, D). Confusion with recalling acoustically similar letters rather than visually similar letters implies that the letters were encoded acoustically. Conrad\\'s (1964) study, however, deals with the encoding of written text; thus, while memory of written language may rely on acoustic components, generalisations to all forms of memory cannot be made.Short-term memory is also known as working memory. Short-term memory allows recall for a period of several seconds to a minute without rehearsal. Its capacity is also very limited: George A. Miller (1956), when working at Bell Laboratories, conducted experiments showing that the store of short-term memory was 7±2 items (the title of his famous paper, \"The magical number 7±2\"). Modern estimates of the capacity of short-term memory are lower, typically of the order of 4–5 items; however, memory capacity can be increased through a process called chunking. For example, in recalling a ten-digit telephone number, a person could chunk the digits into three groups: first, the area code (such as 123), then a three-digit chunk (456) and lastly a four-digit chunk (7890). This method of remembering telephone numbers is far more effective than attempting to remember a string of 10 digits; this is because we are able to chunk the information into meaningful groups of numbers. This may be reflected in some countries in the tendency to display telephone numbers as several chunks of two to four numbers.The storage in sensory memory and short-term memory generally has a strictly limited capacity and duration, which means that information is not retained indefinitely. By contrast, long-term memory can store much larger quantities of information for potentially unlimited duration (sometimes a whole life span). Its capacity is immeasurable. For example, given a random seven-digit number we may remember it for only a few seconds before forgetting, suggesting it was stored in our short-term memory. On the other hand, we can remember telephone numbers for many years through repetition; this information is said to be stored in long-term memory.The model also shows all the memory stores as being a single unit whereas research into this shows differently. For example, short-term memory can be broken up into different units such as visual information and acoustic information. In a study by Zlonoga and Gerber (1986), patient \\'KF\\' demonstrated certain deviations from the Atkinson–Shiffrin model. Patient KF was brain damaged, displaying difficulties regarding short-term memory. Recognition of sounds such as spoken numbers, letters, words and easily identifiable noises (such as doorbells and cats meowing) were all impacted. Interestingly, visual short-term memory was unaffected, suggesting a dichotomy between visual and audial memory.Short-term memory is supported by transient patterns of neuronal communication, dependent on regions of the frontal lobe (especially dorsolateral prefrontal cortex) and the parietal lobe. Long-term memory, on the other hand, is maintained by more stable and permanent changes in neural connections widely spread throughout the brain. The hippocampus is essential (for learning new information) to the consolidation of information from short-term to long-term memory, although it does not seem to store information itself. Without the hippocampus, new memories are unable to be stored into long-term memory, as learned from patient Henry Molaison after removal of both his hippocampi, and there will be a very short attention span. Furthermore, it may be involved in changing neural connections for a period of three months or more after the initial learning.Sensory memory holds sensory information less than one second after an item is perceived. The ability to look at an item and remember what it looked like with just a split second of observation, or memorization, is the example of sensory memory. It is out of cognitive control and is an automatic response. With very short presentations, participants often report that they seem to \"see\" more than they can actually report. The first experiments exploring this form of sensory memory were conducted by George Sperling (1963) using the \"partial report paradigm\". Subjects were presented with a grid of 12 letters, arranged into three rows of four. After a brief presentation, subjects were then played either a high, medium or low tone, cuing them which of the rows to report. Based on these partial report experiments,Sperling was able to show that the capacity of sensory memory was approximately 12 items, but that it degraded very quickly (within a few hundred milliseconds). Because this form of memory degrades so quickly, participants would see the display but be unable to report all of the items (12 in the \"whole report\" procedure) before they decayed. This type of memory cannot be prolonged via rehearsal.While short-term memory encodes information acoustically, long-term memory encodes it semantically: Baddeley (1966) discovered that, after 20 minutes, test subjects had the most difficulty recalling a collection of words that had similar meanings (e.g. big, large, great, huge) long-term. Another part of long-term memory is episodic memory, \"which attempts to capture information such as \\'what\\', \\'when\\' and \\'where\\'\". With episodic memory, individuals are able to recall specific events such as birthday parties and weddings.Infants do not have the language ability to report on their memories and so verbal reports cannot be used to assess very young children’s memory. Throughout the years, however, researchers have adapted and developed a number of measures for assessing both infants’ recognition memory and their recall memory. Habituation and operant conditioning techniques have been used to assess infants’ recognition memory and the deferred and elicited imitation techniques have been used to assess infants’ recall memory.Another major way to distinguish different memory functions is whether the content to be remembered is in the past, retrospective memory, or in the future, prospective memory. Thus, retrospective memory as a category includes semantic, episodic and autobiographical memory. In contrast, prospective memory is memory for future intentions, or remembering to remember (Winograd, 1988). Prospective memory can be further broken down into event- and time-based prospective remembering. Time-based prospective memories are triggered by a time-cue, such as going to the doctor (action) at 4pm (cue). Event-based prospective memories are intentions triggered by cues, such as remembering to post a letter (action) after seeing a mailbox (cue). Cues do not need to be related to the action (as the mailbox/letter example), and lists, sticky-notes, knotted handkerchiefs, or string around the finger all exemplify cues that people use as strategies to enhance prospective memory.Hebb distinguished between short-term and long-term memory. He postulated that any memory that stayed in short-term storage for a long enough time would be consolidated into a long-term memory. Later research showed this to be false. Research has shown that direct injections of cortisol or epinephrine help the storage of recent experiences. This is also true for stimulation of the amygdala. This proves that excitement enhances memory by the stimulation of hormones that affect the amygdala. Excessive or prolonged stress (with prolonged cortisol) may hurt memory storage. Patients with amygdalar damage are no more likely to remember emotionally charged words than nonemotionally charged ones. The hippocampus is important for explicit memory. The hippocampus is also important for memory consolidation. The hippocampus receives input from different parts of the cortex and sends its output out to different parts of the brain also. The input comes from secondary and tertiary sensory areas that have processed the information a lot already. Hippocampal damage may also cause memory loss and problems with memory storage. This memory loss includes, retrograde amnesia which is the loss of memory for events that occurred shortly before the time of brain damage.One question that is crucial in cognitive neuroscience is how information and mental experiences are coded and represented in the brain. Scientists have gained much knowledge about the neuronal codes from the studies of plasticity, but most of such research has been focused on simple learning in simple neuronal circuits; it is considerably less clear about the neuronal changes involved in more complex examples of memory, particularly declarative memory that requires the storage of facts and events (Byrne 2007). Convergence-divergence zones might be the neural networks where memories are stored and retrieved.Cognitive neuroscientists consider memory as the retention, reactivation, and reconstruction of the experience-independent internal representation. The term of internal representation implies that such definition of memory contains two components: the expression of memory at the behavioral or conscious level, and the underpinning physical neural changes (Dudai 2007). The latter component is also called engram or memory traces (Semon 1904). Some neuroscientists and psychologists mistakenly equate the concept of engram and memory, broadly conceiving all persisting after-effects of experiences as memory; others argue against this notion that memory does not exist until it is revealed in behavior or thought (Moscovitch 2007).In contrast, procedural memory (or implicit memory) is not based on the conscious recall of information, but on implicit learning. It can best be summarized as remember how to do something. Procedural memory is primarily employed in learning motor skills and should be considered a subset of implicit memory. It is revealed when one does better in a given task due only to repetition - no new explicit memories have been formed, but one is unconsciously accessing aspects of those previous experiences. Procedural memory involved in motor learning depends on the cerebellum and basal ganglia.The working memory model explains many practical observations, such as why it is easier to do two different tasks (one verbal and one visual) than two similar tasks (e.g., two visual), and the aforementioned word-length effect. However, the concept of a central executive as noted here has been criticised as inadequate and vague.[citation needed] Working memory is also the premise for what allows us to do everyday activities involving thought. It is the section of memory where we carry out thought processes and use them to learn and reason about topics.One of the key concerns of older adults is the experience of memory loss, especially as it is one of the hallmark symptoms of Alzheimer\\'s disease. However, memory loss is qualitatively different in normal aging from the kind of memory loss associated with a diagnosis of Alzheimer\\'s (Budson & Price, 2005). Research has revealed that individuals’ performance on memory tasks that rely on frontal regions declines with age. Older adults tend to exhibit deficits on tasks that involve knowing the temporal order in which they learned information; source memory tasks that require them to remember the specific circumstances or context in which they learned information; and prospective memory tasks that involve remembering to perform an act at a future time. Older adults can manage their problems with prospective memory by using appointment books, for example.It should be noted that although 6-month-olds can recall information over the short-term, they have difficulty recalling the temporal order of information. It is only by 9 months of age that infants can recall the actions of a two-step sequence in the correct temporal order - that is, recalling step 1 and then step 2. In other words, when asked to imitate a two-step action sequence (such as putting a toy car in the base and pushing in the plunger to make the toy roll to the other end), 9-month-olds tend to imitate the actions of the sequence in the correct order (step 1 and then step 2). Younger infants (6-month-olds) can only recall one step of a two-step sequence. Researchers have suggested that these age differences are probably due to the fact that the dentate gyrus of the hippocampus and the frontal components of the neural network are not fully developed at the age of 6-months.Declarative memory can be further sub-divided into semantic memory, concerning principles and facts taken independent of context; and episodic memory, concerning information specific to a particular context, such as a time and place. Semantic memory allows the encoding of abstract knowledge about the world, such as \"Paris is the capital of France\". Episodic memory, on the other hand, is used for more personal memories, such as the sensations, emotions, and personal associations of a particular place or time. Episodic memories often reflect the \"firsts\" in life such as a first kiss, first day of school or first time winning a championship. These are key events in one\\'s life that can be remembered clearly. Autobiographical memory - memory for particular events within one\\'s own life - is generally viewed as either equivalent to, or a subset of, episodic memory. Visual memory is part of memory preserving some characteristics of our senses pertaining to visual experience. One is able to place in memory information that resembles objects, places, animals or people in sort of a mental image. Visual memory can result in priming and it is assumed some kind of perceptual representational system underlies this phenomenon.[citation needed]Stress has a significant effect on memory formation and learning. In response to stressful situations, the brain releases hormones and neurotransmitters (ex. glucocorticoids and catecholamines) which affect memory encoding processes in the hippocampus. Behavioural research on animals shows that chronic stress produces adrenal hormones which impact the hippocampal structure in the brains of rats. An experimental study by German cognitive psychologists L. Schwabe and O. Wolf demonstrates how learning under stress also decreases memory recall in humans. In this study, 48 healthy female and male university students participated in either a stress test or a control group. Those randomly assigned to the stress test group had a hand immersed in ice cold water (the reputable SECPT or ‘Socially Evaluated Cold Pressor Test’) for up to three minutes, while being monitored and videotaped. Both the stress and control groups were then presented with 32 words to memorize. Twenty-four hours later, both groups were tested to see how many words they could remember (free recall) as well as how many they could recognize from a larger list of words (recognition performance). The results showed a clear impairment of memory performance in the stress test group, who recalled 30% fewer words than the control group. The researchers suggest that stress experienced during learning distracts people by diverting their attention during the memory encoding process.Interference can hamper memorization and retrieval. There is retroactive interference, when learning new information makes it harder to recall old information and proactive interference, where prior learning disrupts recall of new information. Although interference can lead to forgetting, it is important to keep in mind that there are situations when old information can facilitate learning of new information. Knowing Latin, for instance, can help an individual learn a related language such as French – this phenomenon is known as positive transfer.Up until the middle of the 1980s it was assumed that infants could not encode, retain, and retrieve information. A growing body of research now indicates that infants as young as 6-months can recall information after a 24-hour delay. Furthermore, research has revealed that as infants grow older they can store information for longer periods of time; 6-month-olds can recall information after a 24-hour period, 9-month-olds after up to five weeks, and 20-month-olds after as long as twelve months. In addition, studies have shown that with age, infants can store information faster. Whereas 14-month-olds can recall a three-step sequence after being exposed to it once, 6-month-olds need approximately six exposures in order to be able to remember it.Brain areas involved in the neuroanatomy of memory such as the hippocampus, the amygdala, the striatum, or the mammillary bodies are thought to be involved in specific types of memory. For example, the hippocampus is believed to be involved in spatial learning and declarative learning, while the amygdala is thought to be involved in emotional memory. Damage to certain areas in patients and animal models and subsequent memory deficits is a primary source of information. However, rather than implicating a specific area, it could be that damage to adjacent areas, or to a pathway traveling through the area is actually responsible for the observed deficit. Further, it is not sufficient to describe memory, and its counterpart, learning, as solely dependent on specific brain regions. Learning and memory are attributed to changes in neuronal synapses, thought to be mediated by long-term potentiation and long-term depression.The more long term the exposure to stress is, the more impact it may have. However, short term exposure to stress also causes impairment in memory by interfering with the function of the hippocampus. Research shows that subjects placed in a stressful situation for a short amount of time still have blood glucocorticoid levels that have increased drastically when measured after the exposure is completed. When subjects are asked to complete a learning task after short term exposure they have often difficulties. Prenatal stress also hinders the ability to learn and memorize by disrupting the development of the hippocampus and can lead to unestablished long term potentiation in the offspring of severely stressed parents. Although the stress is applied prenatally, the offspring show increased levels of glucocorticoids when they are subjected to stress later on in life.Stressful life experiences may be a cause of memory loss as a person ages. Glucocorticoids that are released during stress damage neurons that are located in the hippocampal region of the brain. Therefore, the more stressful situations that someone encounters, the more susceptible they are to memory loss later on. The CA1 neurons found in the hippocampus are destroyed due to glucocorticoids decreasing the release of glucose and the reuptake of glutamate. This high level of extracellular glutamate allow calcium to enter NMDA receptors which in return kills neurons. Stressful life experiences can also cause repression of memories where a person moves an unbearable memory to the unconscious mind. This directly relates to traumatic events in one\\'s past such as kidnappings, being prisoners of war or sexual abuse as a child.Sleep does not affect acquisition or recall while one is awake. Therefore, sleep has the greatest effect on memory consolidation. During sleep, the neural connections in the brain are strengthened. This enhances the brain’s abilities to stabilize and retain memories. There have been several studies which show that sleep improves the retention of memory, as memories are enhanced through active consolidation. System consolidation takes place during slow-wave sleep (SWS). This process implicates that memories are reactivated during sleep, but that the process doesn’t enhance every memory. It also implicates that qualitative changes are made to the memories when they are transferred to long-term store during sleep. When you are sleeping, the hippocampus replays the events of the day for the neocortex. The neocortex then reviews and processes memories, which moves them into long-term memory. When you do not get enough sleep it makes it more difficult to learn as these neural connections are not as strong, resulting in a lower retention rate of memories. Sleep deprivation makes it harder to focus, resulting in inefficient learning. Furthermore, some studies have shown that sleep deprivation can lead to false memories as the memories are not properly transferred to long-term memory. Therefore, it is important to get the proper amount of sleep so that memory can function at the highest level. One of the primary functions of sleep is thought to be the improvement of the consolidation of information, as several studies have demonstrated that memory depends on getting sufficient sleep between training and test. Additionally, data obtained from neuroimaging studies have shown activation patterns in the sleeping brain that mirror those recorded during the learning of tasks from the previous day, suggesting that new memories may be solidified through such rehearsal.A UCLA research study published in the June 2006 issue of the American Journal of Geriatric Psychiatry found that people can improve cognitive function and brain efficiency through simple lifestyle changes such as incorporating memory exercises, healthy eating, physical fitness and stress reduction into their daily lives. This study examined 17 subjects, (average age 53) with normal memory performance. Eight subjects were asked to follow a \"brain healthy\" diet, relaxation, physical, and mental exercise (brain teasers and verbal memory training techniques). After 14 days, they showed greater word fluency (not memory) compared to their baseline performance. No long term follow up was conducted, it is therefore unclear if this intervention has lasting effects on memory.Much of the current knowledge of memory has come from studying memory disorders, particularly amnesia. Loss of memory is known as amnesia. Amnesia can result from extensive damage to: (a) the regions of the medial temporal lobe, such as the hippocampus, dentate gyrus, subiculum, amygdala, the parahippocampal, entorhinal, and perirhinal cortices or the (b) midline diencephalic region, specifically the dorsomedial nucleus of the thalamus and the mammillary bodies of the hypothalamus. There are many sorts of amnesia, and by studying their different forms, it has become possible to observe apparent defects in individual sub-systems of the brain\\'s memory systems, and thus hypothesize their function in the normally working brain. Other neurological disorders such as Alzheimer\\'s disease and Parkinson\\'s disease can also affect memory and cognition. Hyperthymesia, or hyperthymesic syndrome, is a disorder that affects an individual\\'s autobiographical memory, essentially meaning that they cannot forget small details that otherwise would not be stored. Korsakoff\\'s syndrome, also known as Korsakoff\\'s psychosis, amnesic-confabulatory syndrome, is an organic brain disease that adversely affects memory by widespread loss or shrinkage of neurons within the prefrontal cortex.Physical exercise, particularly continuous aerobic exercises such as running, cycling and swimming, has many cognitive benefits and effects on the brain. Influences on the brain include increases in neurotransmitter levels, improved oxygen and nutrient delivery, and increased neurogenesis in the hippocampus. The effects of exercise on memory have important implications for improving children\\'s academic performance, maintaining mental abilities in old age, and the prevention and potential cure of neurological diseases.However, memory performance can be enhanced when material is linked to the learning context, even when learning occurs under stress. A separate study by cognitive psychologists Schwabe and Wolf shows that when retention testing is done in a context similar to or congruent with the original learning task (i.e., in the same room), memory impairment and the detrimental effects of stress on learning can be attenuated. Seventy-two healthy female and male university students, randomly assigned to the SECPT stress test or to a control group, were asked to remember the locations of 15 pairs of picture cards – a computerized version of the card game \"Concentration\" or \"Memory\". The room in which the experiment took place was infused with the scent of vanilla, as odour is a strong cue for memory. Retention testing took place the following day, either in the same room with the vanilla scent again present, or in a different room without the fragrance. The memory performance of subjects who experienced stress during the object-location task decreased significantly when they were tested in an unfamiliar room without the vanilla scent (an incongruent context); however, the memory performance of stressed subjects showed no impairment when they were tested in the original room with the vanilla scent (a congruent context). All participants in the experiment, both stressed and unstressed, performed faster when the learning and retrieval contexts were similar.Interestingly, research has revealed that asking individuals to repeatedly imagine actions that they have never performed or events that they have never experienced could result in false memories. For instance, Goff and Roediger (1998) asked participants to imagine that they performed an act (e.g., break a toothpick) and then later asked them whether they had done such a thing. Findings revealed that those participants who repeatedly imagined performing such an act were more likely to think that they had actually performed that act during the first session of the experiment. Similarly, Garry and her colleagues (1996) asked college students to report how certain they were that they experienced a number of events as children (e.g., broke a window with their hand) and then two weeks later asked them to imagine four of those events. The researchers found that one-fourth of the students asked to imagine the four events reported that they had actually experienced such events as children. That is, when asked to imagine the events they were more confident that they experienced the events.Although people often think that memory operates like recording equipment, it is not the case. The molecular mechanisms underlying the induction and maintenance of memory are very dynamic and comprise distinct phases covering a time window from seconds to even a lifetime. In fact, research has revealed that our memories are constructed. People can construct their memories when they encode them and/or when they recall them. To illustrate, consider a classic study conducted by Elizabeth Loftus and John Palmer (1974) in which people were instructed to watch a film of a traffic accident and then asked about what they saw. The researchers found that the people who were asked, \"How fast were the cars going when they smashed into each other?\" gave higher estimates than those who were asked, \"How fast were the cars going when they hit each other?\" Furthermore, when asked a week later whether they have seen broken glass in the film, those who had been asked the question with smashed were twice more likely to report that they have seen broken glass than those who had been asked the question with hit. There was no broken glass depicted in the film. Thus, the wording of the questions distorted viewers’ memories of the event. Importantly, the wording of the question led people to construct different memories of the event – those who were asked the question with smashed recalled a more serious car accident than they had actually seen. The findings of this experiment were replicated around the world, and researchers consistently demonstrated that when people were provided with misleading information they tended to misremember, a phenomenon known as the misinformation effect.Memorization is a method of learning that allows an individual to recall information verbatim. Rote learning is the method most often used. Methods of memorizing things have been the subject of much discussion over the years with some writers, such as Cosmos Rossellius using visual alphabets. The spacing effect shows that an individual is more likely to remember a list of items when rehearsal is spaced over an extended period of time. In contrast to this is cramming: an intensive memorization in a short period of time. Also relevant is the Zeigarnik effect which states that people remember uncompleted or interrupted tasks better than completed ones. The so-called Method of loci uses spatial memory to memorize non-spatial information.',\n",
       "             9: 'In signal processing, data compression, source coding, or bit-rate reduction involves encoding information using fewer bits than the original representation. Compression can be either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression reduces bits by identifying unnecessary information and removing it. The process of reducing the size of a data file is referred to as data compression. In the context of data transmission, it is called source coding (encoding done at the source of the data before it is stored or transmitted) in opposition to channel coding.Compression is useful because it helps reduce resource usage, such as data storage space or transmission capacity. Because compressed data must be decompressed to use, this extra processing imposes computational or other costs through decompression; this situation is far from being a free lunch. Data compression is subject to a space–time complexity trade-off. For instance, a compression scheme for video may require expensive hardware for the video to be decompressed fast enough to be viewed as it is being decompressed, and the option to decompress the video in full before watching it may be inconvenient or require additional storage. The design of data compression schemes involves trade-offs among various factors, including the degree of compression, the amount of distortion introduced (when using lossy data compression), and the computational resources required to compress and decompress the data.Lossless data compression algorithms usually exploit statistical redundancy to represent data without losing any information, so that the process is reversible. Lossless compression is possible because most real-world data exhibits statistical redundancy. For example, an image may have areas of colour that do not change over several pixels; instead of coding \"red pixel, red pixel, ...\" the data may be encoded as \"279 red pixels\". This is a basic example of run-length encoding; there are many schemes to reduce file size by eliminating redundancy.The Lempel–Ziv (LZ) compression methods are among the most popular algorithms for lossless storage. DEFLATE is a variation on LZ optimized for decompression speed and compression ratio, but compression can be slow. DEFLATE is used in PKZIP, Gzip and PNG. LZW (Lempel–Ziv–Welch) is used in GIF images. Also noteworthy is the LZR (Lempel-Ziv–Renau) algorithm, which serves as the basis for the Zip method.[citation needed] LZ methods use a table-based compression model where table entries are substituted for repeated strings of data. For most LZ methods, this table is generated dynamically from earlier data in the input. The table itself is often Huffman encoded (e.g. SHRI, LZX). Current LZ-based coding schemes that perform well are Brotli and LZX. LZX is used in Microsoft\\'s CAB format.In a further refinement of the direct use of probabilistic modelling, statistical estimates can be coupled to an algorithm called arithmetic coding. Arithmetic coding is a more modern coding technique that uses the mathematical calculations of a finite-state machine to produce a string of encoded bits from a series of input data symbols. It can achieve superior compression to other techniques such as the better-known Huffman algorithm. It uses an internal memory state to avoid the need to perform a one-to-one mapping of individual input symbols to distinct representations that use an integer number of bits, and it clears out the internal memory only after encoding the entire string of data symbols. Arithmetic coding applies especially well to adaptive data compression tasks where the statistics vary and are context-dependent, as it can be easily coupled with an adaptive model of the probability distribution of the input data. An early example of the use of arithmetic coding was its use as an optional (but not widely used) feature of the JPEG image coding standard. It has since been applied in various other designs including H.264/MPEG-4 AVC and HEVC for video coding.Lossy data compression is the converse of lossless data compression. In these schemes, some loss of information is acceptable. Dropping nonessential detail from the data source can save storage space. Lossy data compression schemes are designed by research on how people perceive the data in question. For example, the human eye is more sensitive to subtle variations in luminance than it is to the variations in color. JPEG image compression works in part by rounding off nonessential bits of information. There is a corresponding trade-off between preserving information and reducing size. A number of popular compression formats exploit these perceptual differences, including those used in music files, images, and video.In lossy audio compression, methods of psychoacoustics are used to remove non-audible (or less audible) components of the audio signal. Compression of human speech is often performed with even more specialized techniques; speech coding, or voice coding, is sometimes distinguished as a separate discipline from audio compression. Different audio and speech compression standards are listed under audio coding formats. Voice compression is used in internet telephony, for example, audio compression is used for CD ripping and is decoded by the audio players.There is a close connection between machine learning and compression: a system that predicts the posterior probabilities of a sequence given its entire history can be used for optimal data compression (by using arithmetic coding on the output distribution) while an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as a justification for using data compression as a benchmark for \"general intelligence.\"Data compression can be viewed as a special case of data differencing: Data differencing consists of producing a difference given a source and a target, with patching producing a target given a source and a difference, while data compression consists of producing a compressed file given a target, and decompression consists of producing a target given only a compressed file. Thus, one can consider data compression as data differencing with empty source data, the compressed file corresponding to a \"difference from nothing.\" This is the same as considering absolute entropy (corresponding to data compression) as a special case of relative entropy (corresponding to data differencing) with no initial data.Audio data compression, not to be confused with dynamic range compression, has the potential to reduce the transmission bandwidth and storage requirements of audio data. Audio compression algorithms are implemented in software as audio codecs. Lossy audio compression algorithms provide higher compression at the cost of fidelity and are used in numerous audio applications. These algorithms almost all rely on psychoacoustics to eliminate less audible or meaningful sounds, thereby reducing the space required to store or transmit them.Lossless audio compression produces a representation of digital data that decompress to an exact digital duplicate of the original audio stream, unlike playback from lossy compression techniques such as Vorbis and MP3. Compression ratios are around 50–60% of original size, which is similar to those for generic lossless data compression. Lossless compression is unable to attain high compression ratios due to the complexity of waveforms and the rapid changes in sound forms. Codecs like FLAC, Shorten and TTA use linear prediction to estimate the spectrum of the signal. Many of these algorithms use convolution with the filter [-1 1] to slightly whiten or flatten the spectrum, thereby allowing traditional lossless compression to work more efficiently. The process is reversed upon decompression.Lossy audio compression is used in a wide range of applications. In addition to the direct applications (mp3 players or computers), digitally compressed audio streams are used in most video DVDs, digital television, streaming media on the internet, satellite and cable radio, and increasingly in terrestrial radio broadcasts. Lossy compression typically achieves far greater compression than lossless compression (data of 5 percent to 20 percent of the original stream, rather than 50 percent to 60 percent), by discarding less-critical data.To determine what information in an audio signal is perceptually irrelevant, most lossy compression algorithms use transforms such as the modified discrete cosine transform (MDCT) to convert time domain sampled waveforms into a transform domain. Once transformed, typically into the frequency domain, component frequencies can be allocated bits according to how audible they are. Audibility of spectral components calculated using the absolute threshold of hearing and the principles of simultaneous masking—the phenomenon wherein a signal is masked by another signal separated by frequency—and, in some cases, temporal masking—where a signal is masked by another signal separated by time. Equal-loudness contours may also be used to weight the perceptual importance of components. Models of the human ear-brain combination incorporating such effects are often called psychoacoustic models.Other types of lossy compressors, such as the linear predictive coding (LPC) used with speech, are source-based coders. These coders use a model of the sound\\'s generator (such as the human vocal tract with LPC) to whiten the audio signal (i.e., flatten its spectrum) before quantization. LPC may be thought of as a basic perceptual coding technique: reconstruction of an audio signal using a linear predictor shapes the coder\\'s quantization noise into the spectrum of the target signal, partially masking it.Latency results from the methods used to encode and decode the data. Some codecs will analyze a longer segment of the data to optimize efficiency, and then code it in a manner that requires a larger segment of data at one time to decode. (Often codecs create segments called a \"frame\" to create discrete data segments for encoding and decoding.) The inherent latency of the coding algorithm can be critical; for example, when there is a two-way transmission of data, such as with a telephone conversation, significant delays may seriously degrade the perceived quality.In contrast to the speed of compression, which is proportional to the number of operations required by the algorithm, here latency refers to the number of samples that must be analysed before a block of audio is processed. In the minimum case, latency is zero samples (e.g., if the coder/decoder simply reduces the number of bits used to quantize the signal). Time domain algorithms such as LPC also often have low latencies, hence their popularity in speech coding for telephony. In algorithms such as MP3, however, a large number of samples have to be analyzed to implement a psychoacoustic model in the frequency domain, and latency is on the order of 23 ms (46 ms for two-way communication)).If the data to be compressed is analog (such as a voltage that varies with time), quantization is employed to digitize it into numbers (normally integers). This is referred to as analog-to-digital (A/D) conversion. If the integers generated by quantization are 8 bits each, then the entire range of the analog signal is divided into 256 intervals and all the signal values within an interval are quantized to the same number. If 16-bit integers are generated, then the range of the analog signal is divided into 65,536 intervals.A literature compendium for a large variety of audio coding systems was published in the IEEE Journal on Selected Areas in Communications (JSAC), February 1988. While there were some papers from before that time, this collection documented an entire variety of finished, working audio coders, nearly all of them using perceptual (i.e. masking) techniques and some kind of frequency analysis and back-end noiseless coding. Several of these papers remarked on the difficulty of obtaining good, clean digital audio for research purposes. Most, if not all, of the authors in the JSAC edition were also active in the MPEG-1 Audio committee.The world\\'s first commercial broadcast automation audio compression system was developed by Oscar Bonello, an engineering professor at the University of Buenos Aires. In 1983, using the psychoacoustic principle of the masking of critical bands first published in 1967, he started developing a practical application based on the recently developed IBM PC computer, and the broadcast automation system was launched in 1987 under the name Audicom. Twenty years later, almost all the radio stations in the world were using similar technology manufactured by a number of companies.The majority of video compression algorithms use lossy compression. Uncompressed video requires a very high data rate. Although lossless video compression codecs perform at a compression factor of 5-12, a typical MPEG-4 lossy compression video has a compression factor between 20 and 200. As in all lossy compression, there is a trade-off between video quality, cost of processing the compression and decompression, and system requirements. Highly compressed video may present visible or distracting artifacts.Some video compression schemes typically operate on square-shaped groups of neighboring pixels, often called macroblocks. These pixel groups or blocks of pixels are compared from one frame to the next, and the video compression codec sends only the differences within those blocks. In areas of video with more motion, the compression must encode more data to keep up with the larger number of pixels that are changing. Commonly during explosions, flames, flocks of animals, and in some panning shots, the high-frequency detail leads to quality decreases or to increases in the variable bitrate.Video data may be represented as a series of still image frames. The sequence of frames contains spatial and temporal redundancy that video compression algorithms attempt to eliminate or code in a smaller size. Similarities can be encoded by only storing differences between frames, or by using perceptual features of human vision. For example, small differences in color are more difficult to perceive than are changes in brightness. Compression algorithms can average a color across these similar areas to reduce space, in a manner similar to those used in JPEG image compression. Some of these methods are inherently lossy while others may preserve all relevant information from the original, uncompressed video.The most powerful used method works by comparing each frame in the video with the previous one. If the frame contains areas where nothing has moved, the system simply issues a short command that copies that part of the previous frame, bit-for-bit, into the next one. If sections of the frame move in a simple manner, the compressor emits a (slightly longer) command that tells the decompressor to shift, rotate, lighten, or darken the copy. This longer command still remains much shorter than intraframe compression. Interframe compression works well for programs that will simply be played back by the viewer, but can cause problems if the video sequence needs to be edited.Because interframe compression copies data from one frame to another, if the original frame is simply cut out (or lost in transmission), the following frames cannot be reconstructed properly. Some video formats, such as DV, compress each frame independently using intraframe compression. Making \\'cuts\\' in intraframe-compressed video is almost as easy as editing uncompressed video: one finds the beginning and ending of each frame, and simply copies bit-for-bit each frame that one wants to keep, and discards the frames one doesn\\'t want. Another difference between intraframe and interframe compression is that, with intraframe systems, each frame uses a similar amount of data. In most interframe systems, certain frames (such as \"I frames\" in MPEG-2) aren\\'t allowed to copy data from other frames, so they require much more data than other frames nearby.Today, nearly all commonly used video compression methods (e.g., those in standards approved by the ITU-T or ISO) apply a discrete cosine transform (DCT) for spatial redundancy reduction. The DCT that is widely used in this regard was introduced by N. Ahmed, T. Natarajan and K. R. Rao in 1974. Other methods, such as fractal compression, matching pursuit and the use of a discrete wavelet transform (DWT) have been the subject of some research, but are typically not used in practical products (except for the use of wavelet coding as still-image coders without motion compensation). Interest in fractal compression seems to be waning, due to recent theoretical analysis showing a comparative lack of effectiveness of such methods.Genetics compression algorithms are the latest generation of lossless algorithms that compress data (typically sequences of nucleotides) using both conventional compression algorithms and genetic algorithms adapted to the specific datatype. In 2012, a team of scientists from Johns Hopkins University published a genetic compression algorithm that does not use a reference genome for compression. HAPZIPPER was tailored for HapMap data and achieves over 20-fold compression (95% reduction in file size), providing 2- to 4-fold better compression and in much faster time than the leading general-purpose compression utilities. For this, Chanda, Elhaik, and Bader introduced MAF based encoding (MAFE), which reduces the heterogeneity of the dataset by sorting SNPs by their minor allele frequency, thus homogenizing the dataset. Other algorithms in 2009 and 2013 (DNAZip and GenomeZip) have compression ratios of up to 1200-fold—allowing 6 billion basepair diploid human genomes to be stored in 2.5 megabytes (relative to a reference genome or averaged over many genomes).',\n",
       "             10: 'USB was designed to standardize the connection of computer peripherals (including keyboards, pointing devices, digital cameras, printers, portable media players, disk drives and network adapters) to personal computers, both to communicate and to supply electric power. It has become commonplace on other devices, such as smartphones, PDAs and video game consoles. USB has effectively replaced a variety of earlier interfaces, such as serial and parallel ports, as well as separate power chargers for portable devices.Unlike other data cables (e.g., Ethernet, HDMI), each end of a USB cable uses a different kind of connector; a Type-A or a Type-B. This kind of design was chosen to prevent electrical overloads and damaged equipment, as only the Type-A socket provides power. There are cables with Type-A connectors on both ends, but they should be used carefully. Therefore, in general, each of the different \"sizes\" requires four different connectors; USB cables have the Type-A and Type-B plugs, and the corresponding receptacles are on the computer or electronic device. In common practice, the Type-A connector is usually the full size, and the Type-B side can vary as needed.Counter-intuitively, the \"micro\" size is the most durable from the point of designed insertion lifetime. The standard and mini connectors were designed for less than daily connections, with a design lifetime of 1,500 insertion-removal cycles. (Improved mini-B connectors have reached 5,000-cycle lifetimes.) Micro connectors were designed with frequent charging of portable devices in mind; not only is design lifetime of the connector improved to 10,000 cycles, but it was also redesigned to place the flexible contacts, which wear out sooner, on the easily replaced cable, while the more durable rigid contacts are located in the micro-USB receptacles. Likewise, the springy part of the retention mechanism (parts that provide required gripping force) were also moved into plugs on the cable side.USB connections also come in five data transfer modes, in ascending order: Low Speed (1.0), Full Speed (1.0), High Speed (2.0), SuperSpeed (3.0), and SuperSpeed+ (3.1). High Speed is supported only by specifically designed USB 2.0 High Speed interfaces (that is, USB 2.0 controllers without the High Speed designation do not support it), as well as by USB 3.0 and newer interfaces. SuperSpeed is supported only by USB 3.0 and newer interfaces, and requires a connector and cable with extra pins and wires, usually distinguishable by the blue inserts in connectors.A group of seven companies began the development of USB in 1994: Compaq, DEC, IBM, Intel, Microsoft, NEC, and Nortel. The goal was to make it fundamentally easier to connect external devices to PCs by replacing the multitude of connectors at the back of PCs, addressing the usability issues of existing interfaces, and simplifying software configuration of all devices connected to USB, as well as permitting greater data rates for external devices. A team including Ajay Bhatt worked on the standard at Intel; the first integrated circuits supporting USB were produced by Intel in 1995.The original USB 1.0 specification, which was introduced in January 1996, defined data transfer rates of 1.5 Mbit/s \"Low Speed\" and 12 Mbit/s \"Full Speed\". Microsoft Windows 95, OSR 2.1 provided OEM support for the devices. The first widely used version of USB was 1.1, which was released in September 1998. The 12 Mbit/s data rate was intended for higher-speed devices such as disk drives, and the lower 1.5 Mbit/s rate for low data rate devices such as joysticks. Apple Inc.\\'s iMac was the first mainstream product with USB and the iMac\\'s success popularized USB itself. Following Apple\\'s design decision to remove all legacy ports from the iMac, many PC manufacturers began building legacy-free PCs, which led to the broader PC market using USB as a standard.The new SuperSpeed bus provides a fourth transfer mode with a data signaling rate of 5.0 Gbit/s, in addition to the modes supported by earlier versions. The payload throughput is 4 Gbit/s[citation needed] (due to the overhead incurred by 8b/10b encoding), and the specification considers it reasonable to achieve around 3.2 Gbit/s (0.4 GB/s or 400 MB/s), which should increase with future hardware advances. Communication is full-duplex in SuperSpeed transfer mode; in the modes supported previously, by 1.x and 2.0, communication is half-duplex, with direction controlled by the host.As with previous USB versions, USB 3.0 ports come in low-power and high-power variants, providing 150 mA and 900 mA respectively, while simultaneously transmitting data at SuperSpeed rates. Additionally, there is a Battery Charging Specification (Version 1.2 – December 2010), which increases the power handling capability to 1.5 A but does not allow concurrent data transmission. The Battery Charging Specification requires that the physical ports themselves be capable of handling 5 A of current[citation needed] but limits the maximum current drawn to 1.5 A.A January 2013 press release from the USB group revealed plans to update USB 3.0 to 10 Gbit/s. The group ended up creating a new USB version, USB 3.1, which was released on 31 July 2013, introducing a faster transfer mode called SuperSpeed USB 10 Gbit/s, putting it on par with a single first-generation Thunderbolt channel. The new mode\\'s logo features a \"Superspeed+\" caption (stylized as SUPERSPEED+). The USB 3.1 standard increases the data signaling rate to 10 Gbit/s in the USB 3.1 Gen2 mode, double that of USB 3.0 (referred to as USB 3.1 Gen1) and reduces line encoding overhead to just 3% by changing the encoding scheme to 128b/132b. The first USB 3.1 implementation demonstrated transfer speeds of 7.2 Gbit/s.Developed at roughly the same time as the USB 3.1 specification, but distinct from it, the USB Type-C Specification 1.0 was finalized in August 2014 and defines a new small reversible-plug connector for USB devices. The Type-C plug connects to both hosts and devices, replacing various Type-A and Type-B connectors and cables with a standard meant to be future-proof, similar to Apple Lightning and Thunderbolt. The 24-pin double-sided connector provides four power/ground pairs, two differential pairs for USB 2.0 data bus (though only one pair is implemented in a Type-C cable), four pairs for high-speed data bus, two \"sideband use\" pins, and two configuration pins for cable orientation detection, dedicated biphase mark code (BMC) configuration data channel, and VCONN +5 V power for active cables. Type-A and Type-B adaptors and cables are required for older devices to plug into Type-C hosts. Adapters and cables with a Type-C receptacle are not allowed.[citation needed]Full-featured USB Type-C cables are active, electronically marked cables that contain a chip with an ID function based on the configuration data channel and vendor-defined messages (VDMs) from the USB Power Delivery 2.0 specification. USB Type-C devices also support power currents of 1.5 A and 3.0 A over the 5 V power bus in addition to baseline 900 mA; devices can either negotiate increased USB current through the configuration line, or they can support the full Power Delivery specification using both BMC-coded configuration line and legacy BFSK-coded VBUS line.The design architecture of USB is asymmetrical in its topology, consisting of a host, a multitude of downstream USB ports, and multiple peripheral devices connected in a tiered-star topology. Additional USB hubs may be included in the tiers, allowing branching into a tree structure with up to five tier levels. A USB host may implement multiple host controllers and each host controller may provide one or more USB ports. Up to 127 devices, including hub devices if present, may be connected to a single host controller. USB devices are linked in series through hubs. One hub—built into the host controller—is the root hub.A physical USB device may consist of several logical sub-devices that are referred to as device functions. A single device may provide several functions, for example, a webcam (video device function) with a built-in microphone (audio device function). This kind of device is called a composite device. An alternative to this is compound device, in which the host assigns each logical device a distinctive address and all logical devices connect to a built-in hub that connects to the physical USB cable.USB device communication is based on pipes (logical channels). A pipe is a connection from the host controller to a logical entity, found on a device, and named an endpoint. Because pipes correspond 1-to-1 to endpoints, the terms are sometimes used interchangeably. A USB device could have up to 32 endpoints (16 IN, 16 OUT), though it\\'s rare to have so many. An endpoint is defined and numbered by the device during initialization (the period after physical connection called \"enumeration\") and so is relatively permanent, whereas a pipe may be opened and closed.An endpoint of a pipe is addressable with a tuple (device_address, endpoint_number) as specified in a TOKEN packet that the host sends when it wants to start a data transfer session. If the direction of the data transfer is from the host to the endpoint, an OUT packet (a specialization of a TOKEN packet) having the desired device address and endpoint number is sent by the host. If the direction of the data transfer is from the device to the host, the host sends an IN packet instead. If the destination endpoint is a uni-directional endpoint whose manufacturer\\'s designated direction does not match the TOKEN packet (e.g. the manufacturer\\'s designated direction is IN while the TOKEN packet is an OUT packet), the TOKEN packet is ignored. Otherwise, it is accepted and the data transaction can start. A bi-directional endpoint, on the other hand, accepts both IN and OUT packets.When a USB device is first connected to a USB host, the USB device enumeration process is started. The enumeration starts by sending a reset signal to the USB device. The data rate of the USB device is determined during the reset signaling. After reset, the USB device\\'s information is read by the host and the device is assigned a unique 7-bit address. If the device is supported by the host, the device drivers needed for communicating with the device are loaded and the device is set to a configured state. If the USB host is restarted, the enumeration process is repeated for all connected devices.High-speed USB 2.0 hubs contain devices called transaction translators that convert between high-speed USB 2.0 buses and full and low speed buses. When a high-speed USB 2.0 hub is plugged into a high-speed USB host or hub, it operates in high-speed mode. The USB hub then uses either one transaction translator per hub to create a full/low-speed bus routed to all full and low speed devices on the hub, or uses one transaction translator per port to create an isolated full/low-speed bus per port on the hub.USB implements connections to storage devices using a set of standards called the USB mass storage device class (MSC or UMS). This was at first intended for traditional magnetic and optical drives and has been extended to support flash drives. It has also been extended to support a wide variety of novel devices as many systems can be controlled with the familiar metaphor of file manipulation within directories. The process of making a novel device look like a familiar device is also known as extension. The ability to boot a write-locked SD card with a USB adapter is particularly advantageous for maintaining the integrity and non-corruptible, pristine state of the booting medium.Though most computers since mid-2004 can boot from USB mass storage devices, USB is not intended as a primary bus for a computer\\'s internal storage. Buses such as Parallel ATA (PATA or IDE), Serial ATA (SATA), or SCSI fulfill that role in PC class computers. However, USB has one important advantage, in that it is possible to install and remove devices without rebooting the computer (hot-swapping), making it useful for mobile peripherals, including drives of various kinds (given SATA or SCSI devices may or may not support hot-swapping).Firstly conceived and still used today for optical storage devices (CD-RW drives, DVD drives, etc.), several manufacturers offer external portable USB hard disk drives, or empty enclosures for disk drives. These offer performance comparable to internal drives, limited by the current number and types of attached USB devices, and by the upper limit of the USB interface (in practice about 30 MB/s for USB 2.0 and potentially 400 MB/s or more for USB 3.0). These external drives typically include a \"translating device\" that bridges between a drive\\'s interface to a USB interface port. Functionally, the drive appears to the user much like an internal drive. Other competing standards for external drive connectivity include eSATA, ExpressCard, FireWire (IEEE 1394), and most recently Thunderbolt.Media Transfer Protocol (MTP) was designed by Microsoft to give higher-level access to a device\\'s filesystem than USB mass storage, at the level of files rather than disk blocks. It also has optional DRM features. MTP was designed for use with portable media players, but it has since been adopted as the primary storage access protocol of the Android operating system from the version 4.1 Jelly Bean as well as Windows Phone 8 (Windows Phone 7 devices had used the Zune protocol which was an evolution of MTP). The primary reason for this is that MTP does not require exclusive access to the storage device the way UMS does, alleviating potential problems should an Android program request the storage while it is attached to a computer. The main drawback is that MTP is not as well supported outside of Windows operating systems.USB mice and keyboards can usually be used with older computers that have PS/2 connectors with the aid of a small USB-to-PS/2 adapter. For mice and keyboards with dual-protocol support, an adaptor that contains no logic circuitry may be used: the hardware in the USB keyboard or mouse is designed to detect whether it is connected to a USB or PS/2 port, and communicate using the appropriate protocol. Converters also exist that connect PS/2 keyboards and mice (usually one of each) to a USB port. These devices present two HID endpoints to the system and use a microcontroller to perform bidirectional data translation between the two standards.By design, it is difficult to insert a USB plug into its receptacle incorrectly. The USB specification states that the required USB icon must be embossed on the \"topside\" of the USB plug, which \"...provides easy user recognition and facilitates alignment during the mating process.\" The specification also shows that the \"recommended\" \"Manufacturer\\'s logo\" (\"engraved\" on the diagram but not specified in the text) is on the opposite side of the USB icon. The specification further states, \"The USB Icon is also located adjacent to each receptacle. Receptacles should be oriented to allow the icon on the plug to be visible during the mating process.\" However, the specification does not consider the height of the device compared to the eye level height of the user, so the side of the cable that is \"visible\" when mated to a computer on a desk can depend on whether the user is standing or kneeling.The standard connectors were deliberately intended to enforce the directed topology of a USB network: Type-A receptacles on host devices that supply power and Type-B receptacles on target devices that draw power. This prevents users from accidentally connecting two USB power supplies to each other, which could lead to short circuits and dangerously high currents, circuit failures, or even fire. USB does not support cyclic networks and the standard connectors from incompatible USB devices are themselves incompatible.The standard connectors were designed to be robust. Because USB is hot-pluggable, the connectors would be used more frequently, and perhaps with less care, than other connectors. Many previous connector designs were fragile, specifying embedded component pins or other delicate parts that were vulnerable to bending or breaking. The electrical contacts in a USB connector are protected by an adjacent plastic tongue, and the entire connecting assembly is usually protected by an enclosing metal sheath.The connector construction always ensures that the external sheath on the plug makes contact with its counterpart in the receptacle before any of the four connectors within make electrical contact. The external metallic sheath is typically connected to system ground, thus dissipating damaging static charges. This enclosure design also provides a degree of protection from electromagnetic interference to the USB signal while it travels through the mated connector pair (the only location when the otherwise twisted data pair travels in parallel). In addition, because of the required sizes of the power and common connections, they are made after the system ground but before the data connections. This type of staged make-break timing allows for electrically safe hot-swapping.The newer micro-USB receptacles are designed for a minimum rated lifetime of 10,000 cycles of insertion and removal between the receptacle and plug, compared to 1,500 for the standard USB and 5,000 for the mini-USB receptacle. Features intended to accomplish include, a locking device was added and the leaf-spring was moved from the jack to the plug, so that the most-stressed part is on the cable side of the connection. This change was made so that the connector on the less expensive cable would bear the most wear instead of the more expensive micro-USB device. However the idea that these changes did in fact make the connector more durable in real world use has been widely disputed, with many contending that they are in fact, much less durable.The USB standard specifies relatively loose tolerances for compliant USB connectors to minimize physical incompatibilities in connectors from different vendors. To address a weakness present in some other connector standards, the USB specification also defines limits to the size of a connecting device in the area around its plug. This was done to prevent a device from blocking adjacent ports due to the size of the cable strain relief mechanism (usually molding integral with the cable outer insulation) at the connector. Compliant devices must either fit within the size restrictions or support a compliant extension cable that does.In general, USB cables have only plugs on their ends, while hosts and devices have only receptacles. Hosts almost universally have Type-A receptacles, while devices have one or another Type-B variety. Type-A plugs mate only with Type-A receptacles, and the same applies to their Type-B counterparts; they are deliberately physically incompatible. However, an extension to the USB standard specification called USB On-The-Go (OTG) allows a single port to act as either a host or a device, which is selectable by the end of the cable that plugs into the receptacle on the OTG-enabled unit. Even after the cable is hooked up and the units are communicating, the two units may \"swap\" ends under program control. This capability is meant for units such as PDAs in which the USB link might connect to a PC\\'s host port as a device in one instance, yet connect as a host itself to a keyboard and mouse device in another instance.Various connectors have been used for smaller devices such as digital cameras, smartphones, and tablet computers. These include the now-deprecated (i.e. de-certified but standardized) mini-A and mini-AB connectors; mini-B connectors are still supported, but are not OTG-compliant (On The Go, used in mobile devices). The mini-B USB connector was standard for transferring data to and from the early smartphones and PDAs. Both mini-A and mini-B plugs are approximately 3 by 7 mm; the mini-A connector and the mini-AB receptacle connector were deprecated on 23 May 2007.The micro plug design is rated for at least 10,000 connect-disconnect cycles, which is more than the mini plug design. The micro connector is also designed to reduce the mechanical wear on the device; instead the easier-to-replace cable is designed to bear the mechanical wear of connection and disconnection. The Universal Serial Bus Micro-USB Cables and Connectors Specification details the mechanical characteristics of micro-A plugs, micro-AB receptacles (which accept both micro-A and micro-B plugs), and micro-B plugs and receptacles, along with a standard-A receptacle to micro-A plug adapter.The cellular phone carrier group Open Mobile Terminal Platform (OMTP) in 2007 endorsed micro-USB as the standard connector for data and power on mobile devices In addition, on 22 October 2009 the International Telecommunication Union (ITU) has also announced that it had embraced micro-USB as the Universal Charging Solution its \"energy-efficient one-charger-fits-all new mobile phone solution,\" and added: \"Based on the Micro-USB interface, UCS chargers also include a 4-star or higher efficiency rating—\\u200b\\u200bup to three times more energy-efficient than an unrated charger.\"The European Standardisation Bodies CEN, CENELEC and ETSI (independent of the OMTP/GSMA proposal) defined a common External Power Supply (EPS) for use with smartphones sold in the EU based on micro-USB. 14 of the world\\'s largest mobile phone manufacturers signed the EU\\'s common EPS Memorandum of Understanding (MoU). Apple, one of the original MoU signers, makes micro-USB adapters available – as permitted in the Common EPS MoU – for its iPhones equipped with Apple\\'s proprietary 30-pin dock connector or (later) Lightning connector.All current USB On-The-Go (OTG) devices are required to have one, and only one, USB connector: a micro-AB receptacle. Non-OTG compliant devices are not allowed to use the micro-AB receptacle, due to power supply shorting hazards on the VBUS line. The micro-AB receptacle is capable of accepting both micro-A and micro-B plugs, attached to any of the legal cables and adapters as defined in revision 1.01 of the micro-USB specification. Prior to the development of micro-USB, USB On-The-Go devices were required to use mini-AB receptacles to perform the equivalent job.The OTG device with the A-plug inserted is called the A-device and is responsible for powering the USB interface when required and by default assumes the role of host. The OTG device with the B-plug inserted is called the B-device and by default assumes the role of peripheral. An OTG device with no plug inserted defaults to acting as a B-device. If an application on the B-device requires the role of host, then the Host Negotiation Protocol (HNP) is used to temporarily transfer the host role to the B-device.USB is a serial bus, using four shielded wires for the USB 2.0 variant: two for power (VBUS and GND), and two for differential data signals (labelled as D+ and D− in pinouts). Non-Return-to-Zero Inverted (NRZI) encoding scheme is used for transferring data, with a sync field to synchronize the host and receiver clocks. D+ and D− signals are transmitted on a twisted pair, providing half-duplex data transfers for USB 2.0. Mini and micro connectors have their GND connections moved from pin #4 to pin #5, while their pin #4 serves as an ID pin for the On-The-Go host/client identification.USB 2.0 provides for a maximum cable length of 5 meters for devices running at Hi Speed (480 Mbit/s). The primary reason for this limit is the maximum allowed round-trip delay of about 1.5 μs. If USB host commands are unanswered by the USB device within the allowed time, the host considers the command lost. When adding USB device response time, delays from the maximum number of hubs added to the delays from connecting cables, the maximum acceptable delay per cable amounts to 26 ns. The USB 2.0 specification requires that cable delay be less than 5.2 ns per meter (192 000 km/s, which is close to the maximum achievable transmission speed for standard copper wire).A unit load is defined as 100 mA in USB 1.x and 2.0, and 150 mA in USB 3.0. A device may draw a maximum of five unit loads from a port in USB 1.x and 2.0 (500 mA), or six unit loads in USB 3.0 (900 mA). There are two types of devices: low-power and high-power. A low-power device (such as a USB HID) draws at most one-unit load, with minimum operating voltage of 4.4 V in USB 2.0, and 4 V in USB 3.0. A high-power device draws, at most, the maximum number of unit loads the standard permits. Every device functions initially as low-power (including high-power functions during their low-power enumeration phases), but may request high-power, and get it if available on the providing bus.Some devices, such as high-speed external disk drives, require more than 500 mA of current and therefore may have power issues if powered from just one USB 2.0 port: erratic function, failure to function, or overloading/damaging the port. Such devices may come with an external power source or a Y-shaped cable that has two USB connectors (one for power and data, the other for power only) to plug into a computer. With such a cable, a device can draw power from two USB ports simultaneously. However, USB compliance specification states that \"use of a \\'Y\\' cable (a cable with two A-plugs) is prohibited on any USB peripheral\", meaning that \"if a USB peripheral requires more power than allowed by the USB specification to which it is designed, then it must be self-powered.\"The USB Battery Charging Specification Revision 1.1 (released in 2007) defines a new type of USB port, called the charging port. Contrary to the standard downstream port, for which current draw by a connected portable device can exceed 100 mA only after digital negotiation with the host or hub, a charging port can supply currents between 500 mA and 1.5 A without the digital negotiation. A charging port supplies up to 500 mA at 5 V, up to the rated current at 3.6 V or more, and drops its output voltage if the portable device attempts to draw more than the rated current. The charger port may shut down if the load is too high.Two types of charging port exist: the charging downstream port (CDP), supporting data transfers as well, and the dedicated charging port (DCP), without data support. A portable device can recognize the type of USB port; on a dedicated charging port, the D+ and D− pins are shorted with a resistance not exceeding 200 ohms, while charging downstream ports provide additional detection logic so their presence can be determined by attached devices. (see ref pg. 2, Section 1.4.5, & Table 5-3 \"Resistances\"—pg. 29).The USB Battery Charging Specification Revision 1.2 (released in 2010) makes clear that there are safety limits to the rated current at 5 A coming from USB 2.0. On the other hand, several changes are made and limits are increasing including allowing 1.5 A on charging downstream ports for unconfigured devices, allowing high speed communication while having a current up to 1.5 A, and allowing a maximum current of 5 A. Also, revision 1.2 removes support for USB ports type detection via resistive detection mechanisms.In July 2012, the USB Promoters Group announced the finalization of the USB Power Delivery (\"PD\") specification, an extension that specifies using certified \"PD aware\" USB cables with standard USB Type-A and Type-B connectors to deliver increased power (more than 7.5 W) to devices with larger power demand. Devices can request higher currents and supply voltages from compliant hosts –  up to 2 A at 5 V (for a power consumption of up to 10 W), and optionally up to 3 A or 5 A at either 12 V (36 W or 60 W) or 20 V (60 W or 100 W). In all cases, both host-to-device and device-to-host configurations are supported.The USB Power Delivery revision 2.0 specification has been released as part of the USB 3.1 suite. It covers the Type-C cable and connector with four power/ground pairs and a separate configuration channel, which now hosts a DC coupled low-frequency BMC-coded data channel that reduces the possibilities for RF interference. Power Delivery protocols have been updated to facilitate Type-C features such as cable ID function, Alternate Mode negotiation, increased VBUS currents, and VCONN-powered accessories.Sleep-and-charge USB ports can be used to charge electronic devices even when the computer is switched off. Normally, when a computer is powered off the USB ports are powered down, preventing phones and other devices from charging. Sleep-and-charge USB ports remain powered even when the computer is off. On laptops, charging devices from the USB port when it is not being powered from AC drains the laptop battery faster; most laptops have a facility to stop charging if their own battery charge level gets too low.On Dell and Toshiba laptops, the port is marked with the standard USB symbol with an added lightning bolt icon on the right side. Dell calls this feature PowerShare, while Toshiba calls it USB Sleep-and-Charge. On Acer Inc. and Packard Bell laptops, sleep-and-charge USB ports are marked with a non-standard symbol (the letters USB over a drawing of a battery); the feature is simply called Power-off USB. On some laptops such as Dell and Apple MacBook models, it is possible to plug a device in, close the laptop (putting it into sleep mode) and have the device continue to charge.[citation needed]The GSM Association (GSMA) followed suit on 17 February 2009, and on 22 April 2009, this was further endorsed by the CTIA – The Wireless Association, with the International Telecommunication Union (ITU) announcing on 22 October 2009 that it had also embraced the Universal Charging Solution as its \"energy-efficient one-charger-fits-all new mobile phone solution,\" and added: \"Based on the Micro-USB interface, UCS chargers will also include a 4-star or higher efficiency rating—up to three times more energy-efficient than an unrated charger.\"In June 2009, many of the world\\'s largest mobile phone manufacturers signed an EC-sponsored Memorandum of Understanding (MoU), agreeing to make most data-enabled mobile phones marketed in the European Union compatible with a common External Power Supply (EPS). The EU\\'s common EPS specification (EN 62684:2010) references the USB Battery Charging standard and is similar to the GSMA/OMTP and Chinese charging solutions. In January 2011, the International Electrotechnical Commission (IEC) released its version of the (EU\\'s) common EPS standard as IEC 62684:2011.Some USB devices require more power than is permitted by the specifications for a single port. This is common for external hard and optical disc drives, and generally for devices with motors or lamps. Such devices can use an external power supply, which is allowed by the standard, or use a dual-input USB cable, one input of which is used for power and data transfer, the other solely for power, which makes the device a non-standard USB device. Some USB ports and external hubs can, in practice, supply more power to USB devices than required by the specification but a standard-compliant device may not depend on this.In addition to limiting the total average power used by the device, the USB specification limits the inrush current (i.e., that used to charge decoupling and filter capacitors) when the device is first connected. Otherwise, connecting a device could cause problems with the host\\'s internal power. USB devices are also required to automatically enter ultra low-power suspend mode when the USB host is suspended. Nevertheless, many USB host interfaces do not cut off the power supply to USB devices when they are suspended.Some non-standard USB devices use the 5 V power supply without participating in a proper USB network, which negotiates power draw with the host interface. These are usually called USB decorations.[citation needed] Examples include USB-powered keyboard lights, fans, mug coolers and heaters, battery chargers, miniature vacuum cleaners, and even miniature lava lamps. In most cases, these items contain no digital circuitry, and thus are not standard compliant USB devices. This may cause problems with some computers, such as drawing too much current and damaging circuitry. Prior to the Battery Charging Specification, the USB specification required that devices connect in a low-power mode (100 mA maximum) and communicate their current requirements to the host, which then permits the device to switch into high-power mode.USB data is transmitted by toggling the data lines between the J state and the opposite K state. USB encodes data using the NRZI line coding; a 0 bit is transmitted by toggling the data lines from J to K or vice versa, while a 1 bit is transmitted by leaving the data lines as-is. To ensure a minimum density of signal transitions remains in the bitstream, USB uses bit stuffing; an extra 0 bit is inserted into the data stream after any appearance of six consecutive 1 bits. Seven consecutive received 1 bits is always an error. USB 3.0 has introduced additional data transmission encodings.A USB packet\\'s end, called EOP (end-of-packet), is indicated by the transmitter driving 2 bit times of SE0 (D+ and D− both below max.) and 1 bit time of J state. After this, the transmitter ceases to drive the D+/D− lines and the aforementioned pull up resistors hold it in the J (idle) state. Sometimes skew due to hubs can add as much as one bit time before the SE0 of the end of packet. This extra bit can also result in a \"bit stuff violation\" if the six bits before it in the CRC are 1s. This bit should be ignored by receiver.USB 2.0 devices use a special protocol during reset, called chirping, to negotiate the high bandwidth mode with the host/hub. A device that is HS capable first connects as an FS device (D+ pulled high), but upon receiving a USB RESET (both D+ and D− driven LOW by host for 10 to 20 ms) it pulls the D− line high, known as chirp K. This indicates to the host that the device is high bandwidth. If the host/hub is also HS capable, it chirps (returns alternating J and K states on D− and D+ lines) letting the device know that the hub operates at high bandwidth. The device has to receive at least three sets of KJ chirps before it changes to high bandwidth terminations and begins high bandwidth signaling. Because USB 3.0 uses wiring separate and additional to that used by USB 2.0 and USB 1.x, such bandwidth negotiation is not required.According to routine testing performed by CNet, write operations to typical Hi-Speed (USB 2.0) hard drives can sustain rates of 25–30 MB/s, while read operations are at 30–42 MB/s; this is 70% of the total available bus bandwidth. For USB 3.0, typical write speed is 70–90 MB/s, while read speed is 90–110 MB/s. Mask Tests, also known as Eye Diagram Tests, are used to determine the quality of a signal in the time domain. They are defined in the referenced document as part of the electrical test description for the high-speed (HS) mode at 480 Mbit/s.After the sync field, all packets are made of 8-bit bytes, transmitted least-significant bit first. The first byte is a packet identifier (PID) byte. The PID is actually 4 bits; the byte consists of the 4-bit PID followed by its bitwise complement. This redundancy helps detect errors. (Note also that a PID byte contains at most four consecutive 1 bits, and thus never needs bit-stuffing, even when combined with the final 1 bit in the sync byte. However, trailing 1 bits in the PID may require bit-stuffing within the first few bits of the payload.)Handshake packets consist of only a single PID byte, and are generally sent in response to data packets. Error detection is provided by transmitting four bits that represent the packet type twice, in a single PID byte using complemented form. Three basic types are ACK, indicating that data was successfully received, NAK, indicating that the data cannot be received and should be retried, and STALL, indicating that the device has an error condition and cannot transfer data until some corrective action (such as device initialization) occurs.IN and OUT tokens contain a seven-bit device number and four-bit function number (for multifunction devices) and command the device to transmit DATAx packets, or receive the following DATAx packets, respectively. An IN token expects a response from a device. The response may be a NAK or STALL response, or a DATAx frame. In the latter case, the host issues an ACK handshake if appropriate. An OUT token is followed immediately by a DATAx frame. The device responds with ACK, NAK, NYET, or STALL, as appropriate.USB 2.0 also added a larger three-byte SPLIT token with a seven-bit hub number, 12 bits of control flags, and a five-bit CRC. This is used to perform split transactions. Rather than tie up the high-bandwidth USB bus sending data to a slower USB device, the nearest high-bandwidth capable hub receives a SPLIT token followed by one or two USB packets at high bandwidth, performs the data transfer at full or low bandwidth, and provides the response at high bandwidth when prompted by a second SPLIT token.There are two basic forms of data packet, DATA0 and DATA1. A data packet must always be preceded by an address token, and is usually followed by a handshake token from the receiver back to the transmitter. The two packet types provide the 1-bit sequence number required by Stop-and-wait ARQ. If a USB host does not receive a response (such as an ACK) for data it has transmitted, it does not know if the data was received or not; the data might have been lost in transit, or it might have been received but the handshake response was lost.Low-bandwidth devices are supported with a special PID value, PRE. This marks the beginning of a low-bandwidth packet, and is used by hubs that normally do not send full-bandwidth packets to low-bandwidth devices. Since all PID bytes include four 0 bits, they leave the bus in the full-bandwidth K state, which is the same as the low-bandwidth J state. It is followed by a brief pause, during which hubs enable their low-bandwidth outputs, already idling in the J state. Then a low-bandwidth packet follows, beginning with a sync sequence and PID byte, and ending with a brief period of SE0. Full-bandwidth devices other than hubs can simply ignore the PRE packet and its low-bandwidth contents, until the final SE0 indicates that a new packet follows.These and other differences reflect the differing design goals of the two buses: USB was designed for simplicity and low cost, while FireWire was designed for high performance, particularly in time-sensitive applications such as audio and video. Although similar in theoretical maximum transfer rate, FireWire 400 is faster than USB 2.0 Hi-Bandwidth in real-use, especially in high-bandwidth use such as external hard-drives. The newer FireWire 800 standard is twice as fast as FireWire 400 and faster than USB 2.0 Hi-Bandwidth both theoretically and practically. However, Firewire\\'s speed advantages rely on low-level techniques such as direct memory access (DMA), which in turn have created opportunities for security exploits such as the DMA attack.The IEEE 802.3af Power over Ethernet (PoE) standard specifies a more elaborate power negotiation scheme than powered USB. It operates at 48 V DC and can supply more power (up to 12.95 W, PoE+ 25.5 W) over a cable up to 100 meters compared to USB 2.0, which provides 2.5 W with a maximum cable length of 5 meters. This has made PoE popular for VoIP telephones, security cameras, wireless access points and other networked devices within buildings. However, USB is cheaper than PoE provided that the distance is short, and power demand is low.Ethernet standards require electrical isolation between the networked device (computer, phone, etc.) and the network cable up to 1500 V AC or 2250 V DC for 60 seconds. USB has no such requirement as it was designed for peripherals closely associated with a host computer, and in fact it connects the peripheral and host grounds. This gives Ethernet a significant safety advantage over USB with peripherals such as cable and DSL modems connected to external wiring that can assume hazardous voltages under certain fault conditions.eSATA does not supply power to external devices. This is an increasing disadvantage compared to USB. Even though USB 3.0\\'s 4.5 W is sometimes insufficient to power external hard drives, technology is advancing and external drives gradually need less power, diminishing the eSATA advantage. eSATAp (power over eSATA; aka ESATA/USB) is a connector introduced in 2009 that supplies power to attached devices using a new, backward compatible, connector. On a notebook eSATAp usually supplies only 5 V to power a 2.5-inch HDD/SSD; on a desktop workstation it can additionally supply 12 V to power larger devices including 3.5-inch HDD/SSD and 5.25-inch optical drives.USB 2.0 High-Speed Inter-Chip (HSIC) is a chip-to-chip variant of USB 2.0 that eliminates the conventional analog transceivers found in normal USB. It was adopted as a standard by the USB Implementers Forum in 2007. The HSIC physical layer uses about 50% less power and 75% less board area compared to traditional USB 2.0. HSIC uses two signals at 1.2 V and has a throughput of 480 Mbit/s. Maximum PCB trace length for HSIC is 10 cm. It does not have low enough latency to support RAM memory sharing between two chips.',\n",
       "             11: 'Unicode is a computing industry standard for the consistent encoding, representation, and handling of text expressed in most of the world\\'s writing systems. Developed in conjunction with the Universal Coded Character Set (UCS) standard and published as The Unicode Standard, the latest version of Unicode contains a repertoire of more than 120,000 characters covering 129 modern and historic scripts, as well as multiple symbol sets. The standard consists of a set of code charts for visual reference, an encoding method and set of standard character encodings, a set of reference data files, and a number of related items, such as character properties, rules for normalization, decomposition, collation, rendering, and bidirectional display order (for the correct display of text containing both right-to-left scripts, such as Arabic and Hebrew, and left-to-right scripts). As of June 2015[update], the most recent version is Unicode 8.0. The standard is maintained by the Unicode Consortium.Unicode can be implemented by different character encodings. The most commonly used encodings are UTF-8, UTF-16 and the now-obsolete UCS-2. UTF-8 uses one byte for any ASCII character, all of which have the same code values in both UTF-8 and ASCII encoding, and up to four bytes for other characters. UCS-2 uses a 16-bit code unit (two 8-bit bytes) for each character but cannot encode every character in the current Unicode standard. UTF-16 extends UCS-2, using one 16-bit unit for the characters that were representable in UCS-2 and two 16-bit units (4 × 8 bits) to handle each of the additional characters.Unicode has the explicit aim of transcending the limitations of traditional character encodings, such as those defined by the ISO 8859 standard, which find wide usage in various countries of the world but remain largely incompatible with each other. Many traditional character encodings share a common problem in that they allow bilingual computer processing (usually using Latin characters and the local script), but not multilingual computer processing (computer processing of arbitrary scripts mixed with each other).The first 256 code points were made identical to the content of ISO-8859-1 so as to make it trivial to convert existing western text. Many essentially identical characters were encoded multiple times at different code points to preserve distinctions used by legacy encodings and therefore, allow conversion from those encodings to Unicode (and back) without losing any information. For example, the \"fullwidth forms\" section of code points encompasses a full Latin alphabet that is separate from the main Latin alphabet section because in Chinese, Japanese, and Korean (CJK) fonts, these Latin characters are rendered at the same width as CJK ideographs, rather than at half the width. For other examples, see Duplicate characters in Unicode.In 1996, a surrogate character mechanism was implemented in Unicode 2.0, so that Unicode was no longer restricted to 16 bits. This increased the Unicode codespace to over a million code points, which allowed for the encoding of many historic scripts (e.g., Egyptian Hieroglyphs) and thousands of rarely used or obsolete characters that had not been anticipated as needing encoding. Among the characters not originally intended for Unicode are rarely used Kanji or Chinese characters, many of which are part of personal and place names, making them rarely used, but much more essential than envisioned in the original architecture of Unicode.Each code point has a single General Category property. The major categories are: Letter, Mark, Number, Punctuation, Symbol, Separator and Other. Within these categories, there are subdivisions. The General Category is not useful for every use, since legacy encodings have used multiple characteristics per single code point. E.g., U+000A <control-000A> Line feed (LF) in ASCII is both a control and a formatting separator; in Unicode the General Category is \"Other, Control\". Often, other properties must be used to specify the characteristics and behaviour of a code point. The possible General Categories are:Code points in the range U+D800–U+DBFF (1,024 code points) are known as high-surrogate code points, and code points in the range U+DC00–U+DFFF (1,024 code points) are known as low-surrogate code points. A high-surrogate code point (also known as a leading surrogate) followed by a low-surrogate code point (also known as a trailing surrogate) together form a surrogate pair used in UTF-16 to represent 1,048,576 code points outside BMP. High and low surrogate code points are not valid by themselves. Thus the range of code points that are available for use as characters is U+0000–U+D7FF and U+E000–U+10FFFF (1,112,064 code points). The value of these code points (i.e., excluding surrogates) is sometimes referred to as the character\\'s scalar value.The set of graphic and format characters defined by Unicode does not correspond directly to the repertoire of abstract characters that is representable under Unicode. Unicode encodes characters by associating an abstract character with a particular code point. However, not all abstract characters are encoded as a single Unicode character, and some abstract characters may be represented in Unicode by a sequence of two or more characters. For example, a Latin small letter \"i\" with an ogonek, a dot above, and an acute accent, which is required in Lithuanian, is represented by the character sequence U+012F, U+0307, U+0301. Unicode maintains a list of uniquely named character sequences for abstract characters that are not directly encoded in Unicode.All graphic, format, and private use characters have a unique and immutable name by which they may be identified. This immutability has been guaranteed since Unicode version 2.0 by the Name Stability policy. In cases where the name is seriously defective and misleading, or has a serious typographical error, a formal alias may be defined, and applications are encouraged to use the formal alias in place of the official character name. For example, U+A015 ꀕ YI SYLLABLE WU has the formal alias yi syllable iteration mark, and U+FE18 ︘ PRESENTATION FORM FOR VERTICAL RIGHT WHITE LENTICULAR BRAKCET (sic) has the formal alias presentation form for vertical right white lenticular bracket.Unicode is developed in conjunction with the International Organization for Standardization and shares the character repertoire with ISO/IEC 10646: the Universal Character Set. Unicode and ISO/IEC 10646 function equivalently as character encodings, but The Unicode Standard contains much more information for implementers, covering—in depth—topics such as bitwise encoding, collation and rendering. The Unicode Standard enumerates a multitude of character properties, including those needed for supporting bidirectional text. The two standards do use slightly different terminology.The Consortium first published The Unicode Standard (ISBN 0-321-18578-1) in 1991 and continues to develop standards based on that original work. The latest version of the standard, Unicode 8.0, was released in June 2015 and is available from the consortium\\'s website. The last of the major versions (versions x.0) to be published in book form was Unicode 5.0 (ISBN 0-321-48091-0), but since Unicode 6.0 the full text of the standard is no longer being published in book form. In 2012, however, it was announced that only the core specification for Unicode version 6.1 would be made available as a 692-page print-on-demand paperback. Unlike the previous major version printings of the Standard, the print-on-demand core specification does not include any code charts or standard annexes, but the entire standard, including the core specification, will still remain freely available on the Unicode website.The Unicode Roadmap Committee (Michael Everson, Rick McGowan, and Ken Whistler) maintain the list of scripts that are candidates or potential candidates for encoding and their tentative code block assignments on the Unicode Roadmap page of the Unicode Consortium Web site. For some scripts on the Roadmap, such as Jurchen, Nü Shu, and Tangut, encoding proposals have been made and they are working their way through the approval process. For others scripts, such as Mayan and Rongorongo, no proposal has yet been made, and they await agreement on character repertoire and other details from the user communities involved.Unicode defines two mapping methods: the Unicode Transformation Format (UTF) encodings, and the Universal Coded Character Set (UCS) encodings. An encoding maps (possibly a subset of) the range of Unicode code points to sequences of values in some fixed-size range, termed code values. The numbers in the names of the encodings indicate the number of bits per code value (for UTF encodings) or the number of bytes per code value (for UCS encodings). UTF-8 and UTF-16 are probably the most commonly used encodings. UCS-2 is an obsolete subset of UTF-16; UCS-4 and UTF-32 are functionally equivalent.The UCS-2 and UTF-16 encodings specify the Unicode Byte Order Mark (BOM) for use at the beginnings of text files, which may be used for byte ordering detection (or byte endianness detection). The BOM, code point U+FEFF has the important property of unambiguity on byte reorder, regardless of the Unicode encoding used; U+FFFE (the result of byte-swapping U+FEFF) does not equate to a legal character, and U+FEFF in other places, other than the beginning of text, conveys the zero-width non-break space (a character with no appearance and no effect other than preventing the formation of ligatures).The same character converted to UTF-8 becomes the byte sequence EF BB BF. The Unicode Standard allows that the BOM \"can serve as signature for UTF-8 encoded text where the character set is unmarked\". Some software developers have adopted it for other encodings, including UTF-8, in an attempt to distinguish UTF-8 from local 8-bit code pages. However RFC 3629, the UTF-8 standard, recommends that byte order marks be forbidden in protocols using UTF-8, but discusses the cases where this may not be possible. In addition, the large restriction on possible patterns in UTF-8 (for instance there cannot be any lone bytes with the high bit set) means that it should be possible to distinguish UTF-8 from other character encodings without relying on the BOM.In UTF-32 and UCS-4, one 32-bit code value serves as a fairly direct representation of any character\\'s code point (although the endianness, which varies across different platforms, affects how the code value manifests as an octet sequence). In the other encodings, each code point may be represented by a variable number of code values. UTF-32 is widely used as an internal representation of text in programs (as opposed to stored or transmitted text), since every Unix operating system that uses the gcc compilers to generate software uses it as the standard \"wide character\" encoding. Some programming languages, such as Seed7, use UTF-32 as internal representation for strings and characters. Recent versions of the Python programming language (beginning with 2.2) may also be configured to use UTF-32 as the representation for Unicode strings, effectively disseminating such encoding in high-level coded software.Unicode includes a mechanism for modifying character shape that greatly extends the supported glyph repertoire. This covers the use of combining diacritical marks. They are inserted after the main character. Multiple combining diacritics may be stacked over the same character. Unicode also contains precomposed versions of most letter/diacritic combinations in normal use. These make conversion to and from legacy encodings simpler, and allow applications to use Unicode as an internal text format without having to implement combining characters. For example, é can be represented in Unicode as U+0065 (LATIN SMALL LETTER E) followed by U+0301 (COMBINING ACUTE ACCENT), but it can also be represented as the precomposed character U+00E9 (LATIN SMALL LETTER E WITH ACUTE). Thus, in many cases, users have multiple ways of encoding the same character. To deal with this, Unicode provides the mechanism of canonical equivalence.The CJK ideographs currently have codes only for their precomposed form. Still, most of those ideographs comprise simpler elements (often called radicals in English), so in principle, Unicode could have decomposed them, as it did with Hangul. This would have greatly reduced the number of required code points, while allowing the display of virtually every conceivable ideograph (which might do away with some of the problems caused by Han unification). A similar idea is used by some input methods, such as Cangjie and Wubi. However, attempts to do this for character encoding have stumbled over the fact that ideographs do not decompose as simply or as regularly as Hangul does.Many scripts, including Arabic and Devanagari, have special orthographic rules that require certain combinations of letterforms to be combined into special ligature forms. The rules governing ligature formation can be quite complex, requiring special script-shaping technologies such as ACE (Arabic Calligraphic Engine by DecoType in the 1980s and used to generate all the Arabic examples in the printed editions of the Unicode Standard), which became the proof of concept for OpenType (by Adobe and Microsoft), Graphite (by SIL International), or AAT (by Apple).Instructions are also embedded in fonts to tell the operating system how to properly output different character sequences. A simple solution to the placement of combining marks or diacritics is assigning the marks a width of zero and placing the glyph itself to the left or right of the left sidebearing (depending on the direction of the script they are intended to be used with). A mark handled this way will appear over whatever character precedes it, but will not adjust its position relative to the width or height of the base glyph; it may be visually awkward and it may overlap some glyphs. Real stacking is impossible, but can be approximated in limited cases (for example, Thai top-combining vowels and tone marks can just be at different heights to start with). Generally this approach is only effective in monospaced fonts, but may be used as a fallback rendering method when more complex methods fail.Several subsets of Unicode are standardized: Microsoft Windows since Windows NT 4.0 supports WGL-4 with 652 characters, which is considered to support all contemporary European languages using the Latin, Greek, or Cyrillic script. Other standardized subsets of Unicode include the Multilingual European Subsets: MES-1 (Latin scripts only, 335 characters), MES-2 (Latin, Greek and Cyrillic 1062 characters) and MES-3A & MES-3B (two larger subsets, not shown here). Note that MES-2 includes every character in MES-1 and WGL-4.Rendering software which cannot process a Unicode character appropriately often displays it as an open rectangle, or the Unicode \"replacement character\" (U+FFFD, �), to indicate the position of the unrecognized character. Some systems have made attempts to provide more information about such characters. The Apple\\'s Last Resort font will display a substitute glyph indicating the Unicode range of the character, and the SIL International\\'s Unicode Fallback font will display a box showing the hexadecimal scalar value of the character.Unicode has become the dominant scheme for internal processing and storage of text. Although a great deal of text is still stored in legacy encodings, Unicode is used almost exclusively for building new information processing systems. Early adopters tended to use UCS-2 (the fixed-width two-byte precursor to UTF-16) and later moved to UTF-16 (the variable-width current standard), as this was the least disruptive way to add support for non-BMP characters. The best known such system is Windows NT (and its descendants, Windows 2000, Windows XP, Windows Vista and Windows 7), which uses UTF-16 as the sole internal character encoding. The Java and .NET bytecode environments, Mac OS X, and KDE also use it for internal representation. Unicode is available on Windows 95 through Microsoft Layer for Unicode, as well as on its descendants, Windows 98 and Windows ME.MIME defines two different mechanisms for encoding non-ASCII characters in email, depending on whether the characters are in email headers (such as the \"Subject:\"), or in the text body of the message; in both cases, the original character set is identified as well as a transfer encoding. For email transmission of Unicode the UTF-8 character set and the Base64 or the Quoted-printable transfer encoding are recommended, depending on whether much of the message consists of ASCII-characters. The details of the two different mechanisms are specified in the MIME standards and generally are hidden from users of email software.Thousands of fonts exist on the market, but fewer than a dozen fonts—sometimes described as \"pan-Unicode\" fonts—attempt to support the majority of Unicode\\'s character repertoire. Instead, Unicode-based fonts typically focus on supporting only basic ASCII and particular scripts or sets of characters or symbols. Several reasons justify this approach: applications and documents rarely need to render characters from more than one or two writing systems; fonts tend to demand resources in computing environments; and operating systems and applications show increasing intelligence in regard to obtaining glyph information from separate font files as needed, i.e., font substitution. Furthermore, designing a consistent set of rendering instructions for tens of thousands of glyphs constitutes a monumental task; such a venture passes the point of diminishing returns for most typefaces.In terms of the newline, Unicode introduced U+2028 LINE SEPARATOR and U+2029 PARAGRAPH SEPARATOR. This was an attempt to provide a Unicode solution to encoding paragraphs and lines semantically, potentially replacing all of the various platform solutions. In doing so, Unicode does provide a way around the historical platform dependent solutions. Nonetheless, few if any Unicode solutions have adopted these Unicode line and paragraph separators as the sole canonical line ending characters. However, a common approach to solving this issue is through newline normalization. This is achieved with the Cocoa text system in Mac OS X and also with W3C XML and HTML recommendations. In this approach every possible newline character is converted internally to a common newline (which one does not really matter since it is an internal operation just for rendering). In other words, the text system can correctly treat the character as a newline, regardless of the input\\'s actual encoding.Unicode has been criticized for failing to separately encode older and alternative forms of kanji which, critics argue, complicates the processing of ancient Japanese and uncommon Japanese names. This is often due to the fact that Unicode encodes characters rather than glyphs (the visual representations of the basic character that often vary from one language to another). Unification of glyphs leads to the perception that the languages themselves, not just the basic character representation, are being merged.[clarification needed] There have been several attempts to create alternative encodings that preserve the stylistic differences between Chinese, Japanese, and Korean characters in opposition to Unicode\\'s policy of Han unification. An example of one is TRON (although it is not widely adopted in Japan, there are some users who need to handle historical Japanese text and favor it).Modern font technology provides a means to address the practical issue of needing to depict a unified Han character in terms of a collection of alternative glyph representations, in the form of Unicode variation sequences. For example, the Advanced Typographic tables of OpenType permit one of a number of alternative glyph representations to be selected when performing the character to glyph mapping process. In this case, information can be provided within plain text to designate which alternate character form to select.Unicode was designed to provide code-point-by-code-point round-trip format conversion to and from any preexisting character encodings, so that text files in older character sets can be naïvely converted to Unicode, and then back and get back the same file. That has meant that inconsistent legacy architectures, such as combining diacritics and precomposed characters, both exist in Unicode, giving more than one method of representing some text. This is most pronounced in the three different encoding forms for Korean Hangul. Since version 3.0, any precomposed characters that can be represented by a combining sequence of already existing characters can no longer be added to the standard in order to preserve interoperability between software using different versions of Unicode.Injective mappings must be provided between characters in existing legacy character sets and characters in Unicode to facilitate conversion to Unicode and allow interoperability with legacy software. Lack of consistency in various mappings between earlier Japanese encodings such as Shift-JIS or EUC-JP and Unicode led to round-trip format conversion mismatches, particularly the mapping of the character JIS X 0208 \\'～\\' (1-33, WAVE DASH), heavily used in legacy database data, to either U+FF5E ～ FULLWIDTH TILDE (in Microsoft Windows) or U+301C 〜 WAVE DASH (other vendors).Indic scripts such as Tamil and Devanagari are each allocated only 128 code points, matching the ISCII standard. The correct rendering of Unicode Indic text requires transforming the stored logical order characters into visual order and the forming of ligatures (aka conjuncts) out of components. Some local scholars argued in favor of assignments of Unicode code points to these ligatures, going against the practice for other writing systems, though Unicode contains some Arabic and other ligatures for backward compatibility purposes only. Encoding of any new ligatures in Unicode will not happen, in part because the set of ligatures is font-dependent, and Unicode is an encoding independent of font variations. The same kind of issue arose for Tibetan script[citation needed] (the Chinese National Standard organization failed to achieve a similar change).Thai alphabet support has been criticized for its ordering of Thai characters. The vowels เ, แ, โ, ใ, ไ that are written to the left of the preceding consonant are in visual order instead of phonetic order, unlike the Unicode representations of other Indic scripts. This complication is due to Unicode inheriting the Thai Industrial Standard 620, which worked in the same way, and was the way in which Thai had always been written on keyboards. This ordering problem complicates the Unicode collation process slightly, requiring table lookups to reorder Thai characters for collation. Even if Unicode had adopted encoding according to spoken order, it would still be problematic to collate words in dictionary order. E.g., the word แสดง  [sa dɛːŋ] \"perform\" starts with a consonant cluster \"สด\" (with an inherent vowel for the consonant \"ส\"), the vowel แ-, in spoken order would come after the ด, but in a dictionary, the word is collated as it is written, with the vowel following the ส.Characters with diacritical marks can generally be represented either as a single precomposed character or as a decomposed sequence of a base letter plus one or more non-spacing marks. For example, ḗ (precomposed e with macron and acute above) and ḗ (e followed by the combining macron above and combining acute above) should be rendered identically, both appearing as an e with a macron and acute accent, but in practice, their appearance may vary depending upon what rendering engine and fonts are being used to display the characters. Similarly, underdots, as needed in the romanization of Indic, will often be placed incorrectly[citation needed]. Unicode characters that map to precomposed glyphs can be used in many cases, thus avoiding the problem, but where no precomposed character has been encoded the problem can often be solved by using a specialist Unicode font such as Charis SIL that uses Graphite, OpenType, or AAT technologies for advanced rendering features.',\n",
       "             12: 'The company originated in 1911 as the Computing-Tabulating-Recording Company (CTR) through the consolidation of The Tabulating Machine Company, the International Time Recording Company, the Computing Scale Company and the Bundy Manufacturing Company. CTR was renamed \"International Business Machines\" in 1924, a name which Thomas J. Watson first used for a CTR Canadian subsidiary. The initialism IBM followed. Securities analysts nicknamed the company Big Blue for its size and common use of the color in products, packaging and its logo.In 2012, Fortune ranked IBM the second largest U.S. firm in terms of number of employees (435,000 worldwide), the fourth largest in terms of market capitalization, the ninth most profitable, and the nineteenth largest firm in terms of revenue. Globally, the company was ranked the 31st largest in terms of revenue by Forbes for 2011. Other rankings for 2011/2012 include №1 company for leaders (Fortune), №1 green company in the United States (Newsweek), №2 best global brand (Interbrand), №2 most respected company (Barron\\'s), №5 most admired company (Fortune), and №18 most innovative company (Fast Company).IBM has 12 research laboratories worldwide, bundled into IBM Research. As of 2013[update] the company held the record for most patents generated by a business for 22 consecutive years. Its employees have garnered five Nobel Prizes, six Turing Awards, ten National Medals of Technology and five National Medals of Science. Notable company inventions or developments include the automated teller machine (ATM), the floppy disk, the hard disk drive, the magnetic stripe card, the relational database, the Universal Product Code (UPC), the financial swap, the Fortran programming language, SABRE airline reservation system, dynamic random-access memory (DRAM), copper wiring in semiconductors, the silicon-on-insulator (SOI) semiconductor manufacturing process, and Watson artificial intelligence.IBM has constantly evolved since its inception. Over the past decade, it has steadily shifted its business mix by exiting commoditizing markets such as PCs, hard disk drives and DRAMs and focusing on higher-value, more profitable markets such as business intelligence, data analytics, business continuity, security, cloud computing, virtualization and green solutions, resulting in a higher quality revenue stream and higher profit margins. IBM\\'s operating margin expanded from 16.8% in 2004 to 24.3% in 2013, and net profit margins expanded from 9.0% in 2004 to 16.5% in 2013.IBM acquired Kenexa (2012) and SPSS (2009) and PwC\\'s consulting business (2002), spinning off companies like printer manufacturer Lexmark (1991), and selling off product lines like its personal computer and x86 server businesses to Lenovo (2005, 2014). In 2014, IBM announced that it would go \"fabless\" by offloading IBM Micro Electronics semiconductor manufacturing to GlobalFoundries, a leader in advanced technology manufacturing, citing that semiconductor manufacturing is a capital-intensive business which is challenging to operate without scale. This transition had progressed as of early 2015[update].On June 16, 1911, their four companies were consolidated in New York State by Charles Ranlett Flint to form the Computing-Tabulating-Recording Company (CTR). CTR\\'s business office was in Endicott. The individual companies owned by CTR continued to operate using their established names until the businesses were integrated in 1933 and the holding company eliminated. The four companies had 1,300 employees and offices and plants in Endicott and Binghamton, New York; Dayton, Ohio; Detroit, Michigan; Washington, D.C.; and Toronto. They manufactured machinery for sale and lease, ranging from commercial scales and industrial time recorders, meat and cheese slicers, to tabulators and punched cards.Thomas J. Watson, Sr., fired from the National Cash Register Company by John Henry Patterson, called on Flint and, in 1914, was offered CTR. Watson joined CTR as General Manager then, 11 months later, was made President when court cases relating to his time at NCR were resolved. Having learned Patterson\\'s pioneering business practices, Watson proceeded to put the stamp of NCR onto CTR\\'s companies. He implemented sales conventions, \"generous sales incentives, a focus on customer service, an insistence on well-groomed, dark-suited salesmen and had an evangelical fervor for instilling company pride and loyalty in every worker\". His favorite slogan, \"THINK\", became a mantra for each company\\'s employees. During Watson\\'s first four years, revenues more than doubled to $9 million and the company\\'s operations expanded to Europe, South America, Asia and Australia. \"Watson had never liked the clumsy hyphenated title of the CTR\" and chose to replace it with the more expansive title \"International Business Machines\". First as a name for a 1917 Canadian subsidiary, then as a line in advertisements. For example, the McClures magazine, v53, May 1921, has a full page ad with, at the bottom:In 1937, IBM\\'s tabulating equipment enabled organizations to process unprecedented amounts of data, its clients including the U.S. Government, during its first effort to maintain the employment records for 26 million people pursuant to the Social Security Act, and the Third Reich, largely through the German subsidiary Dehomag. During the Second World War the company produced small arms for the American war effort (M1 Carbine, and Browning Automatic Rifle). IBM provided translation services for the Nuremberg Trials. In 1947, IBM opened its first office in Bahrain, as well as an office in Saudi Arabia to service the needs of the Arabian-American Oil Company that would grow to become Saudi Business Machines (SBM).In 1952, Thomas Watson, Sr., stepped down after almost 40 years at the company helm; his son, Thomas Watson, Jr., was named president. In 1956, the company demonstrated the first practical example of artificial intelligence when Arthur L. Samuel of IBM\\'s Poughkeepsie, New York, laboratory programmed an IBM 704 not merely to play checkers but \"learn\" from its own experience. In 1957, the FORTRAN (FORmula TRANslation) scientific programming language was developed. In 1961, Thomas J. Watson, Jr., was elected chairman of the board and Albert L. Williams became company president. The same year IBM developed the SABRE (Semi-Automatic Business-Related Environment) reservation system for American Airlines and introduced the highly successful Selectric typewriter.In 2002, IBM acquired PwC consulting. In 2003 it initiated a project to redefine company values. Using its Jam technology, it hosted a three-day Internet-based online discussion of key business issues with 50,000 employees. Results were data mined with sophisticated text analysis software (eClassifier) for common themes. Three emerged, expressed as: \"Dedication to every client\\'s success\", \"Innovation that matters—for our company and for the world\", and \"Trust and personal responsibility in all relationships\". Another three-day Jam took place in 2004, with 52,000 employees discussing ways to implement company values in practice.In 2005, the company sold its personal computer business to Chinese technology company Lenovo, and in the same year it agreed to acquire Micromuse. A year later IBM launched Secure Blue, a low-cost hardware design for data encryption that can be built into a microprocessor. In 2009 it acquired software company SPSS Inc. Later in 2009, IBM\\'s Blue Gene supercomputing program was awarded the National Medal of Technology and Innovation by U.S. President Barack Obama. In 2011, IBM gained worldwide attention for its artificial intelligence program Watson, which was exhibited on Jeopardy! where it won against game-show champions Ken Jennings and Brad Rutter. As of 2012[update], IBM had been the top annual recipient of U.S. patents for 20 consecutive years.On October 28, 2015, IBM announced its acquisition of digital assets from The Weather Company—a holding company of Bain Capital, The Blackstone Group and NBCUniversal which owns The Weather Channel, including its weather data platforms (such as Weather Services International), websites (Weather.com and Weather Underground) and mobile apps. The acquisition seeks to use Watson for weather analytics and predictions. The acquisition does not include The Weather Channel itself, which will enter into a long-term licensing agreement with IBM for use of its data. The sale closed on January 29, 2016 The company\\'s 14 member Board of Directors is responsible for overall corporate management. As of Cathie Black\\'s resignation in November 2010 its membership (by affiliation and year of joining) included: Alain J. P. Belda \\'08 (Alcoa), William R. Brody \\'07 (Salk Institute / Johns Hopkins University), Kenneth Chenault \\'98 (American Express), Michael L. Eskew \\'05 (UPS), Shirley Ann Jackson \\'05 (Rensselaer Polytechnic Institute), Andrew N. Liveris \\'10 (Dow Chemical), W. James McNerney, Jr. \\'09 (Boeing), James W. Owens \\'06 (Caterpillar), Samuel J. Palmisano \\'00 (IBM), Joan Spero \\'04 (Doris Duke Charitable Foundation), Sidney Taurel \\'01 (Eli Lilly), and Lorenzo Zambrano \\'03 (Cemex).On January 21, 2014 IBM announced that company executives would forgo bonuses for fiscal year 2013. The move came as the firm reported a 5% drop in sales and 1% decline in net profit over 2012. It also committed to a $1.2bn plus expansion of its data center and cloud-storage business, including the development of 15 new data centers. After ten successive quarters of flat or sliding sales under Chief Executive Virginia Rometty IBM is being forced to look at new approaches. Said Rometty, “We’ve got to reinvent ourselves like we’ve done in prior generations.”Other major campus installations include towers in Montreal, Paris, and Atlanta; software labs in Raleigh-Durham, Rome, Cracow and Toronto; Johannesburg, Seattle; and facilities in Hakozaki and Yamato. The company also operates the IBM Scientific Center, Hursley House, the Canada Head Office Building, IBM Rochester, and the Somers Office Complex. The company\\'s contributions to architecture and design, which include works by Eero Saarinen, Ludwig Mies van der Rohe, and I.M. Pei, have been recognized. Van der Rohe\\'s 330 North Wabash building in Chicago, the original center of the company\\'s research division post-World War II, was recognized with the 1990 Honor Award from the National Building Museum.IBM\\'s employee management practices can be traced back to its roots. In 1914, CEO Thomas J. Watson boosted company spirit by creating employee sports teams, hosting family outings, and furnishing a company band. IBM sports teams still continue in the present day; the IBM Big Blue continue to exist as semi-professional company rugby and American football teams. In 1924 the Quarter Century Club, which recognizes employees with 25 years of service, was organized and the first issue of Business Machines, IBM\\'s internal publication, was published. In 1925, the first meeting of the Hundred Percent Club, composed of IBM salesmen who meet their quotas, convened in Atlantic City, New Jersey.IBM was among the first corporations to provide group life insurance (1934), survivor benefits (1935) and paid vacations (1937). In 1932 IBM created an Education Department to oversee training for employees, which oversaw the completion of the IBM Schoolhouse at Endicott in 1933. In 1935, the employee magazine Think was created. Also that year, IBM held its first training class for female systems service professionals. In 1942, IBM launched a program to train and employ disabled people in Topeka, Kansas. The next year classes began in New York City, and soon the company was asked to join the President\\'s Committee for Employment of the Handicapped. In 1946, the company hired its first black salesman, 18 years before the Civil Rights Act of 1964. In 1947, IBM announced a Total and Permanent Disability Income Plan for employees. A vested rights pension was added to the IBM retirement plan. During IBM\\'s management transformation in the 1990s revisions were made to these pension plans to reduce IBM\\'s pension liabilities.In 1952, Thomas J. Watson, Jr., published the company\\'s first written equal opportunity policy letter, one year before the U.S. Supreme Court decision in Brown vs. Board of Education and 11 years before the Civil Rights Act of 1964. In 1961, IBM\\'s nondiscrimination policy was expanded to include sex, national origin, and age. The following year, IBM hosted its first Invention Award Dinner honoring 34 outstanding IBM inventors; and in 1963, the company named the first eight IBM Fellows in a new Fellowship Program that recognizes senior IBM scientists, engineers and other professionals for outstanding technical achievements.On September 21, 1953, Thomas Watson, Jr., the company\\'s president at the time, sent out a controversial letter to all IBM employees stating that IBM needed to hire the best people, regardless of their race, ethnic origin, or gender. He also publicized the policy so that in his negotiations to build new manufacturing plants with the governors of two states in the U.S. South, he could be clear that IBM would not build \"separate-but-equal\" workplaces. In 1984, IBM added sexual orientation to its nondiscrimination policy. The company stated that this would give IBM a competitive advantage because IBM would then be able to hire talented people its competitors would turn down.IBM has been a leading proponent of the Open Source Initiative, and began supporting Linux in 1998. The company invests billions of dollars in services and software based on Linux through the IBM Linux Technology Center, which includes over 300 Linux kernel developers. IBM has also released code under different open source licenses, such as the platform-independent software framework Eclipse (worth approximately US$40 million at the time of the donation), the three-sentence International Components for Unicode (ICU) license, and the Java-based relational database management system (RDBMS) Apache Derby. IBM\\'s open source involvement has not been trouble-free, however (see SCO v. IBM).DeveloperWorks is a website run by IBM for software developers and IT professionals. It contains how-to articles and tutorials, as well as software downloads and code samples, discussion forums, podcasts, blogs, wikis, and other resources for developers and technical professionals. Subjects range from open, industry-standard technologies like Java, Linux, SOA and web services, web development, Ajax, PHP, and XML to IBM\\'s products (WebSphere, Rational, Lotus, Tivoli and Information Management). In 2007, developerWorks was inducted into the Jolt Hall of Fame.Virtually all console gaming systems of the previous generation used microprocessors developed by IBM. The Xbox 360 contains a PowerPC tri-core processor, which was designed and produced by IBM in less than 24 months. Sony\\'s PlayStation 3 features the Cell BE microprocessor designed jointly by IBM, Toshiba, and Sony. IBM also provided the microprocessor that serves as the heart of Nintendo\\'s new Wii U system, which debuted in 2012. The new Power Architecture-based microprocessor includes IBM\\'s latest technology in an energy-saving silicon package. Nintendo\\'s seventh-generation console, Wii, features an IBM chip codenamed Broadway. The older Nintendo GameCube utilizes the Gekko processor, also designed by IBM.IBM announced it will launch its new software, called \"Open Client Offering\" which is to run on Linux, Microsoft Windows and Apple\\'s Mac OS X. The company states that its new product allows businesses to offer employees a choice of using the same software on Windows and its alternatives. This means that \"Open Client Offering\" is to cut costs of managing whether to use Linux or Apple relative to Windows. There will be no necessity for companies to pay Microsoft for its licenses for operating systems since the operating systems will no longer rely on software which is Windows-based. One alternative to Microsoft\\'s office document formats is the Open Document Format software, whose development IBM supports. It is going to be used for several tasks like: word processing, presentations, along with collaboration with Lotus Notes, instant messaging and blog tools as well as an Internet Explorer competitor – the Mozilla Firefox web browser. IBM plans to install Open Client on 5% of its desktop PCs. The Linux offering has been made available as the IBM Client for Smart Work product on the Ubuntu and Red Hat Enterprise Linux platforms.In 2006, IBM launched Secure Blue, encryption hardware that can be built into microprocessors. A year later, IBM unveiled Project Big Green, a re-direction of $1 billion per year across its businesses to increase energy efficiency. On November 2008, IBM’s CEO, Sam Palmisano, during a speech at the Council on Foreign Relations, outlined a new agenda for building a Smarter Planet. On March 1, 2011, IBM announced the Smarter Computing framework to support Smarter Planet. On Aug 18, 2011, as part of its effort in cognitive computing, IBM has produced chips that imitate neurons and synapses. These microprocessors do not use von Neumann architecture, and they consume less memory and power.IBM also holds the SmartCamp program globally. The program searches for fresh start-up companies that IBM can partner with to solve world problems. IBM holds 17 SmartCamp events around the world. Since July 2011, IBM has partnered with Pennies, the electronic charity box, and produced a software solution for IBM retail customers that provides an easy way to donate money when paying in-store by credit or debit card. Customers donate just a few pence (1p-99p) a time and every donation goes to UK charities.The birthplace of IBM, Endicott, suffered pollution for decades, however. IBM used liquid cleaning agents in circuit board assembly operation for more than two decades, and six spills and leaks were recorded, including one leak in 1979 of 4,100 gallons from an underground tank. These left behind volatile organic compounds in the town\\'s soil and aquifer. Traces of volatile organic compounds have been identified in Endicott’s drinking water, but the levels are within regulatory limits. Also, from 1980, IBM has pumped out 78,000 gallons of chemicals, including trichloroethane, freon, benzene and perchloroethene to the air and allegedly caused several cancer cases among the townspeople. IBM Endicott has been identified by the Department of Environmental Conservation as the major source of pollution, though traces of contaminants from a local dry cleaner and other polluters were also found. Remediation and testing are ongoing, however according to city officials, tests show that the water is safe to drink.',\n",
       "             13: 'As the number of possible tests for even simple software components is practically infinite, all software testing uses some strategy to select tests that are feasible for the available time and resources. As a result, software testing typically (but not exclusively) attempts to execute a program or application with the intent of finding software bugs (errors or other defects). The job of testing is an iterative process as when one bug is fixed, it can illuminate other, deeper bugs, or can even create new ones.Although testing can determine the correctness of software under the assumption of some specific hypotheses (see hierarchy of testing difficulty below), testing cannot identify all the defects within software. Instead, it furnishes a criticism or comparison that compares the state and behavior of the product against oracles—principles or mechanisms by which someone might recognize a problem. These oracles may include (but are not limited to) specifications, contracts, comparable products, past versions of the same product, inferences about intended or expected purpose, user or customer expectations, relevant standards, applicable laws, or other criteria.A primary purpose of testing is to detect software failures so that defects may be discovered and corrected. Testing cannot establish that a product functions properly under all conditions but can only establish that it does not function properly under specific conditions. The scope of software testing often includes examination of code as well as execution of that code in various environments and conditions as well as examining the aspects of code: does it do what it is supposed to do and do what it needs to do. In the current culture of software development, a testing organization may be separate from the development team. There are various roles for testing team members. Information derived from software testing may be used to correct the process by which software is developed.Software faults occur through the following processes. A programmer makes an error (mistake), which results in a defect (fault, bug) in the software source code. If this defect is executed, in certain situations the system will produce wrong results, causing a failure. Not all defects will necessarily result in failures. For example, defects in dead code will never result in failures. A defect can turn into a failure when the environment is changed. Examples of these changes in environment include the software being run on a new computer hardware platform, alterations in source data, or interacting with different software. A single defect may result in a wide range of failure symptoms.A fundamental problem with software testing is that testing under all combinations of inputs and preconditions (initial state) is not feasible, even with a simple product.:17-18 This means that the number of defects in a software product can be very large and defects that occur infrequently are difficult to find in testing. More significantly, non-functional dimensions of quality (how it is supposed to be versus what it is supposed to do)—usability, scalability, performance, compatibility, reliability—can be highly subjective; something that constitutes sufficient value to one person may be intolerable to another.Software developers can\\'t test everything, but they can use combinatorial test design to identify the minimum number of tests needed to get the coverage they want. Combinatorial test design enables users to get greater test coverage with fewer tests. Whether they are looking for speed or test depth, they can use combinatorial test design methods to build structured variation into their test cases. Note that \"coverage\", as used here, is referring to combinatorial coverage, not requirements coverage.It is commonly believed that the earlier a defect is found, the cheaper it is to fix it. The following table shows the cost of fixing the defect depending on the stage it was found. For example, if a problem in the requirements is found only post-release, then it would cost 10–100 times more to fix than if it had already been found by the requirements review. With the advent of modern continuous deployment practices and cloud-based services, the cost of re-deployment and maintenance may lessen over time.There are many approaches available in software testing. Reviews, walkthroughs, or inspections are referred to as static testing, whereas actually executing programmed code with a given set of test cases is referred to as dynamic testing. Static testing is often implicit, as proofreading, plus when programming tools/text editors check source code structure or compilers (pre-compilers) check syntax and data flow as static program analysis. Dynamic testing takes place when the program itself is run. Dynamic testing may begin before the program is 100% complete in order to test particular sections of code and are applied to discrete functions or modules. Typical techniques for this are either using stubs/drivers or execution from a debugger environment.White-box testing (also known as clear box testing, glass box testing, transparent box testing and structural testing, by seeing the source code) tests internal structures or workings of a program, as opposed to the functionality exposed to the end-user. In white-box testing an internal perspective of the system, as well as programming skills, are used to design test cases. The tester chooses inputs to exercise paths through the code and determine the appropriate outputs. This is analogous to testing nodes in a circuit, e.g. in-circuit testing (ICT).Black-box testing treats the software as a \"black box\", examining functionality without any knowledge of internal implementation, without seeing the source code. The testers are only aware of what the software is supposed to do, not how it does it. Black-box testing methods include: equivalence partitioning, boundary value analysis, all-pairs testing, state transition tables, decision table testing, fuzz testing, model-based testing, use case testing, exploratory testing and specification-based testing.Specification-based testing aims to test the functionality of software according to the applicable requirements. This level of testing usually requires thorough test cases to be provided to the tester, who then can simply verify that for a given input, the output value (or behavior), either \"is\" or \"is not\" the same as the expected value specified in the test case. Test cases are built around specifications and requirements, i.e., what the application is supposed to do. It uses external descriptions of the software, including specifications, requirements, and designs to derive test cases. These tests can be functional or non-functional, though usually functional.One advantage of the black box technique is that no programming knowledge is required. Whatever biases the programmers may have had, the tester likely has a different set and may emphasize different areas of functionality. On the other hand, black-box testing has been said to be \"like a walk in a dark labyrinth without a flashlight.\" Because they do not examine the source code, there are situations when a tester writes many test cases to check something that could have been tested by only one test case, or leaves some parts of the program untested.Grey-box testing (American spelling: gray-box testing) involves having knowledge of internal data structures and algorithms for purposes of designing tests, while executing those tests at the user, or black-box level. The tester is not required to have full access to the software\\'s source code.[not in citation given] Manipulating input data and formatting output do not qualify as grey-box, because the input and output are clearly outside of the \"black box\" that we are calling the system under test. This distinction is particularly important when conducting integration testing between two modules of code written by two different developers, where only the interfaces are exposed for test.By knowing the underlying concepts of how the software works, the tester makes better-informed testing choices while testing the software from outside. Typically, a grey-box tester will be permitted to set up an isolated testing environment with activities such as seeding a database. The tester can observe the state of the product being tested after performing certain actions such as executing SQL statements against the database and then executing queries to ensure that the expected changes have been reflected. Grey-box testing implements intelligent test scenarios, based on limited information. This will particularly apply to data type handling, exception handling, and so on.There are generally four recognized levels of tests: unit testing, integration testing, component interface testing, and system testing. Tests are frequently grouped by where they are added in the software development process, or by the level of specificity of the test. The main levels during the development process as defined by the SWEBOK guide are unit-, integration-, and system testing that are distinguished by the test target without implying a specific process model. Other test levels are classified by the testing objective.Unit testing is a software development process that involves synchronized application of a broad spectrum of defect prevention and detection strategies in order to reduce software development risks, time, and costs. It is performed by the software developer or engineer during the construction phase of the software development lifecycle. Rather than replace traditional QA focuses, it augments it. Unit testing aims to eliminate construction errors before code is promoted to QA; this strategy is intended to increase the quality of the resulting software as well as the efficiency of the overall development and QA process.The practice of component interface testing can be used to check the handling of data passed between various units, or subsystem components, beyond full integration testing between those units. The data being passed can be considered as \"message packets\" and the range or data types can be checked, for data generated from one unit, and tested for validity before being passed into another unit. One option for interface testing is to keep a separate log file of data items being passed, often with a timestamp logged to allow analysis of thousands of cases of data passed between units for days or weeks. Tests can include checking the handling of some extreme data values while other interface variables are passed as normal values. Unusual data values in an interface can help explain unexpected performance in the next unit. Component interface testing is a variation of black-box testing, with the focus on the data values beyond just the related actions of a subsystem component.Operational Acceptance is used to conduct operational readiness (pre-release) of a product, service or system as part of a quality management system. OAT is a common type of non-functional software testing, used mainly in software development and software maintenance projects. This type of testing focuses on the operational readiness of the system to be supported, and/or to become part of the production environment. Hence, it is also known as operational readiness testing (ORT) or Operations readiness and assurance (OR&A) testing. Functional testing within OAT is limited to those tests which are required to verify the non-functional aspects of the system.A common cause of software failure (real or perceived) is a lack of its compatibility with other application software, operating systems (or operating system versions, old or new), or target environments that differ greatly from the original (such as a terminal or GUI application intended to be run on the desktop now being required to become a web application, which must render in a web browser). For example, in the case of a lack of backward compatibility, this can occur because the programmers develop and test software only on the latest version of the target environment, which not all users may be running. This results in the unintended consequence that the latest work may not function on earlier versions of the target environment, or on older hardware that earlier versions of the target environment was capable of using. Sometimes such issues can be fixed by proactively abstracting operating system functionality into a separate program module or library.Regression testing focuses on finding defects after a major code change has occurred. Specifically, it seeks to uncover software regressions, as degraded or lost features, including old bugs that have come back. Such regressions occur whenever software functionality that was previously working correctly, stops working as intended. Typically, regressions occur as an unintended consequence of program changes, when the newly developed part of the software collides with the previously existing code. Common methods of regression testing include re-running previous sets of test-cases and checking whether previously fixed faults have re-emerged. The depth of testing depends on the phase in the release process and the risk of the added features. They can either be complete, for changes added late in the release or deemed to be risky, or be very shallow, consisting of positive tests on each feature, if the changes are early in the release or deemed to be of low risk. Regression testing is typically the largest test effort in commercial software development, due to checking numerous details in prior software features, and even new software can be developed while using some old test-cases to test parts of the new design to ensure prior functionality is still supported.Beta testing comes after alpha testing and can be considered a form of external user acceptance testing. Versions of the software, known as beta versions, are released to a limited audience outside of the programming team known as beta testers. The software is released to groups of people so that further testing can ensure the product has few faults or bugs. Beta versions can be made available to the open public to increase the feedback field to a maximal number of future users and to deliver value earlier, for an extended or even indefinite period of time (perpetual beta).[citation needed]Destructive testing attempts to cause the software or a sub-system to fail. It verifies that the software functions properly even when it receives invalid or unexpected inputs, thereby establishing the robustness of input validation and error-management routines.[citation needed] Software fault injection, in the form of fuzzing, is an example of failure testing. Various commercial non-functional testing tools are linked from the software fault injection page; there are also numerous open-source and free software tools available that perform destructive testing.Load testing is primarily concerned with testing that the system can continue to operate under a specific load, whether that be large quantities of data or a large number of users. This is generally referred to as software scalability. The related load testing activity of when performed as a non-functional activity is often referred to as endurance testing. Volume testing is a way to test software functions even when certain components (for example a file or database) increase radically in size. Stress testing is a way to test reliability under unexpected or rare workloads. Stability testing (often referred to as load or endurance testing) checks to see if the software can continuously function well in or above an acceptable period.Development Testing is a software development process that involves synchronized application of a broad spectrum of defect prevention and detection strategies in order to reduce software development risks, time, and costs. It is performed by the software developer or engineer during the construction phase of the software development lifecycle. Rather than replace traditional QA focuses, it augments it. Development Testing aims to eliminate construction errors before code is promoted to QA; this strategy is intended to increase the quality of the resulting software as well as the efficiency of the overall development and QA process.In contrast, some emerging software disciplines such as extreme programming and the agile software development movement, adhere to a \"test-driven software development\" model. In this process, unit tests are written first, by the software engineers (often with pair programming in the extreme programming methodology). Of course these tests fail initially; as they are expected to. Then as code is written it passes incrementally larger portions of the test suites. The test suites are continuously updated as new failure conditions and corner cases are discovered, and they are integrated with any regression tests that are developed. Unit tests are maintained along with the rest of the software source code and generally integrated into the build process (with inherently interactive tests being relegated to a partially manual build acceptance process). The ultimate goal of this test process is to achieve continuous integration where software updates can be published to the public frequently.  Bottom Up Testing is an approach to integrated testing where the lowest level components (modules, procedures, and functions) are tested first, then integrated and used to facilitate the testing of higher level components. After the integration testing of lower level integrated modules, the next level of modules will be formed and can be used for integration testing. The process is repeated until the components at the top of the hierarchy are tested. This approach is helpful only when all or most of the modules of the same development level are ready.[citation needed] This method also helps to determine the levels of software developed and makes it easier to report testing progress in the form of a percentage.[citation needed]It has been proved that each class is strictly included into the next. For instance, testing when we assume that the behavior of the implementation under test can be denoted by a deterministic finite-state machine for some known finite sets of inputs and outputs and with some known number of states belongs to Class I (and all subsequent classes). However, if the number of states is not known, then it only belongs to all classes from Class II on. If the implementation under test must be a deterministic finite-state machine failing the specification for a single trace (and its continuations), and its number of states is unknown, then it only belongs to classes from Class III on. Testing temporal machines where transitions are triggered if inputs are produced within some real-bounded interval only belongs to classes from Class IV on, whereas testing many non-deterministic systems only belongs to Class V (but not all, and some even belong to Class I). The inclusion into Class I does not require the simplicity of the assumed computation model, as some testing cases involving implementations written in any programming language, and testing implementations defined as machines depending on continuous magnitudes, have been proved to be in Class I. Other elaborated cases, such as the testing framework by Matthew Hennessy under must semantics, and temporal machines with rational timeouts, belong to Class II.Several certification programs exist to support the professional aspirations of software testers and quality assurance specialists. No certification now offered actually requires the applicant to show their ability to test software. No certification is based on a widely accepted body of knowledge. This has led some to declare that the testing field is not ready for certification. Certification itself cannot measure an individual\\'s productivity, their skill, or practical knowledge, and cannot guarantee their competence, or professionalism as a tester.Software testing is a part of the software quality assurance (SQA) process.:347 In SQA, software process specialists and auditors are concerned for the software development process rather than just the artifacts such as documentation, code and systems. They examine and change the software engineering process itself to reduce the number of faults that end up in the delivered software: the so-called \"defect rate\". What constitutes an \"acceptable defect rate\" depends on the nature of the software; A flight simulator video game would have much higher defect tolerance than software for an actual airplane. Although there are close links with SQA, testing departments often exist independently, and there may be no SQA function in some companies.[citation needed]',\n",
       "             14: 'A database management system (DBMS) is a computer software application that interacts with the user, other applications, and the database itself to capture and analyze data. A general-purpose DBMS is designed to allow the definition, creation, querying, update, and administration of databases. Well-known DBMSs include MySQL, PostgreSQL, Microsoft SQL Server, Oracle, Sybase, SAP HANA, and IBM DB2. A database is not generally portable across different DBMSs, but different DBMS can interoperate by using standards such as SQL and ODBC or JDBC to allow a single application to work with more than one DBMS. Database management systems are often classified according to the database model that they support; the most popular database systems since the 1980s have all supported the relational model as represented by the SQL language.[disputed – discuss] Sometimes a DBMS is loosely referred to as a \\'database\\'.Formally, a \"database\" refers to a set of related data and the way it is organized. Access to these data is usually provided by a \"database management system\" (DBMS) consisting of an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized.Physically, database servers are dedicated computers that hold the actual databases and run only the DBMS and related software. Database servers are usually multiprocessor computers, with generous memory and RAID disk arrays used for stable storage. RAID is used for recovery of data if any of the disks fail. Hardware database accelerators, connected to one or more servers via a high-speed channel, are also used in large volume transaction processing environments. DBMSs are found at the heart of most database applications. DBMSs may be built around a custom multitasking kernel with built-in networking support, but modern DBMSs typically rely on a standard operating system to provide these functions. from databases before the inception of Structured Query Language (SQL). The data recovered was disparate, redundant and disorderly, since there was no proper method to fetch it and arrange it in a concrete structure.[citation needed]A DBMS has evolved into a complex software system and its development typically requires thousands of human years of development effort.[a] Some general-purpose DBMSs such as Adabas, Oracle and DB2 have been undergoing upgrades since the 1970s. General-purpose DBMSs aim to meet the needs of as many applications as possible, which adds to the complexity. However, the fact that their development cost can be spread over a large number of users means that they are often the most cost-effective approach. However, a general-purpose DBMS is not always the optimal solution: in some cases a general-purpose DBMS may introduce unnecessary overhead. Therefore, there are many examples of systems that use special-purpose databases. A common example is an email system that performs many of the functions of a general-purpose DBMS such as the insertion and deletion of messages composed of various items of data or associating messages with a particular email address; but these functions are limited to what is required to handle email and don\\'t provide the user with all of the functionality that would be available using a general-purpose DBMS.Many other databases have application software that accesses the database on behalf of end-users, without exposing the DBMS interface directly. Application programmers may use a wire protocol directly, or more likely through an application programming interface. Database designers and database administrators interact with the DBMS through dedicated interfaces to build and maintain the applications\\' databases, and thus need some more knowledge and understanding about how DBMSs operate and the DBMSs\\' external interfaces and tuning parameters.The relational model, first proposed in 1970 by Edgar F. Codd, departed from this tradition by insisting that applications should search for data by content, rather than by following links. The relational model employs sets of ledger-style tables, each used for a different type of entity. Only in the mid-1980s did computing hardware become powerful enough to allow the wide deployment of relational systems (DBMSs plus applications). By the early 1990s, however, relational systems dominated in all large-scale data processing applications, and as of 2015[update] they remain dominant : IBM DB2, Oracle, MySQL and Microsoft SQL Server are the top DBMS. The dominant database language, standardised SQL for the relational model, has influenced database languages for other data models.[citation needed]As computers grew in speed and capability, a number of general-purpose database systems emerged; by the mid-1960s a number of such systems had come into commercial use. Interest in a standard began to grow, and Charles Bachman, author of one such product, the Integrated Data Store (IDS), founded the \"Database Task Group\" within CODASYL, the group responsible for the creation and standardization of COBOL. In 1971 the Database Task Group delivered their standard, which generally became known as the \"CODASYL approach\", and soon a number of commercial products based on this approach entered the market.IBM also had their own DBMS in 1966, known as Information Management System (IMS). IMS was a development of software written for the Apollo program on the System/360. IMS was generally similar in concept to CODASYL, but used a strict hierarchy for its model of data navigation instead of CODASYL\\'s network model. Both concepts later became known as navigational databases due to the way data was accessed, and Bachman\\'s 1973 Turing Award presentation was The Programmer as Navigator. IMS is classified[by whom?] as a hierarchical database. IDMS and Cincom Systems\\' TOTAL database are classified as network databases. IMS remains in use as of 2014[update].In this paper, he described a new system for storing and working with large databases. Instead of records being stored in some sort of linked list of free-form records as in CODASYL, Codd\\'s idea was to use a \"table\" of fixed-length records, with each table used for a different type of entity. A linked-list system would be very inefficient when storing \"sparse\" databases where some of the data for any one record could be left empty. The relational model solved this by splitting the data into a series of normalized tables (or relations), with optional elements being moved out of the main table to where they would take up room only if needed. Data may be freely inserted, deleted and edited in these tables, with the DBMS doing whatever maintenance needed to present a table view to the application/user.The relational model also allowed the content of the database to evolve without constant rewriting of links and pointers. The relational part comes from entities referencing other entities in what is known as one-to-many relationship, like a traditional hierarchical model, and many-to-many relationship, like a navigational (network) model. Thus, a relational model can express both hierarchical and navigational models, as well as its native tabular model, allowing for pure or combined modeling in terms of these three models, as the application requires.For instance, a common use of a database system is to track information about users, their name, login information, various addresses and phone numbers. In the navigational approach all of this data would be placed in a single record, and unused items would simply not be placed in the database. In the relational approach, the data would be normalized into a user table, an address table and a phone number table (for instance). Records would be created in these optional tables only if the address or phone numbers were actually provided.Linking the information back together is the key to this system. In the relational model, some bit of information was used as a \"key\", uniquely defining a particular record. When information was being collected about a user, information stored in the optional tables would be found by searching for this key. For instance, if the login name of a user is unique, addresses and phone numbers for that user would be recorded with the login name as its key. This simple \"re-linking\" of related data back into a single collection is something that traditional computer languages are not designed for.Just as the navigational approach would require programs to loop in order to collect records, the relational approach would require loops to collect information about any one record. Codd\\'s solution to the necessary looping was a set-oriented language, a suggestion that would later spawn the ubiquitous SQL. Using a branch of mathematics known as tuple calculus, he demonstrated that such a system could support all the operations of normal databases (inserting, updating etc.) as well as providing a simple system for finding and returning sets of data in a single operation.Codd\\'s paper was picked up by two people at Berkeley, Eugene Wong and Michael Stonebraker. They started a project known as INGRES using funding that had already been allocated for a geographical database project and student programmers to produce code. Beginning in 1973, INGRES delivered its first test products which were generally ready for widespread use in 1979. INGRES was similar to System R in a number of ways, including the use of a \"language\" for data access, known as QUEL. Over time, INGRES moved to the emerging SQL standard.Another approach to hardware support for database management was ICL\\'s CAFS accelerator, a hardware disk controller with programmable search capabilities. In the long term, these efforts were generally unsuccessful because specialized database machines could not keep pace with the rapid development and progress of general-purpose computers. Thus most database systems nowadays are software systems running on general-purpose hardware, using general-purpose computer data storage. However this idea is still pursued for certain applications by some companies like Netezza and Oracle (Exadata).IBM started working on a prototype system loosely based on Codd\\'s concepts as System R in the early 1970s. The first version was ready in 1974/5, and work then started on multi-table systems in which the data could be split so that all of the data for a record (some of which is optional) did not have to be stored in a single large \"chunk\". Subsequent multi-user versions were tested by customers in 1978 and 1979, by which time a standardized query language – SQL[citation needed] – had been added. Codd\\'s ideas were establishing themselves as both workable and superior to CODASYL, pushing IBM to develop a true production version of System R, known as SQL/DS, and, later, Database 2 (DB2).The 1980s ushered in the age of desktop computing. The new computers empowered their users with spreadsheets like Lotus 1-2-3 and database software like dBASE. The dBASE product was lightweight and easy for any computer user to understand out of the box. C. Wayne Ratliff the creator of dBASE stated: \"dBASE was different from programs like BASIC, C, FORTRAN, and COBOL in that a lot of the dirty work had already been done. The data manipulation is done by dBASE instead of by the user, so the user can concentrate on what he is doing, rather than having to mess with the dirty details of opening, reading, and closing files, and managing space allocation.\" dBASE was one of the top selling software titles in the 1980s and early 1990s.The 1990s, along with a rise in object-oriented programming, saw a growth in how data in various databases were handled. Programmers and designers began to treat the data in their databases as objects. That is to say that if a person\\'s data were in a database, that person\\'s attributes, such as their address, phone number, and age, were now considered to belong to that person instead of being extraneous data. This allows for relations between data to be relations to objects and their attributes and not to individual fields. The term \"object-relational impedance mismatch\" described the inconvenience of translating between programmed objects and database tables. Object databases and object-relational databases attempt to solve this problem by providing an object-oriented language (sometimes as extensions to SQL) that programmers can use as alternative to purely relational SQL. On the programming side, libraries known as object-relational mappings (ORMs) attempt to solve the same problem.\\nXML databases are a type of structured document-oriented database that allows querying based on XML document attributes. XML databases are mostly used in enterprise database management, where XML is being used as the machine-to-machine data interoperability standard. XML database management systems include commercial software MarkLogic and Oracle Berkeley DB XML, and a free use software Clusterpoint Distributed XML/JSON Database. All are enterprise software database platforms and support industry standard ACID-compliant transaction processing with strong database consistency characteristics and high level of database security.In recent years there was a high demand for massively distributed databases with high partition tolerance but according to the CAP theorem it is impossible for a distributed system to simultaneously provide consistency, availability and partition tolerance guarantees. A distributed system can satisfy any two of these guarantees at the same time, but not all three. For that reason many NoSQL databases are using what is called eventual consistency to provide both availability and partition tolerance guarantees with a reduced level of data consistency.The first task of a database designer is to produce a conceptual data model that reflects the structure of the information to be held in the database. A common approach to this is to develop an entity-relationship model, often with the aid of drawing tools. Another popular approach is the Unified Modeling Language. A successful data model will accurately reflect the possible state of the external world being modeled: for example, if people can have more than one phone number, it will allow this information to be captured. Designing a good conceptual data model requires a good understanding of the application domain; it typically involves asking deep questions about the things of interest to an organisation, like \"can a customer also be a supplier?\", or \"if a product is sold with two different forms of packaging, are those the same product or different products?\", or \"if a plane flies from New York to Dubai via Frankfurt, is that one flight or two (or maybe even three)?\". The answers to these questions establish definitions of the terminology used for entities (customers, products, flights, flight segments) and their relationships and attributes.Having produced a conceptual data model that users are happy with, the next stage is to translate this into a schema that implements the relevant data structures within the database. This process is often called logical database design, and the output is a logical data model expressed in the form of a schema. Whereas the conceptual data model is (in theory at least) independent of the choice of database technology, the logical data model will be expressed in terms of a particular database model supported by the chosen DBMS. (The terms data model and database model are often used interchangeably, but in this article we use data model for the design of a specific database, and database model for the modelling notation used to express that design.)The final stage of database design is to make the decisions that affect performance, scalability, recovery, security, and the like. This is often called physical database design. A key goal during this stage is data independence, meaning that the decisions made for performance optimization purposes should be invisible to end-users and applications. Physical design is driven mainly by performance requirements, and requires a good knowledge of the expected workload and access patterns, and a deep understanding of the features offered by the chosen DBMS.While there is typically only one conceptual (or logical) and physical (or internal) view of the data, there can be any number of different external views. This allows users to see database information in a more business-related way rather than from a technical, processing viewpoint. For example, a financial department of a company needs the payment details of all employees as part of the company\\'s expenses, but does not need details about employees that are the interest of the human resources department. Thus different departments need different views of the company\\'s database.The conceptual view provides a level of indirection between internal and external. On one hand it provides a common view of the database, independent of different external view structures, and on the other hand it abstracts away details of how the data are stored or managed (internal level). In principle every level, and even every external view, can be presented by a different data model. In practice usually a given DBMS uses the same data model for both the external and the conceptual levels (e.g., relational model). The internal level, which is hidden inside the DBMS and depends on its implementation, requires a different level of detail and uses its own types of data structure types.Database storage is the container of the physical materialization of a database. It comprises the internal (physical) level in the database architecture. It also contains all the information needed (e.g., metadata, \"data about the data\", and internal data structures) to reconstruct the conceptual level and external level from the internal level when needed. Putting data into permanent storage is generally the responsibility of the database engine a.k.a. \"storage engine\". Though typically accessed by a DBMS through the underlying operating system (and often utilizing the operating systems\\' file systems as intermediates for storage layout), storage properties and configuration setting are extremely important for the efficient operation of the DBMS, and thus are closely maintained by database administrators. A DBMS, while in operation, always has its database residing in several types of storage (e.g., memory and external storage). The database data and the additional needed information, possibly in very large amounts, are coded into bits. Data typically reside in the storage in structures that look completely different from the way the data look in the conceptual and external levels, but in ways that attempt to optimize (the best possible) these levels\\' reconstruction when needed by users and programs, as well as for computing additional types of needed information from the data (e.g., when querying the database).Database access control deals with controlling who (a person or a certain computer program) is allowed to access what information in the database. The information may comprise specific database objects (e.g., record types, specific records, data structures), certain computations over certain objects (e.g., query types, or specific queries), or utilizing specific access paths to the former (e.g., using specific indexes or other data structures to access information). Database access controls are set by special authorized (by the database owner) personnel that uses dedicated protected security DBMS interfaces.This may be managed directly on an individual basis, or by the assignment of individuals and privileges to groups, or (in the most elaborate models) through the assignment of individuals and groups to roles which are then granted entitlements. Data security prevents unauthorized users from viewing or updating the database. Using passwords, users are allowed access to the entire database or subsets of it called \"subschemas\". For example, an employee database can contain all the data about an individual employee, but one group of users may be authorized to view only payroll data, while others are allowed access to only work history and medical data. If the DBMS provides a way to interactively enter and update the database, as well as interrogate it, this capability allows for managing personal databases.Database transactions can be used to introduce some level of fault tolerance and data integrity after recovery from a crash. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction\\'s programmer via special transaction commands).A database built with one DBMS is not portable to another DBMS (i.e., the other DBMS cannot run it). However, in some situations it is desirable to move, migrate a database from one DBMS to another. The reasons are primarily economical (different DBMSs may have different total costs of ownership or TCOs), functional, and operational (different DBMSs may have different capabilities). The migration involves the database\\'s transformation from one DBMS type to another. The transformation should maintain (if possible) the database related application (i.e., all related application programs) intact. Thus, the database\\'s conceptual and external architectural levels should be maintained in the transformation. It may be desired that also some aspects of the architecture internal level are maintained. A complex or large database migration may be a complicated and costly (one-time) project by itself, which should be factored into the decision to migrate. This in spite of the fact that tools may exist to help migration between specific DBMSs. Typically a DBMS vendor provides tools to help importing databases from other popular DBMSs.Sometimes it is desired to bring a database back to a previous state (for many reasons, e.g., cases when the database is found corrupted due to a software error, or if it has been updated with erroneous data). To achieve this a backup operation is done occasionally or continuously, where each desired database state (i.e., the values of its data and their embedding in database\\'s data structures) is kept within dedicated backup files (many techniques exist to do this effectively). When this state is needed, i.e., when it is decided by a database administrator to bring the database back to this state (e.g., by specifying this state by a desired point in time when the database was in this state), these files are utilized to restore that state.Static analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the *Abstract interpretation framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques. The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database system has many interesting applications, in particular, for security purposes, such as fine grained access control, watermarking, etc.',\n",
       "             15: 'When the board has no embedded components it is more correctly called a printed wiring board (PWB) or etched wiring board. However, the term printed wiring board has fallen into disuse. A PCB populated with electronic components is called a printed circuit assembly (PCA), printed circuit board assembly or PCB assembly (PCBA). The IPC preferred term for assembled boards is circuit card assembly (CCA), and for assembled backplanes it is backplane assemblies. The term PCB is used informally both for bare and assembled boards.Initially PCBs were designed manually by creating a photomask on a clear mylar sheet, usually at two or four times the true size. Starting from the schematic diagram the component pin pads were laid out on the mylar and then traces were routed to connect the pads. Rub-on dry transfers of common component footprints increased efficiency. Traces were made with self-adhesive tape. Pre-printed non-reproducing grids on the mylar assisted in layout. To fabricate the board, the finished photomask was photolithographically reproduced onto a photoresist coating on the blank copper-clad boards.Panelization is a procedure whereby a number of PCBs are grouped for manufacturing onto a larger board - the panel. Usually a panel consists of a single design but sometimes multiple designs are mixed on a single panel. There are two types of panels: assembly panels - often called arrays - and bare board manufacturing panels. The assemblers often mount components on panels rather than single PCBs because this is efficient. The bare board manufactures always uses panels, not only for efficiency, but because of the requirements the plating process. Thus a manufacturing panel can consist of a grouping of individual PCBs or of arrays, depending on what must be delivered.The panel is eventually broken apart into individual PCBs; this is called depaneling. Separating the individual PCBs is frequently aided by drilling or routing perforations along the boundaries of the individual circuits, much like a sheet of postage stamps. Another method, which takes less space, is to cut V-shaped grooves across the full dimension of the panel. The individual PCBs can then be broken apart along this line of weakness. Today depaneling is often done by lasers which cut the board with no contact. Laser panelization reduces stress on the fragile circuits.Subtractive methods remove copper from an entirely copper-coated board to leave only the desired copper pattern. In additive methods the pattern is electroplated onto a bare substrate using a complex process. The advantage of the additive method is that less material is needed and less waste is produced. In the full additive process the bare laminate is covered with a photosensitive film which is imaged (exposed to light through a mask and then developed which removes the unexposed film). The exposed areas are sensitized in a chemical bath, usually containing palladium and similar to that used for through hole plating which makes the exposed area capable of bonding metal ions. The laminate is then plated with copper in the sensitized areas. When the mask is stripped, the PCB is finished.Semi-additive is the most common process: The unpatterned board has a thin layer of copper already on it. A reverse mask is then applied. (Unlike a subtractive process mask, this mask exposes those parts of the substrate that will eventually become the traces.) Additional copper is then plated onto the board in the unmasked areas; copper may be plated to any desired weight. Tin-lead or other surface platings are then applied. The mask is stripped away and a brief etching step removes the now-exposed bare original copper laminate from the board, isolating the individual traces. Some single-sided boards which have plated-through holes are made in this way. General Electric made consumer radio sets in the late 1960s using additive boards.The simplest method, used for small-scale production and often by hobbyists, is immersion etching, in which the board is submerged in etching solution such as ferric chloride. Compared with methods used for mass production, the etching time is long. Heat and agitation can be applied to the bath to speed the etching rate. In bubble etching, air is passed through the etchant bath to agitate the solution and speed up etching. Splash etching uses a motor-driven paddle to splash boards with etchant; the process has become commercially obsolete since it is not as fast as spray etching. In spray etching, the etchant solution is distributed over the boards by nozzles, and recirculated by pumps. Adjustment of the nozzle pattern, flow rate, temperature, and etchant composition gives predictable control of etching rates and high production rates.Multi-layer printed circuit boards have trace layers inside the board. This is achieved by laminating a stack of materials in a press by applying pressure and heat for a period of time. This results in an inseparable one piece product. For example, a four-layer PCB can be fabricated by starting from a two-sided copper-clad laminate, etch the circuitry on both sides, then laminate to the top and bottom pre-preg and copper foil. It is then drilled, plated, and etched again to get traces on top and bottom layers.Holes through a PCB are typically drilled with small-diameter drill bits made of solid coated tungsten carbide. Coated tungsten carbide is recommended since many board materials are very abrasive and drilling must be high RPM and high feed to be cost effective. Drill bits must also remain sharp so as not to mar or tear the traces. Drilling with high-speed-steel is simply not feasible since the drill bits will dull quickly and thus tear the copper and ruin the boards. The drilling is performed by automated drilling machines with placement controlled by a drill tape or drill file. These computer-generated files are also called numerically controlled drill (NCD) files or \"Excellon files\". The drill file describes the location and size of each drilled hole.The hole walls for boards with two or more layers can be made conductive and then electroplated with copper to form plated-through holes.  These holes electrically connect the conducting layers of the PCB. For multi-layer boards, those with three layers or more, drilling typically produces a smear of the high temperature decomposition products of bonding agent in the laminate system. Before the holes can be plated through, this smear must be removed by a chemical de-smear process, or by plasma-etch. The de-smear process ensures that a good connection is made to the copper layers when the hole is plated through. On high reliability boards a process called etch-back is performed chemically with a potassium permanganate based etchant or plasma. The etch-back removes resin and the glass fibers so that the copper layers extend into the hole and as the hole is plated become integral with the deposited copper.Matte solder is usually fused to provide a better bonding surface or stripped to bare copper. Treatments, such as benzimidazolethiol, prevent surface oxidation of bare copper. The places to which components will be mounted are typically plated, because untreated bare copper oxidizes quickly, and therefore is not readily solderable. Traditionally, any exposed copper was coated with solder by hot air solder levelling (HASL). The HASL finish prevents oxidation from the underlying copper, thereby guaranteeing a solderable surface. This solder was a tin-lead alloy, however new solder compounds are now used to achieve compliance with the RoHS directive in the EU and US, which restricts the use of lead. One of these lead-free compounds is SN100CL, made up of 99.3% tin, 0.7% copper, 0.05% nickel, and a nominal of 60ppm germanium.Other platings used are OSP (organic surface protectant), immersion silver (IAg), immersion tin, electroless nickel with immersion gold coating (ENIG), electroless nickel electroless palladium immersion gold (ENEPIG) and direct gold plating (over nickel). Edge connectors, placed along one edge of some boards, are often nickel plated then gold plated. Another coating consideration is rapid diffusion of coating metal into Tin solder. Tin forms intermetallics such as Cu5Sn6 and Ag3Cu that dissolve into the Tin liquidus or solidus(@50C), stripping surface coating or leaving voids.Electrochemical migration (ECM) is the growth of conductive metal filaments on or in a printed circuit board (PCB) under the influence of a DC voltage bias. Silver, zinc, and aluminum are known to grow whiskers under the influence of an electric field. Silver also grows conducting surface paths in the presence of halide and other ions, making it a poor choice for electronics use. Tin will grow \"whiskers\" due to tension in the plated surface. Tin-Lead or solder plating also grows whiskers, only reduced by the percentage Tin replaced. Reflow to melt solder or tin plate to relieve surface stress lowers whisker incidence. Another coating issue is tin pest, the transformation of tin to a powdery allotrope at low temperature.Areas that should not be soldered may be covered with solder resist (solder mask). One of the most common solder resists used today is called \"LPI\" (liquid photoimageable solder mask).  A photo-sensitive coating is applied to the surface of the PWB, then exposed to light through the solder mask image film, and finally developed where the unexposed areas are washed away. Dry film solder mask is similar to the dry film used to image the PWB for plating or etching. After being laminated to the PWB surface it is imaged and develop as LPI. Once common but no longer commonly used because of its low accuracy and resolution is to screen print epoxy ink. Solder resist also provides protection from the environment.Unpopulated boards are usually bare-board tested for \"shorts\" and \"opens\". A short is a connection between two points that should not be connected. An open is a missing connection between points that should be connected. For high-volume production a fixture or a rigid needle adapter is used to make contact with copper lands on the board. Building the adapter is a significant fixed cost and is only economical for high-volume or high-value production. For small or medium volume production flying probe testers are used where test probes are moved over the board by an XY drive to make contact with the copper lands. The CAM system instructs the electrical tester to apply a voltage to each contact point as required and to check that this voltage appears on the appropriate contact points and only on these.Often, through-hole and surface-mount construction must be combined in a single assembly because some required components are available only in surface-mount packages, while others are available only in through-hole packages. Another reason to use both methods is that through-hole mounting can provide needed strength for components likely to endure physical stress, while components that are expected to go untouched will take up less space using surface-mount techniques. For further comparison, see the SMT page.In boundary scan testing, test circuits integrated into various ICs on the board form temporary connections between the PCB traces to test that the ICs are mounted correctly. Boundary scan testing requires that all the ICs to be tested use a standard test configuration procedure, the most common one being the Joint Test Action Group (JTAG) standard. The JTAG test architecture provides a means to test interconnects between integrated circuits on a board without using physical test probes. JTAG tool vendors provide various types of stimulus and sophisticated algorithms, not only to detect the failing nets, but also to isolate the faults to specific nets, devices, and pins.PCBs intended for extreme environments often have a conformal coating, which is applied by dipping or spraying after the components have been soldered. The coat prevents corrosion and leakage currents or shorting due to condensation. The earliest conformal coats were wax; modern conformal coats are usually dips of dilute solutions of silicone rubber, polyurethane, acrylic, or epoxy. Another technique for applying a conformal coating is for plastic to be sputtered onto the PCB in a vacuum chamber. The chief disadvantage of conformal coatings is that servicing of the board is rendered extremely difficult.Many assembled PCBs are static sensitive, and therefore must be placed in antistatic bags during transport. When handling these boards, the user must be grounded (earthed). Improper handling techniques might transmit an accumulated static charge through the board, damaging or destroying components. Even bare boards are sometimes static sensitive. Traces have become so fine that it\\'s quite possible to blow an etch off the board (or change its characteristics) with a static charge. This is especially true on non-traditional PCBs such as MCMs and microwave PCBs.The first PCBs used through-hole technology, mounting electronic components by leads inserted through holes on one side of the board and soldered onto copper traces on the other side. Boards may be single-sided, with an unplated component side, or more compact double-sided boards, with components soldered on both sides. Horizontal installation of through-hole parts with two axial leads (such as resistors, capacitors, and diodes) is done by bending the leads 90 degrees in the same direction, inserting the part in the board (often bending leads located on the back of the board in opposite directions to improve the part\\'s mechanical strength), soldering the leads, and trimming off the ends. Leads may be soldered either manually or by a wave soldering machine.Through-hole manufacture adds to board cost by requiring many holes to be drilled accurately, and limits the available routing area for signal traces on layers immediately below the top layer on multi-layer boards since the holes must pass through all layers to the opposite side. Once surface-mounting came into use, small-sized SMD components were used where possible, with through-hole mounting only of components unsuitably large for surface-mounting due to power requirements or mechanical limitations, or subject to mechanical stress which might damage the PCB.Surface-mount technology emerged in the 1960s, gained momentum in the early 1980s and became widely used by the mid-1990s. Components were mechanically redesigned to have small metal tabs or end caps that could be soldered directly onto the PCB surface, instead of wire leads to pass through holes. Components became much smaller and component placement on both sides of the board became more common than with through-hole mounting, allowing much smaller PCB assemblies with much higher circuit densities. Surface mounting lends itself well to a high degree of automation, reducing labor costs and greatly increasing production rates. Components can be supplied mounted on carrier tapes. Surface mount components can be about one-quarter to one-tenth of the size and weight of through-hole components, and passive components much cheaper; prices of semiconductor surface mount devices (SMDs) are determined more by the chip itself than the package, with little price advantage over larger packages. Some wire-ended components, such as 1N4148 small-signal switch diodes, are actually significantly cheaper than SMD equivalents.Each trace consists of a flat, narrow part of the copper foil that remains after etching. The resistance, determined by width and thickness, of the traces must be sufficiently low for the current the conductor will carry. Power and ground traces may need to be wider than signal traces. In a multi-layer board one entire layer may be mostly solid copper to act as a ground plane for shielding and power return. For microwave circuits, transmission lines can be laid out in the form of stripline and microstrip with carefully controlled dimensions to assure a consistent impedance. In radio-frequency and fast switching circuits the inductance and capacitance of the printed circuit board conductors become significant circuit elements, usually undesired; but they can be used as a deliberate part of the circuit design, obviating the need for additional discrete components.The cloth or fiber material used, resin material, and the cloth to resin ratio determine the laminate\\'s type designation (FR-4, CEM-1, G-10, etc.) and therefore the characteristics of the laminate produced. Important characteristics are the level to which the laminate is fire retardant, the dielectric constant (er), the loss factor (tδ), the tensile strength, the shear strength, the glass transition temperature (Tg), and the Z-axis expansion coefficient (how much the thickness changes with temperature).There are quite a few different dielectrics that can be chosen to provide different insulating values depending on the requirements of the circuit. Some of these dielectrics are polytetrafluoroethylene (Teflon), FR-4, FR-1, CEM-1 or CEM-3. Well known pre-preg materials used in the PCB industry are FR-2 (phenolic cotton paper), FR-3 (cotton paper and epoxy), FR-4 (woven glass and epoxy), FR-5 (woven glass and epoxy), FR-6 (matte glass and polyester), G-10 (woven glass and epoxy), CEM-1 (cotton paper and epoxy), CEM-2 (cotton paper and epoxy), CEM-3 (non-woven glass and epoxy), CEM-4 (woven glass and epoxy), CEM-5 (woven glass and polyester). Thermal expansion is an important consideration especially with ball grid array (BGA) and naked die technologies, and glass fiber offers the best dimensional stability.The reinforcement type defines two major classes of materials - woven and non-woven. Woven reinforcements are cheaper, but the high dielectric constant of glass may not be favorable for many higher-frequency applications. The spatially nonhomogeneous structure also introduces local variations in electrical parameters, due to different resin/glass ratio at different areas of the weave pattern. Nonwoven reinforcements, or materials with low or no reinforcement, are more expensive but more suitable for some RF/analog applications.At the glass transition temperature the resin in the composite softens and significantly increases thermal expansion; exceeding Tg then exerts mechanical overload on the board components - e.g. the joints and the vias. Below Tg the thermal expansion of the resin roughly matches copper and glass, above it gets significantly higher. As the reinforcement and copper confine the board along the plane, virtually all volume expansion projects to the thickness and stresses the plated-through holes. Repeated soldering or other exposition to higher temperatures can cause failure of the plating, especially with thicker boards; thick boards therefore require high Tg matrix.Moisture absorption occurs when the material is exposed to high humidity or water. Both the resin and the reinforcement may absorb water; water may be also soaked by capillary forces through voids in the materials and along the reinforcement. Epoxies of the FR-4 materials aren\\'t too susceptible, with absorption of only 0.15%. Teflon has very low absorption of 0.01%. Polyimides and cyanate esters, on the other side, suffer from high water absorption. Absorbed water can lead to significant degradation of key parameters; it impairs tracking resistance, breakdown voltage, and dielectric parameters. Relative dielectric constant of water is about 73, compared to about 4 for common circuitboard materials. Absorbed moisture can also vaporize on heating and cause cracking and delamination, the same effect responsible for \"popcorning\" damage on wet packaging of electronic parts. Careful baking of the substrates may be required.The printed circuit board industry defines heavy copper as layers exceeding three ounces of copper, or approximately 0.0042 inches (4.2 mils, 105 μm) thick. PCB designers and fabricators often use heavy copper when design and manufacturing circuit boards in order to increase current-carrying capacity as well as resistance to thermal strains. Heavy copper plated vias transfer heat to external heat sinks. IPC 2152 is a standard for determining current-carrying capacity of printed circuit board traces.Since it was quite easy to stack interconnections (wires) inside the embedding matrix, the approach allowed designers to forget completely about the routing of wires (usually a time-consuming operation of PCB design): Anywhere the designer needs a connection, the machine will draw a wire in straight line from one location/pin to another. This led to very short design times (no complex algorithms to use even for high density designs) as well as reduced crosstalk (which is worse when wires run parallel to each other—which almost never happens in Multiwire), though the cost is too high to compete with cheaper PCB technologies when large quantities are needed.Cordwood construction can save significant space and was often used with wire-ended components in applications where space was at a premium (such as missile guidance and telemetry systems) and in high-speed computers, where short traces were important. In cordwood construction, axial-leaded components were mounted between two parallel planes. The components were either soldered together with jumper wire, or they were connected to other components by thin nickel ribbon welded at right angles onto the component leads. To avoid shorting together different interconnection layers, thin insulating cards were placed between them. Perforations or holes in the cards allowed component leads to project through to the next interconnection layer. One disadvantage of this system was that special nickel-leaded components had to be used to allow the interconnecting welds to be made. Differential thermal expansion of the component could put pressure on the leads of the components and the PCB traces and cause physical damage (as was seen in several modules on the Apollo program). Additionally, components located in the interior are difficult to replace. Some versions of cordwood construction used soldered single-sided PCBs as the interconnection method (as pictured), allowing the use of normal-leaded components.Development of the methods used in modern printed circuit boards started early in the 20th century. In 1903, a German inventor, Albert Hanson, described flat foil conductors laminated to an insulating board, in multiple layers. Thomas Edison experimented with chemical methods of plating conductors onto linen paper in 1904. Arthur Berry in 1913 patented a print-and-etch method in Britain, and in the United States Max Schoop obtained a patent to flame-spray metal onto a board through a patterned mask. Charles Ducas in 1927 patented a method of electroplating circuit patterns.The Austrian engineer Paul Eisler invented the printed circuit as part of a radio set while working in England around 1936. Around 1943 the USA began to use the technology on a large scale to make proximity fuses for use in World War II. After the war, in 1948, the USA released the invention for commercial use. Printed circuits did not become commonplace in consumer electronics until the mid-1950s, after the Auto-Sembly process was developed by the United States Army. At around the same time in Britain work along similar lines was carried out by Geoffrey Dummer, then at the RRDE.During World War II, the development of the anti-aircraft proximity fuse required an electronic circuit that could withstand being fired from a gun, and could be produced in quantity. The Centralab Division of Globe Union submitted a proposal which met the requirements: a ceramic plate would be screenprinted with metallic paint for conductors and carbon material for resistors, with ceramic disc capacitors and subminiature vacuum tubes soldered in place. The technique proved viable, and the resulting patent on the process, which was classified by the U.S. Army, was assigned to Globe Union. It was not until 1984 that the Institute of Electrical and Electronics Engineers (IEEE) awarded Mr. Harry W. Rubinstein, the former head of Globe Union\\'s Centralab Division, its coveted Cledo Brunetti Award for early key contributions to the development of printed components and conductors on a common insulating substrate. As well, Mr. Rubinstein was honored in 1984 by his alma mater, the University of Wisconsin-Madison, for his innovations in the technology of printed electronic circuits and the fabrication of capacitors.Originally, every electronic component had wire leads, and the PCB had holes drilled for each wire of each component. The components\\' leads were then passed through the holes and soldered to the PCB trace. This method of assembly is called through-hole construction. In 1949, Moe Abramson and Stanislaus F. Danko of the United States Army Signal Corps developed the Auto-Sembly process in which component leads were inserted into a copper foil interconnection pattern and dip soldered. The patent they obtained in 1956 was assigned to the U.S. Army. With the development of board lamination and etching techniques, this concept evolved into the standard printed circuit board fabrication process in use today. Soldering could be done automatically by passing the board over a ripple, or wave, of molten solder in a wave-soldering machine. However, the wires and holes are wasteful since drilling holes is expensive and the protruding wires are merely cut off.'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "016b6925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer_start': 0, 'text': 'Computational complexity theory'},\n",
       " {'answer_start': 0, 'text': 'Computational complexity theory'},\n",
       " {'answer_start': 0, 'text': 'Computational complexity theory'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.iloc[0].get('answers(list of dict)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e1da94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_df = pd.DataFrame(context_dict.items(), columns=[\"id\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "776351f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_df['Length'] = contexts_df[\"text\"].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ac8a19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.500000</td>\n",
       "      <td>24985.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.760952</td>\n",
       "      <td>10647.514902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10001.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.750000</td>\n",
       "      <td>18655.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.500000</td>\n",
       "      <td>22280.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.250000</td>\n",
       "      <td>27949.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>47293.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id        Length\n",
       "count  16.000000     16.000000\n",
       "mean    7.500000  24985.875000\n",
       "std     4.760952  10647.514902\n",
       "min     0.000000  10001.000000\n",
       "25%     3.750000  18655.750000\n",
       "50%     7.500000  22280.000000\n",
       "75%    11.250000  27949.500000\n",
       "max    15.000000  47293.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(contexts_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f2794ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Length', ylabel='Density'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sb\n",
    "sb.kdeplot(contexts_df[\"Length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3baf13da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we control what is stored in our sensory memory?\n",
      "what is another name for a high-surrogate code point? \n",
      "What are commonly used techniques during dynamic testing?\n",
      "What was the Macintosh II's new modular design similar to?\n",
      "How many iMac units sold in the first 139 days?\n",
      "What does not qualify as grey-box testing?\n",
      "What is used to recover data if disks suffer a failure?\n",
      "Why was dBASE unique?\n",
      "What are the three primary expressions used to represent case complexity?\n",
      "What programming language uses UTF-32 as internal representation of characters? \n",
      "Where was the Antikythera mechanism found in 1901?\n",
      "If polynomial time can be utilized within an NP-complete problem, what does the imply P is equal to?\n",
      "If changes need to occur during the softwares early release with regression testing how much of an impact does this have on the team as related to other testing? \n",
      "What is an important advantage of USB?\n",
      "What as a drawback of previous connectors?\n",
      "What is it called when a public test continues indefinitely?\n",
      "How long did the Mac Plus remain in production, unchanged?\n",
      "How are the packets routed \n",
      "In the encoding of mathematical objects, what is the way in which integers are commonly expressed?\n",
      "In what year was CTR created?\n",
      "Since what version can already existing characters no longer be added to the standard? \n",
      "Boolean logic consists of what?\n",
      "Disk encryption and Trusted Platform Module are designed to prevent what?\n",
      "What site was created to mitigate issues with incomplete copies of websites?\n",
      "SuperSpeed is only supported by?\n",
      "What is Donald Davies credited with \n",
      "Why were loose tolerances allowed for compliant USB connectors?\n",
      "How many types of charging ports exist?\n",
      "How istricking the user accomplished in clickjacking?\n",
      "What did Apple begin offering as standard features meant for playing DVD's?\n",
      "What year did Arthur Berry patent his print-and-etch method?\n",
      "What can skew due to hubs add?\n",
      "What machines were involved in the physical security breaches that played a large role in the incident?\n",
      "Calculi during the Fertile Crescent refers to what?\n",
      "Quick development of what kind of browsers led to non-standard HTML dialects?\n",
      "What does ASCII correspondence allow digital devices to do?\n",
      "What's the minimum amount of copper a layer in a PCB can have to be considered \"heavy copper\"?\n",
      "Complexity classes are generally classified into what?\n",
      "A computer that stores its program in memory and kept separate from the data is called what?\n",
      "Which company thought that Wayback Machine data was important for its argument?\n",
      "What was the name of the project to create a geographical database?\n",
      "What exactly does short-term memory allow a person to do?\n",
      "Why was Media Transfer Protocol designed?\n",
      "What was the estimated installed base of Mac computers in 2009?\n",
      "In the old PCB design method, what was used to connect component pin pads?\n",
      "What does a bi-directional endpoint accept?\n",
      "In what theoretical machine is it confirmed that a problem in P belies membership in the NP class?\n",
      "What year was Raskin hired by Apple?\n",
      "Who returned to Apple in 1997?\n",
      "Ag3Cu is one intermetallic that tin forms; what's the other one?\n"
     ]
    }
   ],
   "source": [
    "for x in result_df.sample(50)['question'] :\n",
    "    print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
