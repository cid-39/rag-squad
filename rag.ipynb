{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6c378b5",
   "metadata": {},
   "source": [
    "Data loading and preproccessing\n",
    "First of, we will laod the data and take the select few related to data science and computers, with one outlier being Memory article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6ea1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "target_articles = {}\n",
    "# Loading articles from dev\n",
    "with open('./data/stanford-question-answering-dataset/dev-v1.1.json', 'r', encoding='utf-8') as f:\n",
    "    squad_data_dev = json.load(f)\n",
    "for i, article in enumerate(squad_data_dev['data']):\n",
    "    if i==4 or i==19 :\n",
    "        target_articles[article['title']] = article['paragraphs']\n",
    "# Loading articles from train\n",
    "with open('./data/stanford-question-answering-dataset/train-v1.1.json', 'r', encoding='utf-8') as f2:\n",
    "    squad_data_train = json.load(f2)\n",
    "for i, article in enumerate(squad_data_train['data']):\n",
    "    if i in {30, 62, 104, 124, 149, 157, 177, 229, 293, 295, 330, 333, 389, 393} :\n",
    "        target_articles[article['title']] = article['paragraphs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf6f35d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: Computational_complexity_theory\n",
      "Article: Packet_switching\n",
      "Article: Wayback_Machine\n",
      "Article: Web_browser\n",
      "Article: Computer\n",
      "Article: Computer_security\n",
      "Article: ASCII\n",
      "Article: Macintosh\n",
      "Article: Memory\n",
      "Article: Data_compression\n",
      "Article: USB\n",
      "Article: Unicode\n",
      "Article: IBM\n",
      "Article: Software_testing\n",
      "Article: Database\n",
      "Article: Printed_circuit_board\n"
     ]
    }
   ],
   "source": [
    "for article_title, paragraphs in target_articles.items():\n",
    "    print(f\"\"\"Article: {article_title}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac180e30",
   "metadata": {},
   "source": [
    "After loading in all the articles, we can prepare text for chunking and questions for testing the retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d03618c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "docs_for_splitter = []\n",
    "questions_ground_truth = []\n",
    "p_idx=0\n",
    "for article_title, paragraphs in target_articles.items():\n",
    "    for _, para_data in enumerate(paragraphs):\n",
    "        context_text = para_data['context']\n",
    "        \n",
    "        # Creating documents with needed metadata\n",
    "        doc = Document(\n",
    "            page_content=context_text,\n",
    "            metadata={\n",
    "                \"para_id\": p_idx,\n",
    "                \"article\": article_title\n",
    "            }\n",
    "        )\n",
    "        docs_for_splitter.append(doc)\n",
    "        \n",
    "        # Extracting questions and linking them to doc that should be retrieved for them\n",
    "        for qa in para_data['qas']:\n",
    "            questions_ground_truth.append({\n",
    "                \"question\": qa['question'],\n",
    "                \"ground_truth_para_id\": p_idx,\n",
    "                # \"qa_id\": qa['id'],\n",
    "                # \"article\": article_title\n",
    "            })\n",
    "        p_idx+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7414e75",
   "metadata": {},
   "source": [
    "############################################################### <br>\n",
    "BELOW IS WORK IN PROGRESS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2006b",
   "metadata": {},
   "source": [
    "Splitting paragraphs into docs that will be embeded. Here I used the same tokenizer from the mixedbread mode with RecursiveCharacterTextSplitter, this way I hope to utilize the whole context window and minimize number of sentences that are split across chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a8ab8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mixedbread-ai/mxbai-embed-large-v1\")\n",
    "splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer,chunk_size=512, chunk_overlap=75)\n",
    "splits = splitter.split_documents(docs_for_splitter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f2fd9b",
   "metadata": {},
   "source": [
    "Embedding the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9d051ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64264e7a5e384e01be711c5d59cc2992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "embed = HuggingFaceEmbeddings(model_name=\"mixedbread-ai/mxbai-embed-large-v1\")\n",
    "vectorstore = Chroma.from_documents(documents=splits,embedding=embed)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a456905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What did Regis McKenna call the \"1984\" ad that was aired during the Super Bowl?\n",
      "Target paragraph id: 270\n",
      "[Document(metadata={'article': 'Macintosh', 'para_id': 270}, page_content='After the Lisa\\'s announcement, John Dvorak discussed rumors of a mysterious \"MacIntosh\" project at Apple in February 1983. The company announced the Macintosh 128K—manufactured at an Apple factory in Fremont, California—in October 1983, followed by an 18-page brochure included with various magazines in December. The Macintosh was introduced by a US$1.5 million Ridley Scott television commercial, \"1984\". It most notably aired during the third quarter of Super Bowl XVIII on January 22, 1984, and is now considered a \"watershed event\" and a \"masterpiece.\" Regis McKenna called the ad \"more successful than the Mac itself.\" \"1984\" used an unnamed heroine to represent the coming of the Macintosh (indicated by a Picasso-style picture of the computer on her white tank top) as a means of saving humanity from the \"conformity\" of IBM\\'s attempts to dominate the computer industry. The ad alludes to George Orwell\\'s novel, Nineteen Eighty-Four, which described a dystopian future ruled by a televised \"Big Brother.\"'), Document(metadata={'para_id': 262, 'article': 'Macintosh'}, page_content='In 1982, Regis McKenna was brought in to shape the marketing and launch of the Macintosh. Later the Regis McKenna team grew to include Jane Anderson, Katie Cadigan and Andy Cunningham, who eventually led the Apple account for the agency. Cunningham and Anderson were the primary authors of the Macintosh launch plan. The launch of the Macintosh pioneered many different tactics that are used today in launching technology products, including the \"multiple exclusive,\" event marketing (credited to John Sculley, who brought the concept over from Pepsi), creating a mystique around a product and giving an inside look into a product\\'s creation.'), Document(metadata={'article': 'Macintosh', 'para_id': 276}, page_content='Two days after \"1984\" aired, the Macintosh went on sale, and came bundled with two applications designed to show off its interface: MacWrite and MacPaint. It was first demonstrated by Steve Jobs in the first of his famous Mac keynote speeches, and though the Mac garnered an immediate, enthusiastic following, some labeled it a mere \"toy.\" Because the operating system was designed largely around the GUI, existing text-mode and command-driven applications had to be redesigned and the programming code rewritten. This was a time-consuming task that many software developers chose not to undertake, and could be regarded as a reason for an initial lack of software for the new system. In April 1984, Microsoft\\'s MultiPlan migrated over from MS-DOS, with Microsoft Word following in January 1985. In 1985, Lotus Software introduced Lotus Jazz for the Macintosh platform after the success of Lotus 1-2-3 for the IBM PC, although it was largely a flop. Apple introduced the Macintosh Office suite the same year with the \"Lemmings\" ad. Infamous for insulting its own potential customers, the ad was not successful.'), Document(metadata={'article': 'Macintosh', 'para_id': 254}, page_content='Apple spent $2.5 million purchasing all 39 advertising pages in a special, post-election issue of Newsweek, and ran a \"Test Drive a Macintosh\" promotion, in which potential buyers with a credit card could take home a Macintosh for 24 hours and return it to a dealer afterwards. While 200,000 people participated, dealers disliked the promotion, the supply of computers was insufficient for demand, and many were returned in such a bad condition that they could no longer be sold. This marketing campaign caused CEO John Sculley to raise the price from US$1,995 to US$2,495 (about $5,200 when adjusted for inflation in 2010). The computer sold well, nonetheless, reportedly outselling the IBM PCjr which also began shipping early that year. By April 1984 the company sold 50,000 Macintoshes, and hoped for 70,000 by early May and almost 250,000 by the end of the year.'), Document(metadata={'para_id': 73, 'article': 'Wayback_Machine'}, page_content='The name Wayback Machine was chosen as a droll reference to a plot device in an animated cartoon series, The Rocky and Bullwinkle Show. In one of the animated cartoon\\'s component segments, Peabody\\'s Improbable History, lead characters Mr. Peabody and Sherman routinely used a time machine called the \"WABAC machine\" (pronounced way-back) to witness, participate in, and, more often than not, alter famous events in history.'), Document(metadata={'para_id': 271, 'article': 'Macintosh'}, page_content='The Macintosh (/ˈmækᵻntɒʃ/ MAK-in-tosh; branded as Mac since 1997) is a series of personal computers (PCs) designed, developed, and marketed by Apple Inc. Steve Jobs introduced the original Macintosh computer on January 24, 1984. This was the first mass-market personal computer featuring an integral graphical user interface and mouse. This first model was later renamed to \"Macintosh 128k\" for uniqueness amongst a populous family of subsequently updated models which are also based on Apple\\'s same proprietary architecture. Since 1998, Apple has largely phased out the Macintosh name in favor of \"Mac\", though the product family has been nicknamed \"Mac\" or \"the Mac\" since the development of the first model.'), Document(metadata={'para_id': 464, 'article': 'IBM'}, page_content='Thomas J. Watson, Sr., fired from the National Cash Register Company by John Henry Patterson, called on Flint and, in 1914, was offered CTR. Watson joined CTR as General Manager then, 11 months later, was made President when court cases relating to his time at NCR were resolved. Having learned Patterson\\'s pioneering business practices, Watson proceeded to put the stamp of NCR onto CTR\\'s companies. He implemented sales conventions, \"generous sales incentives, a focus on customer service, an insistence on well-groomed, dark-suited salesmen and had an evangelical fervor for instilling company pride and loyalty in every worker\". His favorite slogan, \"THINK\", became a mantra for each company\\'s employees. During Watson\\'s first four years, revenues more than doubled to $9 million and the company\\'s operations expanded to Europe, South America, Asia and Australia. \"Watson had never liked the clumsy hyphenated title of the CTR\" and chose to replace it with the more expansive title \"International Business Machines\". First as a name for a 1917 Canadian subsidiary, then as a line in advertisements. For example, the McClures magazine, v53, May 1921, has a full page ad with, at the bottom:'), Document(metadata={'para_id': 278, 'article': 'Macintosh'}, page_content=\"The Macintosh's minimal memory became apparent, even compared with other personal computers in 1984, and could not be expanded easily. It also lacked a hard disk drive or the means to easily attach one. Many small companies sprang up to address the memory issue. Suggestions revolved around either upgrading the memory to 512 KB or removing the computer's 16 memory chips and replacing them with larger-capacity chips, a tedious and difficult operation. In October 1984, Apple introduced the Macintosh 512K, with quadruple the memory of the original, at a price of US$3,195. It also offered an upgrade for 128k Macs that involved replacing the logic board.\"), Document(metadata={'para_id': 250, 'article': 'ASCII'}, page_content='Code 127 is officially named \"delete\" but the Teletype label was \"rubout\". Since the original standard did not give detailed interpretation for most control codes, interpretations of this code varied. The original Teletype meaning, and the intent of the standard, was to make it an ignored character, the same as NUL (all zeroes). This was useful specifically for paper tape, because punching the all-ones bit pattern on top of an existing mark would obliterate it. Tapes designed to be \"hand edited\" could even be produced with spaces of extra NULs (blank tape) so that a block of characters could be \"rubbed out\" and then replacements put into the empty space.'), Document(metadata={'para_id': 247, 'article': 'ASCII'}, page_content='Probably the most influential single device on the interpretation of these characters was the Teletype Model 33 ASR, which was a printing terminal with an available paper tape reader/punch option. Paper tape was a very popular medium for long-term program storage until the 1980s, less costly and in some ways less fragile than magnetic tape. In particular, the Teletype Model 33 machine assignments for codes 17 (Control-Q, DC1, also known as XON), 19 (Control-S, DC3, also known as XOFF), and 127 (Delete) became de facto standards. The Model 33 was also notable for taking the description of Control-G (BEL, meaning audibly alert the operator) literally as the unit contained an actual bell which it rang when it received a BEL character. Because the keytop for the O key also showed a left-arrow symbol (from ASCII-1963, which had this character instead of underscore), a noncompliant use of code 15 (Control-O, Shift In) interpreted as \"delete previous character\" was also adopted by many early timesharing systems but eventually became neglected.')]\n"
     ]
    }
   ],
   "source": [
    "id = 1003\n",
    "print(f\"Question: {questions_ground_truth[id]['question']}\")\n",
    "print(f\"Target paragraph id: {questions_ground_truth[id]['ground_truth_para_id']}\")\n",
    "\n",
    "print(retriever.invoke(questions_ground_truth[id]['question']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9b35e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8937fe28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on 2265 questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2265/2265 [08:24<00:00,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Retrieval Performance ---\n",
      "Hit Rate@1: 73.38% (1662/2265)\n",
      "Hit Rate@5: 91.96% (2083/2265)\n",
      "Hit Rate@7: 93.77% (2124/2265)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm # For a nice progress bar\n",
    "\n",
    "def evaluate_retrieval(questions, retriever, k_values=[1, 5, 7]):\n",
    "    hits = {k: 0 for k in k_values}\n",
    "    total_questions = len(questions)\n",
    "\n",
    "    print(f\"Starting evaluation on {total_questions} questions...\")\n",
    "\n",
    "    for q_data in tqdm(questions):\n",
    "        question = q_data['question']\n",
    "        target_id = q_data['ground_truth_para_id']\n",
    "        \n",
    "        retrieved_docs = retriever.invoke(question) \n",
    "        \n",
    "        retrieved_ids = [doc.metadata.get('para_id') for doc in retrieved_docs]\n",
    "\n",
    "        for k in k_values:\n",
    "            # Looking only at the top k retrieved IDs\n",
    "            if target_id in retrieved_ids[:k]:\n",
    "                hits[k] += 1\n",
    "\n",
    "    print(\"\\n--- Retrieval Performance ---\")\n",
    "    for k in k_values:\n",
    "        accuracy = (hits[k] / total_questions) * 100\n",
    "        print(f\"Hit Rate@{k}: {accuracy:.2f}% ({hits[k]}/{total_questions})\")\n",
    "\n",
    "evaluate_retrieval(questions_ground_truth, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f678bbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on 2265 questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2265/2265 [08:59<00:00,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Retrieval Performance ---\n",
      "Hit Rate@1: 71.48% (1619/2265)\n",
      "Hit Rate@5: 90.99% (2061/2265)\n",
      "Hit Rate@7: 93.07% (2108/2265)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_retrieval2(questions, retriever, k_values=[1, 5, 7]):\n",
    "    hits = {k: 0 for k in k_values}\n",
    "    total_questions = len(questions)\n",
    "\n",
    "    print(f\"Starting evaluation on {total_questions} questions...\")\n",
    "\n",
    "    for q_data in tqdm(questions):\n",
    "        question = q_data['question']\n",
    "        target_id = q_data['ground_truth_para_id']\n",
    "        \n",
    "        retrieved_docs = retriever.invoke(f\"\"\"Represent this sentence for searching relevant passages: {question}\"\"\") \n",
    "        \n",
    "        retrieved_ids = [doc.metadata.get('para_id') for doc in retrieved_docs]\n",
    "\n",
    "        for k in k_values:\n",
    "            # Looking only at the top k retrieved IDs\n",
    "            if target_id in retrieved_ids[:k]:\n",
    "                hits[k] += 1\n",
    "\n",
    "    print(\"\\n--- Retrieval Performance ---\")\n",
    "    for k in k_values:\n",
    "        accuracy = (hits[k] / total_questions) * 100\n",
    "        print(f\"Hit Rate@{k}: {accuracy:.2f}% ({hits[k]}/{total_questions})\")\n",
    "\n",
    "evaluate_retrieval2(questions_ground_truth, retriever)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
