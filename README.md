# rag-squad
RAG system built and tested for select few articles about computer science and similar topics. Wikipedia articles and questions used come from Stanford Question Answering Dataset (v1.0).<br>
_Note: Even though the project is named rag-squad, the focus here is mostly on retrieval_
# Introduction
RAG (short for Retrieval Augmented Generation) is a system ment to help LLMs be useful long after they were trained. Another use case for RAG is [opis primene lokalnog chatbota kompanije]. The general idea is that we use the LLM as the "brain" and we feed it information when needed to answer specific questions with information the model wasn't trained on. This is possible by embedding the source text and keeping it in a database (vector store). When a question is asked, it is embedded with the same model and the vector store is searched for most similar entries. This type of RAG is called naive as we hope that question and text will end up with similar embeddings. <br>
The idea behind this project is building a simple RAG system that can run locally on almost any modern computer. The models we use for embedding will have around 300 million parameters. We will look at two models and how they perform, the models are Mixedbread's embedding model (v1 large) and Google's gemma embedding model. After finding the best performing setting between these two model's, we will see how a reranker will aid in raising the metrics. Reranker is a cross-encoder, this type of encoder takes two inputs in the same time embedding them and looking at their contextual similarities. This reranker will look at the documents we get from the simple RAG query (the users question) and rerank them so the most similar document (or paragraph as well later see) is ranked highest.
# Notebook 0
Stanford's question answering dataset is the bedrock of this project, this dataset gives us paragraphs of text and questions regarding information from the paragraph itself, originally it was made for testing LLM's ability to comprehend text and answer questions showing the understanding of the said text. Here we'll use it as for each paragraph of an article, we have questions we can use to check how good our system is working. This shortens the work needed to start testing, we have text and questions that are already mapped to the source of ground truth answers. This let's us simply check if the id of the question is matching of the id of the paragraph we were given from RAG. <br>
Regarding the metrics, we will use only hit rates at 1, 3, 5, 7 and 10 and mean reciprocal rank (MRR) as there is always one and only one paragraph we need to retrieve.<br> 
In the start, I was looking through the dataset. First step was navigating through the json itself, unraveling it and looking through the titles of Wikipedia articles that will be used as the basis of the testing. Original idea was building an AI assistant for a computer science student, because of this we will pick 15 articles regarding CS and one article about human memmory (hoping to maybe confuse the retriever).<br>
At the bottom of the notebook there was a graph plotted to see the lenght of paragraphs of text. This was important to check because of the models we will use to embed the text. These models have their context window, if we are over this window, the text will be truncated and information will be lost. Major part of paragraphs are under 1500 characters, meaning when they are tokenized (where 1 token in english is around 4 characters) they will almost never be over the limit which in case of 2 models used is 512 tokens. This eases the work, but leaves us unable to test one extremely important part of RAG systems - chunking. For real world scenarios where text is not already split into paragraphs, we would need to break it into chunks to fit into the context window, this is important because when splitting the text sometimes we can lose the context of sentences between two chunks. There is lots of ways to chunk text, most notable are: Fixed Size Chunking, Recursive Chunking, Document Based Chunking, Semantic Chunking and Agentic Chunking.
# Notebook 1
This notebook is quite short and doesnt need much explanation. Here we are reading the json files, taking the paragraphs from the articles we need, reading the questions and mapping them for later use. Each paragraph is wrapped inside a Document object we use to save the metadata about the text, in bigger systems this metadata would carry important data about text, but here it will only carry the ids we used to map questions. The list of documents and questions are saved in a pickle file which will be used in next notebooks, for purpose of reading the files there is utils "library" which holds couple important functions, one of them used for reading the data we saved in this notebook.
# Notebook 2
In notebook 2, we finally get to actual retrieval. This is main component of a RAG system. <br>
First step is loading the data from the pickle file, then we get to chunking, embedding and saving the data. For chunking we'll use RecursiveCharacterTextSplitter from langchain's library, this text splitter recursively splitts text into chunks at specified separators, such as comma, period or new line. After splitting the text it start fitting them into chunks, adding part by part while under the defined chunk size (in rare cases he will leave a chunk oversize if no other option is left). This type of splitter is relatively fast but efficient, as in most cases the sentances and text will be split into coherent segments. Chunk overlap is an important setting that overcomes the problem of splitting in a middle of a sentance or explanation of ideas by having the last (75 tokens in this case) present in both resulting chunks. This is an extremely important setting in realistic systems as overlap is better for accuracy (less contextual loss) but worse for performance as there is more redundant data in the database. <br>
Tokenizier is also an important part, it's best to use the same tokenizer as the model we use for embedding, this way the splitter will break tokenized text into chunks making sure our chunks use the context window efficiently, if we were to just use characters, we would get chunks that are _around_ the context window, for example if on avrage tokens from english are 4 characters, we'd give chunk_size of 2000, with that some resulting chunks passed into the embedding model would be tokenized into segments that are sometimes over 512 tokens (losing information from truncated text) and sometimes under 512 tokens (not using context window and being slightly less efficient with storage). <br>
In this case, after passing the documents through the splitter, we only split one document, so chunking is an extremely important part of RAG that won't be considered or tested in this project.<br>
Next, we will do the embedding and storage, we will use Chroma database from langchains library, this enables us to just pass the data and the model into Chroma object and the library takes care of the rest. When this is done, we'll have SQLite database in which entries will consist of a vector and a corresponding document. With that, when we pass a question (a query), the question is converted into a vector and the database is searched for closest vectors according to some type of similarity (usually dot product, L2 or cosine similarity); the result of the querry are documents that correspond to those vectors. <br>
After making the vector store all that is left is to test the performance, in utils.py we have a function that will go through given questions and for each retrieve 10 documents that are most similar, metrics are calculated based on those docs.<br>
We already mentioned which metrics will be used. Hit rates are quite simple, if the paragraph that contains the answer to the given question is withing first k documents, there is a hit. For example if hit rate at k=5 is 92%, that means in 92% of questions we got the right paragraph in first five documents. <br>
MRR is calculated as 1 devided by the rank of the document we need. With that in mind the best case scenario is to have MRR=1 which is equivalent of having hit rate at 1 100%. This metrics is useful as it shows how close the document we need is to the top. <br>
Finally, looking at the metrics, they are extremely high for something so simple. Explenation for this is simple, with such a small set of document to look through, it's easy to not make mistakes. This means that if we used the whole squad dataset the metrics would drop by large.
# Notebook 3
In notebook 2 we implemented our first version of a retrieval system. We used mixedbread's model with chroma database. Important thing that wasn't noted were vectors and the similarity score that was used. Vectors that we got from the model were not normalized, meaning they were all different lengths; the similarity score that was used was L2 by default, meaning we measured how far two points in the n-dimensional space are apart as a similarity for meanings of text.<br>
Here, we will run the same model, with same tokenization and everything the same, but we will change the similarity score. First of all, we will tell the model to normalize vectors and chroma to use cosine similarity for the "closeness" metric. Cosine similarity just means that instead of looking at euclidian distance as a measure, we are looking at the angle between the two vectors.<br>
Note: Since we use normalized vectors we dont have to use cosine similarity as the results would be the same.<br>
With this change, we se marginally better results (1% change at best).
# Notebook 4
We've seen the performance of Mixedbread's model, now let's take a look at another model with similar amount of parameters.<br>
The model that will be in focus is Google's Gemma, after the model a reranker will be introduced. Regarding the gemma, there isn't much to be said, we'll use cosine similarity as default here. The metrics of bare gemma are already much better than mixedbread's model, important to add, the speed of execution for embedding the documents and evaluation is much faster with google's model. <br>
Next important improvement on rag systems are rerankers. We've explained what it does, but lets look at why it's idea came to be and why its so useful. When we retrieve 1 document, we might miss it, soo we would just retrieve more documents, three, five and more. By the metrics themsleves, we see that hitrates at 10 are extremely high, this is where our reranker comes into play, we cant just give 10 (or even more) documents to the LLM that will actually answer the questions, but instead of making perfect embedding model, we can just pull 5/10/20 document from our database and have another model (the reranker) go through those documents and compare them with the question to see the most similar one. This is extremely effiecient when looking at processing power needed to achieve same effects with only the retrieval according to the primary model's embeddings. Per instance, the cross encoder is slower, but it has much larger capabilities of understaning semantical relationships of words in text. Point is, an "okay" model can be used for embedding and initial retrieval, this one should be fast and precise enough, after that we use a better (and slower) model to go through the smaller amount of top results.<br>
The model that will be used for reranking here is a cross-encoder made for MS Marco (mini v2). This model was chosen at it is extremely small with 22.7 million parameters which fits with the idea of locally run RAG system.
Results of retrieval with reranker are much better, around 10% increase in top 1 rate is very important, but more important to note, without a reranker the spread between top 1 and top 10 is 18.63%, with a reranker is tightened to 7.73%; this shows us that the reranker was able to reduce it by more than 10% but also means that a better or ideal reranker would be able to raise top 1 metric even higher.
# Notebook 5

# Notebook 6

# Notebook 7