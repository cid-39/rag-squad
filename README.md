# rag-squad
RAG system built and tested for select few articles about computer science and similar topics. Wikipedia articles and questions used come from Stanford Question Answering Dataset (v1.0).
# Introduction
RAG (short for Retrieval Augmented Generation) is a system ment to help LLMs be useful long after they were trained. Another use case for RAG is [opis primene lokalnog chatbota kompanije]. The general idea is that we use the LLM as the "brain" and we feed it information when needed to answer specific questions with information the model wasn't trained on. This is possible by embedding the source text and keeping it in a database (vector store). When a question is asked, it is embedded with the same model and the vector store is searched for most similar entries. This type of RAG is called naive as we hope that question and text will end up with similar embeddings. <br>
The idea behind this project is building a simple RAG system that can run locally on almost any modern computer. The models we use for embedding will have around 300 million parameters. We will look at two models and how they perform, the models are Mixedbread's embedding model (v1 large) and Google's gemma embedding model. After finding the best performing setting between these two model's, we will see how a reranker will aid in raising the metrics. Reranker is a cross-encoder, this type of encoder takes two inputs in the same time embedding them and looking at their contextual similarities. This reranker will look at the documents we get from the simple RAG query (the users question) and rerank them so the most similar document (or paragraph as well later see) is ranked highest.
# Notebook 0
Stanford's question answering dataset is the bedrock of this project, this dataset gives us paragraphs of text and questions regarding information from the paragraph itself, originally it was made for testing LLM's ability to comprehend text and answer questions showing the understanding of the said text. Here we'll use it as for each paragraph of an article, we have questions we can use to check how good our system is working. This shortens the work needed to start testing, we have text and questions that are already mapped to the source of ground truth answers. This let's us simply check if the id of the question is matching of the id of the paragraph we were given from RAG. <br>
Regarding the metrics, we will use only hit rates at 1, 3, 5, 7 and 10 and mean reciprocal rank (MRR) as there is always one and only one paragraph we need to retrieve.<br> 
In the start, I was looking through the dataset. First step was navigating through the json itself, unraveling it and looking through the titles of Wikipedia articles that will be used as the basis of the testing. Original idea was building an AI assistant for a computer science student, because of this we will pick 15 articles regarding CS and one article about human memmory (hoping to maybe confuse the retriever).<br>
At the bottom of the notebook there was a graph plotted to see the lenght of paragraphs of text. This was important to check because of the models we will use to embed the text. These models have their context window, if we are over this window, the text will be truncated and information will be lost. Major part of paragraphs are under 1500 characters, meaning when they are tokenized (where 1 token in english is around 4 characters) they will almost never be over the limit which in case of 2 models used is 512 tokens. This eases the work, but leaves us unable to test one extremely important part of RAG systems - chunking. For real world scenarios where text is not already split into paragraphs, we would need to break it into chunks to fit into the context window, this is important because when splitting the text sometimes we can lose the context of sentences between two chunks. There is lots of ways to chunk text, most notable are: Fixed Size Chunking, Recursive Chunking, Document Based Chunking, Semantic Chunking and Agentic Chunking.
# Notebook 1
This notebook is quite short and doesnt need much explanation. Here we are reading the json files, taking the paragraphs from the articles we need, reading the questions and mapping them for later use. Each paragraph is wrapped inside a Document object we use to save the metadata about the text, in bigger sistems this metadata would carry important data about text, but here it will only carry the ids we used to map questions. The list of documents and questions are saved in a pickle file which will be used in next notebooks, for purpose of reading the files there is utils "library" which holds couple important functions, one of them used for reading the data we saved in this notebook.
# Notebook 2

# Notebook 3

# Notebook 4

# Notebook 5

# Notebook 6

# Notebook 7