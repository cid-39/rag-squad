# rag-squad
RAG system built and tested for select few articles about computer science and similar topics. Wikipedia articles and questions used come from Stanford Question Answering Dataset (v1.0).<br><br>
_Note: Even though the project is named rag-squad, the focus here is mostly on retrieval._

## Table of Contents
* [Introduction](#introduction)
* [Notebook 0: Data Exploration & Analysis](#notebook-0)
* [Notebook 1: Data Preprocessing](#notebook-1)
* [Notebook 2: Naive Retrieval Implementation](#notebook-2)
* [Notebook 3: Optimizing Similarity Metrics](#notebook-3)
* [Notebook 4: Gemma & The Power of Reranking](#notebook-4)
* [Notebook 5: Creating an Evaluation Subset](#notebook-5)
* [Notebook 6: Making Some Noise](#notebook-6)
* [Notebook 7: Local Generation with Phi-3](#notebook-7)
* [Conclusion](#conclusion)

## Introduction
RAG (short for Retrieval Augmented Generation) is a system meant to help LLMs stay useful long after they were trained. Another use case for RAG is applying it in businesses, where you'd want to use the power of modern LLM, but need to give it companies information that isn't meant for public use. The general idea is that we use the LLM as the "brain" and we feed it information when needed to answer specific questions with information the model wasn't trained on. This is possible by embedding the source text and keeping it in a database (vector store). When a question is asked, it is embedded with the same model and the vector store is searched for most similar entries. This type of RAG is called naive as we hope that question and text will end up with similar embeddings. <br><br>
The idea behind this project is building a simple RAG system that can run locally on almost any modern computer. The models we use for embedding will have around 300 million parameters. We will look at two models and how they perform, the models are Mixedbread's embedding model (v1 large) and Google's gemma embedding model. After finding the best performing setting between these two model's, we will see how a reranker will aid in raising the metrics. Reranker is a cross-encoder, this type of encoder takes two inputs in the same time embedding them and looking at their contextual similarities. This reranker will look at the documents we get from the simple RAG query (the users question) and rerank them so the most similar document (or paragraph as well later see) is ranked highest.
## Notebook 0
Stanford's question answering dataset is the bedrock of this project, this dataset gives us paragraphs of text and questions regarding information from the paragraph itself, originally it was made for testing LLM's ability to comprehend text and answer questions showing the understanding of the said text. Here we'll use it as for each paragraph of an article, we have questions we can use to check how good our system is working. This shortens the work needed to start testing, we have text and questions that are already mapped to the source of ground truth answers. This let's us simply check if the id of the question is matching of the id of the paragraph we were given from RAG. <br><br>
Regarding the metrics, we will use only hit rates at 1, 3, 5, 7 and 10 and mean reciprocal rank (MRR) as there is always one and only one paragraph we need to retrieve.<br><br> 
In the start, I was looking through the dataset. First step was navigating through the json itself, unraveling it and looking through the titles of Wikipedia articles that will be used as the basis of the testing. Original idea was building an AI assistant for a computer science student, because of this we will pick 15 articles regarding CS and one article about human memory (hoping to maybe confuse the retriever).<br><br>
At the bottom of the notebook there was a graph plotted to see the length of paragraphs of text. This was important to check because of the models we will use to embed the text. These models have their context window, if we are over this window, the text will be truncated and information will be lost. Major part of paragraphs are under 1500 characters, meaning when they are tokenized (where 1 token in english is around 4 characters) they will almost never be over the limit which in case of 2 models used is 512 tokens. This eases the work, but leaves us unable to test one extremely important part of RAG systems - chunking. For real world scenarios where text is not already split into paragraphs, we would need to break it into chunks to fit into the context window, this is important because when splitting the text sometimes we can lose the context of sentences between two chunks. There is lots of ways to chunk text, most notable are: Fixed Size Chunking, Recursive Chunking, Document Based Chunking, Semantic Chunking and Agentic Chunking.
## Notebook 1
This notebook is quite short and doesnt need much explanation. Here we are reading the json files, taking the paragraphs from the articles we need, reading the questions and mapping them for later use. Each paragraph is wrapped inside a Document object we use to save the metadata about the text, in bigger systems this metadata would carry important data about text, but here it will only carry the ids we used to map questions. The list of documents and questions are saved in a pickle file which will be used in next notebooks, for purpose of reading the files there is utils "library" which holds couple important functions, one of them used for reading the data we saved in this notebook.
## Notebook 2
In notebook 2, we finally get to actual retrieval. This is main component of a RAG system. <br><br>
First step is loading the data from the pickle file, then we get to chunking, embedding and saving the data. For chunking we'll use RecursiveCharacterTextSplitter from langchain's library, this text splitter recursively splitts text into chunks at specified separators, such as comma, period or new line. After splitting the text it start fitting them into chunks, adding part by part while under the defined chunk size (in rare cases he will leave a chunk oversize if no other option is left). This type of splitter is relatively fast but efficient, as in most cases the sentences and text will be split into coherent segments. Chunk overlap is an important setting that overcomes the problem of splitting in a middle of a sentence or explanation of ideas by having the last (75 tokens in this case) present in both resulting chunks. This is an extremely important setting in realistic systems as overlap is better for accuracy (less contextual loss) but worse for performance as there is more redundant data in the database. <br><br>
Tokenizier is also an important part, it's best to use the same tokenizer as the model we use for embedding, this way the splitter will break tokenized text into chunks making sure our chunks use the context window efficiently, if we were to just use characters, we would get chunks that are _around_ the context window, for example if on avrage tokens from english are 4 characters, we'd give chunk_size of 2000, with that some resulting chunks passed into the embedding model would be tokenized into segments that are sometimes over 512 tokens (losing information from truncated text) and sometimes under 512 tokens (not using context window and being slightly less efficient with storage). <br><br>
In this case, after passing the documents through the splitter, we only split one document, so chunking is an extremely important part of RAG that won't be considered or tested in this project.<br><br>
Next, we will do the embedding and storage, we will use Chroma database from langchains library, this enables us to just pass the data and the model into Chroma object and the library takes care of the rest. When this is done, we'll have SQLite database in which entries will consist of a vector and a corresponding document. With that, when we pass a question (a query), the question is converted into a vector and the database is searched for closest vectors according to some type of similarity (usually dot product, L2 or cosine similarity); the result of the query are documents that correspond to those vectors. <br><br>
After making the vector store all that is left is to test the performance, in utils.py we have a function that will go through given questions and for each retrieve 10 documents that are most similar, metrics are calculated based on those docs.<br><br>
We already mentioned which metrics will be used. Hit rates are quite simple, if the paragraph that contains the answer to the given question is withing first k documents, there is a hit. For example if hit rate at k=5 is 92%, that means in 92% of questions we got the right paragraph in first five documents. <br><br>
MRR is calculated as 1 divided by the rank of the document we need. With that in mind the best case scenario is to have MRR=1 which is equivalent of having hit rate at 1 100%. This metrics is useful as it shows how close the document we need is to the top. <br><br>
Finally, looking at the metrics, they are extremely high for something so simple. Explanation for this is simple, with such a small set of document to look through, it's easy to not make mistakes. This means that if we used the whole squad dataset the metrics would drop by large.<br><br>
_Note: Testing this idea of having more data to drop metrics was analyzed in notebook 6._
## Notebook 3
In notebook 2 we implemented our first version of a retrieval system. We used mixedbread's model with chroma database. Important thing that wasn't noted were vectors and the similarity score that was used. Vectors that we got from the model were not normalized, meaning they were all different lengths; the similarity score that was used was L2 by default, meaning we measured how far two points in the n-dimensional space are apart as a similarity for meanings of text.<br><br>
Here, we will run the same model, with same tokenization and everything the same, but we will change the similarity score. First of all, we will tell the model to normalize vectors and chroma to use cosine similarity for the "closeness" metric. Cosine similarity just means that instead of looking at euclidian distance as a measure, we are looking at the angle between the two vectors.<br><br>
Note: Since we use normalized vectors we dont have to use cosine similarity as the results would be the same.<br><br>
With this change, we se marginally better results (1% change at best).
## Notebook 4
We've seen the performance of Mixedbread's model, now let's take a look at another model with similar amount of parameters.<br><br>
The model that will be in focus is Google's Gemma, after the model a reranker will be introduced. Regarding the gemma, there isn't much to be said, we'll use cosine similarity as default here. The metrics of bare gemma are already much better than mixedbread's model, important to add, the speed of execution for embedding the documents and evaluation is much faster with google's model. <br><br>
Next important improvement on rag systems are rerankers. We've explained what it does, but lets look at why it's idea came to be and why its so useful. When we retrieve 1 document, we might miss it, soo we would just retrieve more documents, three, five and more. By the metrics themsleves, we see that hitrates at 10 are extremely high, this is where our reranker comes into play, we cant just give 10 (or even more) documents to the LLM that will actually answer the questions, but instead of making perfect embedding model, we can just pull 5/10/20 document from our database and have another model (the reranker) go through those documents and compare them with the question to see the most similar one. This is extremely effiecient when looking at processing power needed to achieve same effects with only the retrieval according to the primary model's embeddings. Per instance, the cross encoder is slower, but it has much larger capabilities of understaning semantic relationships of words in text. Point is, an "okay" model can be used for embedding and initial retrieval, this one should be fast and precise enough, after that we use a better (and slower) model to go through the smaller amount of top results.<br><br>
The model that will be used for reranking here is a cross-encoder made for MS Marco (mini v2). This model was chosen at it is extremely small with 22.7 million parameters which fits with the idea of locally run RAG system.
Results of retrieval with reranker are much better, around 10% increase in top 1 rate is very important, but more important to note, without a reranker the spread between top 1 and top 10 is 18.63%, with a reranker is tightened to 7.73%; this shows us that the reranker was able to reduce it by more than 10% but also means that a better or ideal reranker would be able to raise top 1 metric even higher.
## Notebook 5
Notebook 5 is not quite important for the project but should be mentioned. Until now, for testing the metrics of models, all 2265 questions were used. In this notebook, the question set for evaluation is reduced to 350 questions to enable fast testing as when playing around with different setting it could take up to 15 minutes to run evaluation with a reranker in place. The procedure was simple, we took the code from the evaluation function and added code to sort questions into two lists, one with questions that there could not be answered (the corresponding paragraph was not retrieved in first 10 documents) and one with questions that were answered. With those two, using random sampling, we made a smaller set of 350 questions while trying to keep the ratio between answered and unanswered questions the same as with the whole set. After that a quick test was done to see that metrics stayed the same after which the questions were stored in another pickle file. <br><br>
Note that hit rates at 5, 7 and 10 have same values, simply explained, from all 2265 questions, only about 8 of them had their answers retrieved between 5th and 10th document, so when picking 350 from 2265, its unlikely that we would pick one of those eight.
## Notebook 6
After seeing results of a model with and without a reranker, last thing that should be addressed is how high the metrics are. Previously mentioned in notebook 2, in realistic systems, RAG would work with 100 or more times more data, some of that data (depending on the usecase) could have similar semantic meaning which would place those chunks close to targeted paragraph of text. With more and more data, retrieval system would make more mistakes. For this reason, in this notebook, we introduce more data with similar concepts and see how much would the metrics drop.<br><br>
The data that we will introduce will come from Wikipedia dumps and articles will be chosen with the idea to be similar to the existing ones, for example: <br>
- Network security -> Computer security
- Flash memory, Random-access memory -> Memory
- Operating_system -> Macintosh

After loading and chunking 19 new articles we are left with 369 chunks of noise. While there is a possibility that a question may have the answer in this new data, this case will be ignored and all new text will be treated purely as noise. <br>
When combining with old data, we get 950 chunks of which about 40% is noise. After evaluation with and without a reranker we see that noise does make a difference as shown in the table.

|         Model| MRR|    HitRate@1        | HitRate@3| HitRate@5|HitRate7|HitRate@10|
|-----------------------------|-----------|--------|--------|--------|--------|-------|
| gemma                       | 0.8527 | 78.15 | 91.39 | 94.66 | 95.85 | 96.78 |
| gemma-noise                 | 0.8088 | 72.29 | 87.71 | 91.71 | 94.57 | 95.71 |
| gemma-rerank                | 0.9226 | 89.05 | 95.32 | 96.42 | 96.64 | 96.78 |
| gemma-rerank-noise          | 0.9060 | 86.57 | 95.14 | 95.71 | 95.71 | 95.71 |

A drop of 5% in HitRate@1 can be seen when noise is introduced showing that the pure amount of data with similar context drastically affects the metrics, but with only so little data, HitRate@10 barely dropped (1%), with such a little drop, we can expect (as it is shown by results) that the reranker would be able to eliminate most of this noise in the final step of retrieval. When looking at the 2nd and 4th row (noise only), we get a more realistic image of how important a reranker is, with no noise, the change in HitRate@1 after adding the reranker is about 11%, while when working with 40 percent noise, adding the reranker increased the same metric by 14%. <br> <br>
Big takeaways from this notebook are: importance of a reranker and pure amount of data RAG is working with.
## Notebook 7
Until now, the whole project was based around information retrieval (IR) part of RAG, another important part is the LLM that actually communicates with the user, the generation part of RAG. This part is a bit tricky to test locally with small LLMs as it would take lots of time to test different temperatures of a model, models ability to not hallucinate answers when retrieval goes wrong, etc.<br> <br>
For sake of the "locally run AI assistant" we will use a smaller LLM like Phi3 mini with ollama. With our retrieval system in place, all we have to do is make a chain and see the results. Temperature was set to 0 so the model would only work with the retrieved context and not be creative with the answers.

## Conclusion
This project shows that building a capable RAG system for local use is entirely possible with models around 300 million parameters. The main takeaway is that while initial retrieval can be quite strong on small datasets, adding a lightweight reranker is the most effective way to handle "noise" and ensure the most relevant information actually reaches the LLM when more data is added.